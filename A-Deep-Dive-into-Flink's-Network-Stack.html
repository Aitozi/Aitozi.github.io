<!-- build time:Tue Jan 21 2020 22:48:41 GMT+0800 (China Standard Time) --><!DOCTYPE html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="google-site-verification" content="DO25iswIsaKZ5NZbYreVDjWtBKTyo1yFAROjSRcJD64"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet" type="text/css"><link href="//fonts.loli.net/css?family=Lato:300,300italic,400,400italic,700,700italic|consolas:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="//cdn.bootcss.com/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css"><meta name="keywords" content="Flink,"><link rel="alternate" href="/atom.xml" title="Aitozi" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2"><meta name="description" content="[toc]本文翻译自flink官网的一篇博文，详细介绍Flink架构中的网络栈，下面就让我们来一睹为快吧。"><meta name="keywords" content="Flink"><meta property="og:type" content="article"><meta property="og:title" content="A Deep-Dive into Flink&#39;s Network Stack"><meta property="og:url" content="http://www.aitozi.com/A-Deep-Dive-into-Flink's-Network-Stack.html"><meta property="og:site_name" content="Aitozi"><meta property="og:description" content="[toc]本文翻译自flink官网的一篇博文，详细介绍Flink架构中的网络栈，下面就让我们来一睹为快吧。"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack1.png"><meta property="og:image" content="https://github.com/Aitozi/images/blob/master/flink/flink-network-stack-1.jpg?raw=true"><meta property="og:image" content="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack2.png"><meta property="og:image" content="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack3.png"><meta property="og:image" content="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack4.png"><meta property="og:image" content="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack6.png"><meta property="og:image" content="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack7.png"><meta property="og:image" content="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack8.png"><meta property="og:image" content="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack9.png"><meta property="og:updated_time" content="2019-06-16T16:39:52.033Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="A Deep-Dive into Flink&#39;s Network Stack"><meta name="twitter:description" content="[toc]本文翻译自flink官网的一篇博文，详细介绍Flink架构中的网络栈，下面就让我们来一睹为快吧。"><meta name="twitter:image" content="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack1.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post",offset:12,offset_float:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:!0,duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://www.aitozi.com/A-Deep-Dive-into-Flink's-Network-Stack.html"><title>A Deep-Dive into Flink's Network Stack | Aitozi</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Aitozi</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description"></h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/A-Deep-Dive-into-Flink's-Network-Stack.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">A Deep-Dive into Flink's Network Stack</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-16T04:47:03+08:00">2019-06-16 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/深度好文/" itemprop="url" rel="index"><span itemprop="name">深度好文</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/A-Deep-Dive-into-Flink's-Network-Stack.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="A-Deep-Dive-into-Flink's-Network-Stack.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span> </span><span class="post-meta-divider">|</span> <span class="page-pv"><i class="fa fa-eye"></i> <span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">3.7k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">13</span></div></div></header><div class="post-body" itemprop="articleBody"><p>[toc]</p><p>本文翻译自flink官网的一篇博文，详细介绍Flink架构中的网络栈，下面就让我们来一睹为快吧。</p><a id="more"></a><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>Flink网络栈是flink中的核心组件，是flink-runtime模块的一部分。它连接了所有TaskManager中独立的工作单元（subtasks）。这是流入数据流经的地方，因此你所观察到的吞吐量和延迟都和他息息相关，可以说Flink的网络栈决定了Flink框架本身性能的好坏。和TaskManager，Jobmanager之间通信所使用的akka rpc框架不同的是，flink网络栈采用了更底层的网络api，使用的是Netty框架。</p><h3 id="Logical-View"><a href="#Logical-View" class="headerlink" title="Logical View"></a>Logical View</h3><p><img src="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack1.png" alt="logical view"></p><p>它抽象了以下三个概念的不同设置：</p><ul><li>Subtask output type (ResultPartitionType):<ul><li>pipelined (bounded or unbounded)： 上游一产生数据，就一条一条的往下游发送，作为有界或无界的数据流</li><li>blocking: 只有当上游的全部结果就绪之后才向下游发送数据</li></ul></li><li>Scheduling type:<ul><li>all at once (eager)： 同时部署所有的subtask（流式应用采用这种模式）</li><li>next stage on first output (lazy): 当上游的生产者开始有输出结果的时候，才开始不是下游的subtask，是一种lazy模式</li><li>next stage on complete output: 当上游的数据全部就绪之后才开始下游subtask的部署。</li></ul></li><li>Transport:<ul><li>high throughput: 不采用一条一条发送数据的模式，Flink缓存一批数据到network buffer中，攒批发送。这个减少了网络开销的单条边际成本，带来了高吞吐</li><li>low latency via buffer timeout: 通过减少发送未攒满一个buffer的timeout时间，牺牲一定的吞吐带来更低的延迟</li></ul></li></ul><p>我们将在下面这部分讨论吞吐和延迟的优化。这一部分将查看网络栈的物理层，对于这部分，我们将详细阐述output type以及调度模式。首先，subtask的输出类型和调度类型是紧密交织在一起的，两者的特定组合才有效。Pipelined result partition是流式的输出，流式输出需要将数据发送到一个正在工作的subtask，因此目标task就需要在上游结果产出下发之前deploy完成或者在任务启动最初完成deploy。 Batch作业产出有限的结果，而stream作业产出无限的结果。</p><p>Batch作业也可以以阻塞的方式产出结果，具体取决于operator和conector的使用模式。在这种方式下，下游算子需要再上游结果完全ready之后才进行部署，在这种方式下资源使用效率会更高.</p><p>下表总结了有效的组合方式：</p><table><thead><tr><th>Output Type</th><th>Scheduling Type</th><th>Applies to…</th></tr></thead><tbody><tr><td>pipelined, unbounded</td><td>all at once</td><td>Streaming jobs</td></tr><tr><td></td><td>next stage on first output</td><td>n/a¹</td></tr><tr><td>pipelined, bounded</td><td>all at once</td><td>n/a²</td></tr><tr><td></td><td>next stage on first output</td><td>Batch jobs</td></tr><tr><td>blocking</td><td>next stage on complete output</td><td>Batch jobs</td></tr></tbody></table><p>【1】: 当前没有再flink中使用<br>【2】: 在<a href="https://flink.apache.org/roadmap.html#batch-and-streaming-unification" target="_blank" rel="noopener">Batch/Streaming unification</a>完成后可能在流式作业中使用</p><p>另外对于有多个输入的subtask的batch作业，调度开始有两种模式，在所有input产出数据或者任意一个输入产出数据的时候。</p><h3 id="Physical-Transport"><a href="#Physical-Transport" class="headerlink" title="Physical Transport"></a>Physical Transport</h3><p>为了理解真实的吴礼数据的连接，请回想一下，在Flink中，不同的task可能会共享一个同一个slot，通过slot sharing group机制，TaskManager也可以提供多个slot来允许一个task的多个subtask跑在一个TaskManager上。</p><p>举个下图中的例子，我们假想一个有四个并发的任务，部署在两个分别有2个slot的TaskManager上。TaskManager 1 运行subtask A.1，A.2，B.1 和 B.2 而TaskManager 2 运行subtaskA.3,A.4,B.3,B.4。假设A和B之间的shuffle方式是<code>keyBy()</code>,这样在每一个TaskManager上都有2x4个逻辑连接，有些走local的，有些是通过网络的，如下图所示。</p><p><img src="https://github.com/Aitozi/images/blob/master/flink/flink-network-stack-1.jpg?raw=true" alt></p><p>不同task之间的每个（远程）网络连接，都将在flink网络栈中获得自己的TCP通道，但是，如果同一个任务的不同子任务被调度到同一个TaskManager上，他们和另一个TaskManager上的TCP连接将会共享（多路复用），在我们的例子中A.1 -&gt; B.3, A.1 -&gt; B.4 以及A.2 -&gt; B.3,A.2 -&gt; B.4将会复用一个tcp连接（这里有点疑问，按我的理解应该是每个TM之间都会共享channel才对）。</p><p><img src="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack2.png" alt></p><p>每个subtask的输出被称作<code>ResultPartition</code>, 每一个又被细分为<code>ResultSubPartition</code>,一个逻辑channel会有一个。在这个阶段，Flink已经不再单独处理每条记录了，而是将一组序列化完的数据打包拷贝到network buffer中，每一个subtask中local buffer pool（发送端和接收端各有一个pool）中所能获取的最多的buffer数量是通过以下的配置决定的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">channels * buffers-per-channel + floating-buffers-per-gate</span><br></pre></td></tr></table></figure><p>通常一个TaskManager上总的buffer数量不需要配置，在需要是可以查看相关的配置项<a href="https://ci.apache.org/projects/flink/flink-docs-release-1.8/ops/config.html#configuring-the-network-buffers" target="_blank" rel="noopener">Configuring the Network Buffers</a></p><h3 id="Inflicting-Backpressure-1"><a href="#Inflicting-Backpressure-1" class="headerlink" title="Inflicting Backpressure (1)"></a>Inflicting Backpressure (1)</h3><p>当一个subtask的发送端的buffer用尽之后 - buffer可能用于result subpartition的buffer队列，也可能正用于低阶的Netty的网络栈中尚未回收。在这种情况下producer就被block住，无法进一步发送数据。</p><p>消费端也是一样的方式，从底层netty网络栈传输来的buffer需要通过network buffer才能被正确消费，如果在消费端的network buffer用尽了，Flink将停止从channel中读入数据，直到有新的network buffer用来做数据的转化。这种情况下将会反压上游所有通过这个tcp channel的多路复用发送数据的生产端，这样也就限制了其他下游的消费能力。在下图阐述了一个有性能瓶颈的B.4 subtask将会引起backpressure，最终将会引起B.3无法消费和处理新的数据，尽管B.3还有足够的network buffer。</p><p><img src="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack3.png" alt></p><p>为了彻底解决这个问题，Flink1.5引入了流控机制。</p><h3 id="Credit-based-Flow-Control"><a href="#Credit-based-Flow-Control" class="headerlink" title="Credit-based Flow Control"></a>Credit-based Flow Control</h3><p>基于流控的网络传输能够确保正在传输的数据在接受端都有可以接受的buffer（传输的数据都是得到确认之后才可以向下游发送的）。新的传输模式仍然基于Flink原有的network buffer的能力，在其上做了一些扩展。除了仅仅有一个共享的本地buffer pool，每一个remote inputchannel现在会有其自己独占的一批buffer池，相对地，共享池中的buffer就被称为发floating buffer因为他们对于每一个inputchannel都是可以获取的。</p><p>接收端现在将会以<code>Credits</code>的形式通知发送端告知它能够接收多少buffer（1 buffer = 1 credit）。 每一个result subpartition将持续记录相对应channel的credits。只有当下游的通道有credits的时候，才会通过netty将上游的buffer发送出去，每发送一个buffer，就会减去一个credits，除了发送buffer数据，同时还会携带当前的backlog信息（指的是当前这个subparititon还有多少buffer在等待发送），接收端拿到这个信息之后就会去申请相应的合适数量的floating buffer来处理subpartition中排队等待处理的buffer。接收端一般会申请和backlog一样多的buffer，但这个并不总是能申请到这么多，也可能当前根本没有buffer可以申请。接收端将会通过监听buffer池，等待buffer使用完回收的时候就可以开始新的buffer申请</p><p><img src="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack4.png" alt></p><p>流控模式下使用<code>buffers-per-channel</code>来指定每一个channel的独占buffer的大小，使用<code>floating-buffers-per-gate</code>来指定local buffer pool的大小这两个参数的默认值理论上可以达到和非流控模式下的最大吞吐量，可能你需要根据你的网络带宽或者rt要求来调整相关的参数</p><h3 id="Inflicting-Backpressure-2"><a href="#Inflicting-Backpressure-2" class="headerlink" title="Inflicting Backpressure (2)"></a>Inflicting Backpressure (2)</h3><p>和没有流控的模式相比，credits能够提供更加直接有效的控制： 如果消费端无法快速处理造成消费端的buffer用尽，这样该recevier的credits就会降为0，发送端就不会再发送数据到这个partition。反压只发生在了这个通道上，并不会影响tcp通道的数据传输，因此其他接收端的消费能力并不会受到影响。</p><h3 id="What-do-we-Gain-Where-is-the-Catch"><a href="#What-do-we-Gain-Where-is-the-Catch" class="headerlink" title="What do we Gain? Where is the Catch?"></a>What do we Gain? Where is the Catch?</h3><p>通过这样的优化整体的资源使用率应该会得到上升，并且通过对正在传输的数据量的直接控制，也带来了checkpoint对齐时间的优化。但是receiver端发送消息也带来了额外的开销（在这之前接收端是只要负责收数据就好了，没有发消息的动作），尤其是在开启了SSL加密的情况下。并且一个input channel无法使用所有的buffer pool，因为独占的buffer无法共享。而且如果你产出数据比你发布credits慢，就会导致无法尽可能快的下发数据，虽然这些情况会带来性能上的损失，但是通常还是建议开启流控的模式，因为带来的诸多好处。</p><p>另一件你可能会注意到的是因为我们在发送端和接收端之间会缓存更少的数据可能会更早的碰到反压，可以通过调节上述的exclusive和floating buffer的大小来缓解</p><h3 id="Writing-Records-into-Network-Buffers-and-Reading-them-again"><a href="#Writing-Records-into-Network-Buffers-and-Reading-them-again" class="headerlink" title="Writing Records into Network Buffers and Reading them again"></a>Writing Records into Network Buffers and Reading them again</h3><p>下图对上面的图做了一些扩展，添加了一些周边组件使用的一些细节</p><p><img src="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack6.png" alt></p><p>在创建一个record将其传递给下游，比如通过<code>Collector#collect()</code>,它将被传递到RecordWriter中，recordwriter将这个java对象序列化成二进制数组，最终被拷贝至networkbuffer中像上述几节中的处理方式进行处理。RecordWriter首先将数据通过SpanningRecordWriter序列化到对上的一个byte数组中。然后将会将这些byte数组写到相应目标channel的的network buffer中，我们再最后一小节再回来讨论这块细节。</p><p>在接收端，netty会将接收到的buffer写入相应的input channel，流式任务的task最后将会读取这些队列中的buffer，将其通过RecordReader反序列化出来，和序列化过程类似，反序列也需要处理一些特殊情况，比如一条记录跨过了多个network buffer，可能是因为一条记录比较大，大过了单个networkbuffer大小（32K），也有可能数据被加入networkbuffer的时候已经没有足够的容纳空间了。</p><h3 id="Flushing-Buffers-to-Netty"><a href="#Flushing-Buffers-to-Netty" class="headerlink" title="Flushing Buffers to Netty"></a>Flushing Buffers to Netty</h3><p>在上图中，流控的机制实际上就是在NettyServer和NettyClient组件中实现的，RecordWriter正在写的buffer总是最开始是空的没有数据的状态被加入到result subpatition中，然后再渐渐的被序列化的记录填满，但是netty是什么时候真正处理这些buffer呢？</p><p>显然他不能一有数据就去请求，因为这会带来巨大的开销，涉及到跨线程通信和同步，并且也会将整个buffer废弃掉。</p><p>在Flink中有3个情况可以将buffer变为NettyServer可以消费的状态：</p><ul><li>buffer写满了</li><li>buffer timeout时间条件满足了</li><li>一个特殊的event发送了，比如checkpoint barrier，为了保证一致性就需要发送</li></ul><h3 id="Flush-after-Buffer-Full"><a href="#Flush-after-Buffer-Full" class="headerlink" title="Flush after Buffer Full"></a>Flush after Buffer Full</h3><p>在序列化完成后，RecordWriter会将这些bytes写出到合适的subpartition的network buffer队列中去，尽管一个RecordWriter可以处理多个subpartition，但是每一个subpartition都只会有一个writer向其写数据。NettyServer会从多个subpartition读取buffer，通过一个channel发送出去。这是经典的生产者和消费者模式，正如下图所示。在(1)序列化和(2)将数据写出到buffer，RecordWriter会更新buffer的writer下标，一旦buffer已经满了，RecordWriter会从他的local buffer pool中申请一块新的buffer用来写当前记录的剩余的bytes，或者写下一条记录，并且将新的buffer添加搭配subpartition的queue中。 （4）通知NettyServer数据已经ready，当netty能够发送数据时，就会（5）从queue中获取buffer，通过TCP channel将数据发送给下游。</p><p><img src="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack7.png" alt></p><h3 id="Flush-after-Buffer-Timeout"><a href="#Flush-after-Buffer-Timeout" class="headerlink" title="Flush after Buffer Timeout"></a>Flush after Buffer Timeout</h3><p>为了能够支持低延迟的处理场景，我们不能只依赖buffer变满的下发信号。可能会由于上游某些通道数据不多带了发送延迟。因此还有周期性的发送机制：<code>OutputFlusher</code>。下图展示了这个flusher机制是如何和其他组件协同工作的。</p><p>RecordWriter进行序列化并将数据写入network buffer。但是并行的output flusher（3，4）会通知NettyServer进行数据的消费。当NettyServer收到通知之后，他将会消费buffer中可以消费的数据，并且更新buffer的reader index。buffer仍然会保留在队列中，下一次对该buffer的读取操作会从上次的reader index开始。严格的说flusher并没有保障数据的发送，他仅仅是通知NettyServer可以进行数据发送，如果正处于反压状态，flusher并没有什么实际效果</p><p><img src="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack8.png" alt></p><h3 id="Flush-after-special-event"><a href="#Flush-after-special-event" class="headerlink" title="Flush after special event"></a>Flush after special event</h3><p>一些特殊消息也会触发数据的flush，最重要的就是checkpoint barrier和end-of-stream</p><h3 id="Future-remarks"><a href="#Future-remarks" class="headerlink" title="Future remarks"></a>Future remarks</h3><p>和Flink1.5之前相比，network buffers现在被放置在了subpartition的队列中，而且每次flush我们并不会关闭buffer，这给我们带来一些好处：</p><ul><li>减少了同步带来的消耗（output flusher和RecordWriter是相互独立的）</li><li>在负载高，Netty是性能瓶颈的情况下我们仍然可以在不完整的buffer中累积数据</li><li>减少了netty的通知信号</li></ul><p>但是你可能也注意到在低负载的场景下，cpu使用率和TCP发包率会变高，这是因为flink会利用cpu来达到想要的低延迟，在高负载情况下性能可能会更好一些，因为去除了一些同步的消耗。</p><h3 id="Buffer-Builder-amp-Buffer-Consumer"><a href="#Buffer-Builder-amp-Buffer-Consumer" class="headerlink" title="Buffer Builder &amp; Buffer Consumer"></a>Buffer Builder &amp; Buffer Consumer</h3><p>这一节可以参考我之前博客中的相关分析<a href="http://aitozi.com/flink-network-feature.html#BufferBuilder%EF%BC%8CBufferConsumer%EF%BC%8CPositionMarker" target="_blank" rel="noopener">BufferBuiler/BufferConsumer</a></p><h3 id="Latency-vs-Throughput"><a href="#Latency-vs-Throughput" class="headerlink" title="Latency vs. Throughput"></a>Latency vs. Throughput</h3><p>Network buffers是被使用来以期望达到更高的资源利用率，并且让数据再buffer中等待一段时间来攒批发送来达到更高的吞吐。尽管这个等待时长可以通过设置buffer timeout时间。你可能会好奇想找出latency和thoughout之间的平衡关系。显然，你是不可能同时拥有这两者的。下图显示了不同的time out时间设置，0ms-100ms所带来的吞吐量的提升，这些测试是跑在100个就节点，每个节点8个slots，没有业务逻辑只有纯粹的网络栈开销。</p><p><img src="https://flink.apache.org/img/blog/2019-06-05-network-stack/flink-network-stack9.png" alt></p><p>如你所见，Flink1.5+，即使是非常低的缓冲区超时（例如1ms）也提供高达默认超时的75％的最大吞吐量。</p><p>参考: <a href="https://flink.apache.org/2019/06/05/flink-network-stack.html" target="_blank" rel="noopener">https://flink.apache.org/2019/06/05/flink-network-stack.html</a></p></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>谢谢支持</div><button id="rewardButton" disable="enable" onclick='var e=document.getElementById("QR");"none"===e.style.display?e.style.display="block":e.style.display="none"'><span>Donate</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/wechatpay.png" alt="aitozi WeChat Pay"><p>WeChat Pay</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Flink/" rel="tag"># Flink</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/jmh-usage.html" rel="next" title="Jmh测试框架和flink benchmark工程"><i class="fa fa-chevron-left"></i> Jmh测试框架和flink benchmark工程</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/whats-the-point-of-our-live.html" rel="prev" title="人生的意义是什么？活着的意义是什么？">人生的意义是什么？活着的意义是什么？ <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview">站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="aitozi"><p class="site-author-name" itemprop="name">aitozi</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">31</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">10</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">19</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="gjying1314@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-globe"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://wuchong.me/" title="wuchong" target="_blank">wuchong</a></li><li class="links-of-blogroll-item"><a href="http://chenyuzhao.me/" title="yuzhao" target="_blank">yuzhao</a></li><li class="links-of-blogroll-item"><a href="http://blog.csdn.net/yanghua_kobe?viewmode=contents" title="vinoyang" target="_blank">vinoyang</a></li><li class="links-of-blogroll-item"><a href="http://blog.csdn.net/lmalds?viewmode=contents" title="Imalds" target="_blank">Imalds</a></li><li class="links-of-blogroll-item"><a href="http://blog.csdn.net/androidlushangderen" title="hadoop" target="_blank">hadoop</a></li><li class="links-of-blogroll-item"><a href="http://www.cnblogs.com/xrq730/p/5260294.html" title="java开发" target="_blank">java开发</a></li><li class="links-of-blogroll-item"><a href="http://www.hollischuang.com/" title="阿里工程师" target="_blank">阿里工程师</a></li><li class="links-of-blogroll-item"><a href="http://www.cnblogs.com/fxjwind/" title="阿里流计算工程师" target="_blank">阿里流计算工程师</a></li><li class="links-of-blogroll-item"><a href="http://jm.taobao.org/" title="阿里中间件博客" target="_blank">阿里中间件博客</a></li><li class="links-of-blogroll-item"><a href="http://armsword.com/" title="duruofei" target="_blank">duruofei</a></li><li class="links-of-blogroll-item"><a href="http://blog.yufeng.info/" title="褚霸" target="_blank">褚霸</a></li><li class="links-of-blogroll-item"><a href="https://yuzhouwan.com" title="宇宙湾" target="_blank">宇宙湾</a></li><li class="links-of-blogroll-item"><a href="http://matt33.com" title="matt" target="_blank">matt</a></li><li class="links-of-blogroll-item"><a href="http://coding-geek.com/" title="coding-geek" target="_blank">coding-geek</a></li></ul></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#简介"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logical-View"><span class="nav-number">2.</span> <span class="nav-text">Logical View</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Physical-Transport"><span class="nav-number">3.</span> <span class="nav-text">Physical Transport</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inflicting-Backpressure-1"><span class="nav-number">4.</span> <span class="nav-text">Inflicting Backpressure (1)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Credit-based-Flow-Control"><span class="nav-number">5.</span> <span class="nav-text">Credit-based Flow Control</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inflicting-Backpressure-2"><span class="nav-number">6.</span> <span class="nav-text">Inflicting Backpressure (2)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#What-do-we-Gain-Where-is-the-Catch"><span class="nav-number">7.</span> <span class="nav-text">What do we Gain? Where is the Catch?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Writing-Records-into-Network-Buffers-and-Reading-them-again"><span class="nav-number">8.</span> <span class="nav-text">Writing Records into Network Buffers and Reading them again</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flushing-Buffers-to-Netty"><span class="nav-number">9.</span> <span class="nav-text">Flushing Buffers to Netty</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flush-after-Buffer-Full"><span class="nav-number">10.</span> <span class="nav-text">Flush after Buffer Full</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flush-after-Buffer-Timeout"><span class="nav-number">11.</span> <span class="nav-text">Flush after Buffer Timeout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Flush-after-special-event"><span class="nav-number">12.</span> <span class="nav-text">Flush after special event</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Future-remarks"><span class="nav-number">13.</span> <span class="nav-text">Future remarks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Buffer-Builder-amp-Buffer-Consumer"><span class="nav-number">14.</span> <span class="nav-text">Buffer Builder &amp; Buffer Consumer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Latency-vs-Throughput"><span class="nav-number">15.</span> <span class="nav-text">Latency vs. Throughput</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">aitozi</span></div><div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div><div class="theme-info">主题 - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user"></i> <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="site-pv"><i class="fa fa-eye"></i> <span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/jquery.lazyload/1.9.3/jquery.lazyload.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.ui.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><script id="dsq-count-scr" src="https://aitozi.disqus.com/count.js" async></script><script type="text/javascript">var disqus_config = function () {
          this.page.url = 'http://www.aitozi.com/A-Deep-Dive-into-Flink's-Network-Stack.html';
          this.page.identifier = 'A-Deep-Dive-into-Flink's-Network-Stack.html';
          this.page.title = 'A Deep-Dive into Flink\'s Network Stack';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://aitozi.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);</script><script>!function(){var t=document.createElement("script"),s=window.location.protocol.split(":")[0];"https"===s?t.src="https://zz.bdstatic.com/linksubmit/push.js":t.src="http://push.zhanzhang.baidu.com/push.js";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script></body></html><!-- rebuild by neat -->