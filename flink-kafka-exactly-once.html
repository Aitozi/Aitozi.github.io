<!-- build time:Tue Jan 21 2020 23:02:05 GMT+0800 (China Standard Time) --><!DOCTYPE html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="google-site-verification" content="DO25iswIsaKZ5NZbYreVDjWtBKTyo1yFAROjSRcJD64"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet" type="text/css"><link href="//fonts.loli.net/css?family=Lato:300,300italic,400,400italic,700,700italic|consolas:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="//cdn.bootcss.com/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css"><meta name="keywords" content="Flink,Kafka,"><link rel="alternate" href="/atom.xml" title="Aitozi" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2"><meta name="description" content="flink kafka-connector0.10版本分析，与1.4版本中kafka11对比"><meta name="keywords" content="Flink,Kafka"><meta property="og:type" content="article"><meta property="og:title" content="上篇·flink 1.4利用kafka0.11实现完整的一致性语义"><meta property="og:url" content="http://www.aitozi.com/flink-kafka-exactly-once.html"><meta property="og:site_name" content="Aitozi"><meta property="og:description" content="flink kafka-connector0.10版本分析，与1.4版本中kafka11对比"><meta property="og:locale" content="zh-Hans"><meta property="og:updated_time" content="2019-03-14T17:04:58.000Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="上篇·flink 1.4利用kafka0.11实现完整的一致性语义"><meta name="twitter:description" content="flink kafka-connector0.10版本分析，与1.4版本中kafka11对比"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post",offset:12,offset_float:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:!0,duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://www.aitozi.com/flink-kafka-exactly-once.html"><title>上篇·flink 1.4利用kafka0.11实现完整的一致性语义 | Aitozi</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Aitozi</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description"></h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-kafka-exactly-once.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline">上篇·flink 1.4利用kafka0.11实现完整的一致性语义</h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-11T22:29:44+08:00">2018-03-11 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/源码分析/" itemprop="url" rel="index"><span itemprop="name">源码分析</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-kafka-exactly-once.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-kafka-exactly-once.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span> </span><span class="post-meta-divider">|</span> <span class="page-pv"><i class="fa fa-eye"></i> <span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">3k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">15</span></div></div></header><div class="post-body" itemprop="articleBody"><p>flink kafka-connector0.10版本分析，与1.4版本中kafka11对比</p><a id="more"></a><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>官方文档在出了1.4之后特意发表了一篇blog，通过以下这两个条件实现了真正意义上的exactly once语义</p><ol><li>kafka producer0.11的事务性</li><li><a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol" target="_blank" rel="noopener">two phase commit protocol</a></li></ol><p>我们先看0.10版本的kafka-connector的行为逻辑.</p><h1 id="Kafka-Connector10"><a href="#Kafka-Connector10" class="headerlink" title="Kafka-Connector10"></a>Kafka-Connector10</h1><h2 id="kafkaConsumer10"><a href="#kafkaConsumer10" class="headerlink" title="kafkaConsumer10"></a>kafkaConsumer10</h2><h3 id="FlinkKafkaConsumerBase"><a href="#FlinkKafkaConsumerBase" class="headerlink" title="FlinkKafkaConsumerBase"></a>FlinkKafkaConsumerBase</h3><p>这个抽象类实现了<code>CheckpointedFunction</code>, 这个接口的描述：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">* This is the core <span class="class"><span class="keyword">interface</span> <span class="title">for</span> &lt;<span class="title">i</span>&gt;<span class="title">stateful</span> <span class="title">transformation</span> <span class="title">functions</span>&lt;/<span class="title">i</span>&gt;, <span class="title">meaning</span> <span class="title">functions</span></span></span><br><span class="line"><span class="class">* <span class="title">that</span> <span class="title">maintain</span> <span class="title">state</span> <span class="title">across</span> <span class="title">individual</span> <span class="title">stream</span> <span class="title">records</span>.</span></span><br><span class="line"><span class="class">* <span class="title">While</span> <span class="title">more</span> <span class="title">lightweight</span> <span class="title">interfaces</span> <span class="title">exist</span> <span class="title">as</span> <span class="title">shortcuts</span> <span class="title">for</span> <span class="title">various</span> <span class="title">types</span> <span class="title">of</span> <span class="title">state</span>, <span class="title">this</span> <span class="title">interface</span> <span class="title">offer</span> <span class="title">the</span></span></span><br><span class="line"><span class="class">* <span class="title">greatest</span> <span class="title">flexibility</span> <span class="title">in</span> <span class="title">managing</span> <span class="title">both</span> &lt;<span class="title">i</span>&gt;<span class="title">keyed</span> <span class="title">state</span>&lt;/<span class="title">i</span>&gt; <span class="title">and</span> &lt;<span class="title">i</span>&gt;<span class="title">operator</span> <span class="title">state</span>&lt;/<span class="title">i</span>&gt;.</span></span><br></pre></td></tr></table></figure><p>这个接口中主要要去做两件事情：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//每一次做checkpoint的时候被调用</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化每一个并发的实例的时候被调用</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br></pre></td></tr></table></figure><p>初始化函数的调用时机是在<code>open</code>之前的：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//StreamTask.java</span></span><br><span class="line">initializeState();</span><br><span class="line">openAllOperators();</span><br></pre></td></tr></table></figure><p>在初始化的函数中提供了一个<code>FunctionSnapshotContext</code><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">void snapshotState(FunctionSnapshotContext context) throws Exception;</span><br></pre></td></tr></table></figure><p></p><p>让你既可以注册一个KeyedStateStore，也可以注册一个OperatorStateStore</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ManagedInitializationContext</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Returns true, if state was restored from the snapshot of a previous execution. This returns always false for</span></span><br><span class="line"><span class="comment">	 * stateless tasks.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">boolean</span> <span class="title">isRestored</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Returns an interface that allows for registering operator state with the backend.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function">OperatorStateStore <span class="title">getOperatorStateStore</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Returns an interface that allows for registering keyed state with the backend.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function">KeyedStateStore <span class="title">getKeyedStateStore</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们可以看到kafka10是怎么利用这个<code>CheckpointedFunction</code>来管理记录内部offset的呢？</p><h3 id="initializeState"><a href="#initializeState" class="headerlink" title="initializeState"></a>initializeState</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始化过程</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// we might have been restored via restoreState() which restores from legacy operator state</span></span><br><span class="line">		<span class="keyword">if</span> (!restored) &#123;</span><br><span class="line">			restored = context.isRestored();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		OperatorStateStore stateStore = context.getOperatorStateStore();</span><br><span class="line">		offsetsStateForCheckpoint = stateStore.getSerializableListState(DefaultOperatorStateBackend.DEFAULT_OPERATOR_STATE_NAME);</span><br><span class="line">		<span class="comment">//如果是在重启恢复的过程中</span></span><br><span class="line">		<span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">			<span class="keyword">if</span> (restoredState == <span class="keyword">null</span>) &#123;</span><br><span class="line">				restoredState = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">				<span class="keyword">for</span> (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : offsetsStateForCheckpoint.get()) &#123;</span><br><span class="line">				<span class="comment">//partition和相应的offset数</span></span><br><span class="line">					restoredState.put(kafkaOffset.f0, kafkaOffset.f1);</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				LOG.info(<span class="string">"Setting restore state in the FlinkKafkaConsumer."</span>);</span><br><span class="line">				<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">					LOG.debug(<span class="string">"Using the following offsets: &#123;&#125;"</span>, restoredState);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			LOG.info(<span class="string">"No restore state for FlinkKafkaConsumer."</span>);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p>那么我们看到恢复或者初始化的时候将<partition ,offset>信息保存到了一组HashMap中，但是这个Map怎么作用于消费阶段呢？</partition></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将从状态中获取到的列表赋予给消费的列表</span></span><br><span class="line">subscribedPartitionsToStartOffsets = restoredState;</span><br></pre></td></tr></table></figure><h3 id="snapshot"><a href="#snapshot" class="headerlink" title="snapshot"></a>snapshot</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (!running) &#123;</span><br><span class="line">		LOG.debug(<span class="string">"snapshotState() called on closed source"</span>);</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line">		offsetsStateForCheckpoint.clear();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">final</span> AbstractFetcher&lt;?, ?&gt; fetcher = <span class="keyword">this</span>.kafkaFetcher;</span><br><span class="line">		<span class="keyword">if</span> (fetcher == <span class="keyword">null</span>) &#123;</span><br><span class="line">			<span class="comment">// fetcher还没有初始化，返回上一次恢复的partition和offset，也就是这里会有个bug。我提了个issue:[FLINK-8869][2]</span></span><br><span class="line">			<span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) &#123;</span><br><span class="line">				offsetsStateForCheckpoint.add(Tuple2.of(subscribedPartition.getKey(), subscribedPartition.getValue()));</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="comment">//</span></span><br><span class="line">			<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">				<span class="comment">// the map cannot be asynchronously updated, because only one checkpoint call can happen</span></span><br><span class="line">				<span class="comment">// on this function at a time: either snapshotState() or notifyCheckpointComplete()</span></span><br><span class="line">				pendingOffsetsToCommit.put(context.getCheckpointId(), restoredState);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			HashMap&lt;KafkaTopicPartition, Long&gt; currentOffsets = fetcher.snapshotCurrentState();</span><br><span class="line"></span><br><span class="line">			<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">				<span class="comment">// the map cannot be asynchronously updated, because only one checkpoint call can happen</span></span><br><span class="line">				<span class="comment">// on this function at a time: either snapshotState() or notifyCheckpointComplete()</span></span><br><span class="line">				pendingOffsetsToCommit.put(context.getCheckpointId(), currentOffsets);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) &#123;</span><br><span class="line">				offsetsStateForCheckpoint.add(</span><br><span class="line">						Tuple2.of(kafkaTopicPartitionLongEntry.getKey(), kafkaTopicPartitionLongEntry.getValue()));</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">			<span class="comment">// truncate the map of pending offsets to commit, to prevent infinite growth</span></span><br><span class="line">			<span class="keyword">while</span> (pendingOffsetsToCommit.size() &gt; MAX_NUM_PENDING_CHECKPOINTS) &#123;</span><br><span class="line">				pendingOffsetsToCommit.remove(<span class="number">0</span>);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="offsetCommitMode"><a href="#offsetCommitMode" class="headerlink" title="offsetCommitMode"></a>offsetCommitMode</h3><p>如上述代码中的<code>offsetCommitMode</code>,主要有以下几种</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DISABLED,</span><br><span class="line">ON_CHECKPOINTS, <span class="comment">// 完成一次checkpoint后向kafka提交消费offset，只有这种模式下才需要我们手动去提交offset到kafka</span></span><br><span class="line">KAFKA_PERIODIC;</span><br></pre></td></tr></table></figure><p>这个配置主要改变了commitOffset回kafka的时机. 首先在snapshot的时候会将对应的checkpointId和相应的offset的列表放入<code>pendingOffsetsToCommit</code>, 在checkpoint完成后回调<code>notifyCheckpointComplete</code>，这里面主要完成了offset的commit工作。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">			<span class="comment">// only one commit operation must be in progress</span></span><br><span class="line">			<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">				LOG.debug(<span class="string">"Committing offsets to Kafka/ZooKeeper for checkpoint "</span> + checkpointId);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">try</span> &#123;</span><br><span class="line">				<span class="keyword">final</span> <span class="keyword">int</span> posInMap = pendingOffsetsToCommit.indexOf(checkpointId);</span><br><span class="line">				<span class="keyword">if</span> (posInMap == -<span class="number">1</span>) &#123;</span><br><span class="line">					LOG.warn(<span class="string">"Received confirmation for unknown checkpoint id &#123;&#125;"</span>, checkpointId);</span><br><span class="line">					<span class="keyword">return</span>;</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				<span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</span><br><span class="line">				HashMap&lt;KafkaTopicPartition, Long&gt; offsets =</span><br><span class="line">					(HashMap&lt;KafkaTopicPartition, Long&gt;) pendingOffsetsToCommit.remove(posInMap);</span><br><span class="line"></span><br><span class="line">				<span class="comment">// remove older checkpoints in map</span></span><br><span class="line">				<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; posInMap; i++) &#123;</span><br><span class="line">					pendingOffsetsToCommit.remove(<span class="number">0</span>);</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				<span class="keyword">if</span> (offsets == <span class="keyword">null</span> || offsets.size() == <span class="number">0</span>) &#123;</span><br><span class="line">					LOG.debug(<span class="string">"Checkpoint state was empty."</span>);</span><br><span class="line">					<span class="keyword">return</span>;</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				fetcher.commitInternalOffsetsToKafka(offsets, offsetCommitCallback);</span><br><span class="line">			&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">				<span class="keyword">if</span> (running) &#123;</span><br><span class="line">					<span class="keyword">throw</span> e;</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="comment">// else ignore exception if we are no longer running</span></span><br><span class="line">			&#125;</span><br></pre></td></tr></table></figure><h3 id="消费partition分配问题"><a href="#消费partition分配问题" class="headerlink" title="消费partition分配问题"></a>消费partition分配问题</h3><ul><li><p>在从上次点恢复的情况下是直接从state中获取应该读取哪一个partition，offset。如果并发度改变了会做出什么样的反馈呢?会正确做出rescale吗</p></li><li><p>第一次进行读取的时候会初始化处当前task（并发度）所需要订阅的partition</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> Map&lt;KafkaTopicPartition, Long&gt; <span class="title">initializeSubscribedPartitionsToStartOffsets</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">			List&lt;KafkaTopicPartition&gt; kafkaTopicPartitions, //topic的所有partition</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="keyword">int</span> indexOfThisSubtask, // 当前task的维度</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="keyword">int</span> numParallelSubtasks, // 总的并发度</span></span></span><br><span class="line"><span class="function"><span class="params">			StartupMode startupMode, // 从哪个offset消费的模式（最新，最老，指定offset）</span></span></span><br><span class="line"><span class="function"><span class="params">			Map&lt;KafkaTopicPartition, Long&gt; specificStartupOffsets)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		Map&lt;KafkaTopicPartition, Long&gt; subscribedPartitionsToStartOffsets = <span class="keyword">new</span> HashMap&lt;&gt;(kafkaTopicPartitions.size());</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (KafkaTopicPartition kafkaTopicPartition : kafkaTopicPartitions) &#123;</span><br><span class="line">			<span class="comment">// only handle partitions that this subtask should subscribe to（选取当前subtask所需要订阅的partition）</span></span><br><span class="line">			<span class="keyword">if</span> (KafkaTopicPartitionAssigner.assign(kafkaTopicPartition, numParallelSubtasks) == indexOfThisSubtask) &#123;</span><br><span class="line">				<span class="keyword">if</span> (startupMode != StartupMode.SPECIFIC_OFFSETS) &#123;</span><br><span class="line">					<span class="comment">//StateSentinel都是一串随机的负数占位符(都是一个标记在KafkaConsumerThread中进行判断)</span></span><br><span class="line">					subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, startupMode.getStateSentinel());</span><br><span class="line">				&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">					<span class="keyword">if</span> (specificStartupOffsets == <span class="keyword">null</span>) &#123;</span><br><span class="line">						<span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(</span><br><span class="line">							<span class="string">"Startup mode for the consumer set to "</span> + StartupMode.SPECIFIC_OFFSETS +</span><br><span class="line">								<span class="string">", but no specific offsets were specified"</span>);</span><br><span class="line">					&#125;</span><br><span class="line"></span><br><span class="line">					Long specificOffset = specificStartupOffsets.get(kafkaTopicPartition);</span><br><span class="line">					<span class="keyword">if</span> (specificOffset != <span class="keyword">null</span>) &#123;</span><br><span class="line">						<span class="comment">// since the specified offsets represent the next record to read, we subtract</span></span><br><span class="line">						<span class="comment">// it by one so that the initial state of the consumer will be correct</span></span><br><span class="line">						<span class="comment">// 这里需要减去1</span></span><br><span class="line">						subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, specificOffset - <span class="number">1</span>);</span><br><span class="line">					&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">						subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET);</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> subscribedPartitionsToStartOffsets;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assign</span><span class="params">(KafkaTopicPartition partition, <span class="keyword">int</span> numParallelSubtasks)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> startIndex = ((partition.getTopic().hashCode() * <span class="number">31</span>) &amp; <span class="number">0x7FFFFFFF</span>) % numParallelSubtasks;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// here, the assumption is that the id of Kafka partitions are always ascending</span></span><br><span class="line">	<span class="comment">// starting from 0, and therefore can be used directly as the offset clockwise from the start index</span></span><br><span class="line">	<span class="comment">// 这里看出：每个Partition只会分配到一个subtask来消费</span></span><br><span class="line">	<span class="comment">// 1. partition &gt; parallel 一个subtask会订阅多个partition</span></span><br><span class="line">	<span class="comment">// 2. partition &lt; parallel 有subtask会是空闲的</span></span><br><span class="line">	<span class="comment">// 3. startIndex由topic名字计算得出</span></span><br><span class="line">	<span class="keyword">return</span> (startIndex + partition.getPartition()) % numParallelSubtasks;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="消费模型kafka10"><a href="#消费模型kafka10" class="headerlink" title="消费模型kafka10"></a>消费模型kafka10</h3><p>在完成partition订阅之后，就要开始真正的run方法了，<code>FlinkKafkaConsumer</code>也是实现自<code>SouceFunction</code>，因此主要的逻辑也都是在run方法中实现。 主要逻辑：</p><p>kafkaConsumerThread和Kafka10Fetcher通过<code>Handover</code>交互,我觉得这段代码写的很不错，可以好好学习下。可以形象的比作在接力跑：<code>kafkaConsumerThread</code>通过真正的消费线程消费放入一个<code>HandOver</code>，再由kafkaFetcher去poll，完成整个消费过程。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// we need only do work, if we actually have partitions assigned</span></span><br><span class="line"><span class="keyword">if</span> (!subscribedPartitionsToStartOffsets.isEmpty()) &#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// create the fetcher that will communicate with the Kafka brokers</span></span><br><span class="line">	<span class="keyword">final</span> AbstractFetcher&lt;T, ?&gt; fetcher = createFetcher(</span><br><span class="line">			sourceContext,</span><br><span class="line">			subscribedPartitionsToStartOffsets,</span><br><span class="line">			periodicWatermarkAssigner,</span><br><span class="line">			punctuatedWatermarkAssigner,</span><br><span class="line">			(StreamingRuntimeContext) getRuntimeContext(),</span><br><span class="line">			offsetCommitMode);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// publish the reference, for snapshot-, commit-, and cancel calls</span></span><br><span class="line">	<span class="comment">// IMPORTANT: We can only do that now, because only now will calls to</span></span><br><span class="line">	<span class="comment">//            the fetchers 'snapshotCurrentState()' method return at least</span></span><br><span class="line">	<span class="comment">//            the restored offsets</span></span><br><span class="line">	<span class="keyword">this</span>.kafkaFetcher = fetcher;</span><br><span class="line">	<span class="keyword">if</span> (!running) &#123;</span><br><span class="line">		<span class="keyword">return</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// (3) run the fetcher' main work method</span></span><br><span class="line">	<span class="comment">// 主要工作方法</span></span><br><span class="line">	fetcher.runFetchLoop();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">	<span class="comment">// this source never completes, so emit a Long.MAX_VALUE watermark</span></span><br><span class="line">	<span class="comment">// to not block watermark forwarding</span></span><br><span class="line">	<span class="comment">// 发送最大的watermark就不会block住下游的watermark更新</span></span><br><span class="line">	sourceContext.emitWatermark(<span class="keyword">new</span> Watermark(Long.MAX_VALUE));</span><br><span class="line"></span><br><span class="line">	<span class="comment">// wait until this is canceled</span></span><br><span class="line">	<span class="keyword">final</span> Object waitLock = <span class="keyword">new</span> Object();</span><br><span class="line">	<span class="keyword">while</span> (running) &#123;</span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			<span class="comment">//noinspection SynchronizationOnLocalVariableOrMethodParameter</span></span><br><span class="line">			<span class="keyword">synchronized</span> (waitLock) &#123;</span><br><span class="line">				waitLock.wait();</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">			<span class="keyword">if</span> (!running) &#123;</span><br><span class="line">				<span class="comment">// restore the interrupted state, and fall through the loop</span></span><br><span class="line">				<span class="comment">// 打断当前线程</span></span><br><span class="line">				Thread.currentThread().interrupt();</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Handover的描述， Handover代码可以再好好学习下。</span><br><span class="line">* The Handover is a utility to hand over data (a buffer of records) and exception from a</span><br><span class="line">* &lt;i&gt;producer&lt;/i&gt; thread to a &lt;i&gt;consumer&lt;/i&gt; thread. It effectively behaves like a</span><br><span class="line">* &quot;size one blocking queue&quot;, with some extras around exception reporting, closing, and</span><br><span class="line">* waking up thread without &#123;@link Thread#interrupt() interrupting&#125; threads.</span><br></pre></td></tr></table></figure><h3 id="KafkaFetcher"><a href="#KafkaFetcher" class="headerlink" title="KafkaFetcher"></a>KafkaFetcher</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">runFetchLoop</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		<span class="keyword">final</span> Handover handover = <span class="keyword">this</span>.handover;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// kick off the actual Kafka consumer</span></span><br><span class="line">		<span class="comment">// 启动真正的消费线程</span></span><br><span class="line">		consumerThread.start();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">while</span> (running) &#123;</span><br><span class="line">			<span class="comment">// this blocks until we get the next records</span></span><br><span class="line">			<span class="comment">// it automatically re-throws exceptions encountered in the fetcher thread</span></span><br><span class="line">			<span class="comment">// 在handover中获取真正的数据，并抛出其他线程中的异常</span></span><br><span class="line">			<span class="keyword">final</span> ConsumerRecords&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; records = handover.pollNext();</span><br><span class="line"></span><br><span class="line">			<span class="comment">// get the records for each topic partition</span></span><br><span class="line">			<span class="comment">// subscribedPartitionStates维护的是每个partition的状态（partition，KPH（partition的描述，依据版本可能不同），offset, committedOffset, watermark）</span></span><br><span class="line">			<span class="keyword">for</span> (KafkaTopicPartitionState&lt;TopicPartition&gt; partition : subscribedPartitionStates()) &#123;</span><br><span class="line"></span><br><span class="line">				List&lt;ConsumerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt; partitionRecords =</span><br><span class="line">						records.records(partition.getKafkaPartitionHandle());</span><br><span class="line"></span><br><span class="line">				<span class="keyword">for</span> (ConsumerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; record : partitionRecords) &#123;</span><br><span class="line">					<span class="keyword">final</span> T value = deserializer.deserialize(</span><br><span class="line">							record.key(), record.value(),</span><br><span class="line">							record.topic(), record.partition(), record.offset());</span><br><span class="line"></span><br><span class="line">					<span class="keyword">if</span> (deserializer.isEndOfStream(value)) &#123;</span><br><span class="line">						<span class="comment">// end of stream signaled</span></span><br><span class="line">						running = <span class="keyword">false</span>;</span><br><span class="line">						<span class="keyword">break</span>;</span><br><span class="line">					&#125;</span><br><span class="line"></span><br><span class="line">					<span class="comment">// emit the actual record. this also updates offset state atomically</span></span><br><span class="line">					<span class="comment">// and deals with timestamps and watermark generation</span></span><br><span class="line">					<span class="comment">// 这里会进入真正的通过sourceContext发送数据的代码 如下</span></span><br><span class="line">					emitRecord(value, partition, record.offset(), record);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">finally</span> &#123;</span><br><span class="line">		<span class="comment">// this signals the consumer thread that no more work is to be done</span></span><br><span class="line">		consumerThread.shutdown();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// on a clean exit, wait for the runner thread</span></span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		consumerThread.join();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">		<span class="comment">// may be the result of a wake-up interruption after an exception.</span></span><br><span class="line">		<span class="comment">// we ignore this here and only restore the interruption state</span></span><br><span class="line">		Thread.currentThread().interrupt();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">emitRecordWithTimestamp</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		T record, KafkaTopicPartitionState&lt;KPH&gt; partitionState, <span class="keyword">long</span> offset, <span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (record != <span class="keyword">null</span>) &#123;</span><br><span class="line">		<span class="keyword">if</span> (timestampWatermarkMode == NO_TIMESTAMPS_WATERMARKS) &#123;</span><br><span class="line">			<span class="comment">// fast path logic, in case there are no watermarks generated in the fetcher</span></span><br><span class="line"></span><br><span class="line">			<span class="comment">// emit the record, using the checkpoint lock to guarantee</span></span><br><span class="line">			<span class="comment">// atomicity of record emission and offset state update</span></span><br><span class="line">			<span class="keyword">synchronized</span> (checkpointLock) &#123;</span><br><span class="line">				sourceContext.collectWithTimestamp(record, timestamp);</span><br><span class="line">				partitionState.setOffset(offset);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (timestampWatermarkMode == PERIODIC_WATERMARKS) &#123;</span><br><span class="line">		    <span class="comment">// 更新partitionstate中的watermark状态</span></span><br><span class="line">			emitRecordWithTimestampAndPeriodicWatermark(record, partitionState, offset, timestamp);</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			emitRecordWithTimestampAndPunctuatedWatermark(record, partitionState, offset, timestamp);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		<span class="comment">// if the record is null, simply just update the offset state for partition</span></span><br><span class="line">		<span class="keyword">synchronized</span> (checkpointLock) &#123;</span><br><span class="line">			partitionState.setOffset(offset);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在设置了kafkaTimestampassigner之后就会进行一个定时任务向下游发送watermark，值为所有partition维护的最小值：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">long</span> minAcrossAll = Long.MAX_VALUE;</span><br><span class="line">	<span class="keyword">for</span> (KafkaTopicPartitionStateWithPeriodicWatermarks&lt;?, ?&gt; state : allPartitions) &#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// we access the current watermark for the periodic assigners under the state</span></span><br><span class="line">		<span class="comment">// lock, to prevent concurrent modification to any internal variables</span></span><br><span class="line">		<span class="keyword">final</span> <span class="keyword">long</span> curr;</span><br><span class="line">		<span class="comment">//noinspection SynchronizationOnLocalVariableOrMethodParameter</span></span><br><span class="line">		<span class="comment">// 这个锁是防止别的线程修改其他变量</span></span><br><span class="line">		<span class="keyword">synchronized</span> (state) &#123;</span><br><span class="line">			curr = state.getCurrentWatermarkTimestamp();</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		minAcrossAll = Math.min(minAcrossAll, curr);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// emit next watermark, if there is one</span></span><br><span class="line">	<span class="keyword">if</span> (minAcrossAll &gt; lastWatermarkTimestamp) &#123;</span><br><span class="line">		lastWatermarkTimestamp = minAcrossAll;</span><br><span class="line">		emitter.emitWatermark(<span class="keyword">new</span> Watermark(minAcrossAll));</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// schedule the next watermark</span></span><br><span class="line">	timerService.registerTimer(timerService.getCurrentProcessingTime() + interval, <span class="keyword">this</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="KafkaConsumerThread"><a href="#KafkaConsumerThread" class="headerlink" title="KafkaConsumerThread"></a>KafkaConsumerThread</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (records == <span class="keyword">null</span>) &#123;</span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					records = consumer.poll(pollTimeout);</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">catch</span> (WakeupException we) &#123;</span><br><span class="line">					<span class="keyword">continue</span>;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">try</span> &#123;</span><br><span class="line">				handover.produce(records);</span><br><span class="line">				records = <span class="keyword">null</span>;</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">catch</span> (Handover.WakeupException e) &#123;</span><br><span class="line">				<span class="comment">// fall through the loop</span></span><br><span class="line">			&#125;</span><br></pre></td></tr></table></figure><p>主要做的工作就是从consumer消费数据塞入handover，等待拉取</p><h3 id="Handover"><a href="#Handover" class="headerlink" title="Handover"></a>Handover</h3><h3 id="桥接模式"><a href="#桥接模式" class="headerlink" title="桥接模式"></a>桥接模式</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumerCallBridge</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">assignPartitions</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, List&lt;TopicPartition&gt; topicPartitions)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		consumer.assign(topicPartitions);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekPartitionToBeginning</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, TopicPartition partition)</span> </span>&#123;</span><br><span class="line">		consumer.seekToBeginning(partition);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekPartitionToEnd</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, TopicPartition partition)</span> </span>&#123;</span><br><span class="line">		consumer.seekToEnd(partition);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>解决08 09 10 版本的api不兼容问题</p><h2 id="Kafkaproducer10"><a href="#Kafkaproducer10" class="headerlink" title="Kafkaproducer10"></a>Kafkaproducer10</h2><p>###initializeState</p><p>什么都不做</p><h3 id="snapshot-1"><a href="#snapshot-1" class="headerlink" title="snapshot"></a>snapshot</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="comment">// check for asynchronous errors and fail the checkpoint if necessary</span></span><br><span class="line">	checkErroneous();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (flushOnCheckpoint) &#123;</span><br><span class="line">		<span class="comment">// flushing is activated: We need to wait until pendingRecords is 0</span></span><br><span class="line">		flush();</span><br><span class="line">		<span class="keyword">synchronized</span> (pendingRecordsLock) &#123;</span><br><span class="line">			<span class="keyword">if</span> (pendingRecords != <span class="number">0</span>) &#123;</span><br><span class="line">				<span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Pending record count must be zero at this point: "</span> + pendingRecords);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// if the flushed requests has errors, we should propagate it also and fail the checkpoint</span></span><br><span class="line">			checkErroneous();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里涉及到一个flushOnCheckPoint的问题，再调用<code>producer.flush</code>期间，producer会将所有没写入的，在buffer中的数据刷盘，然后调用commitCallBack，这就保证了ckpt之后数据不会丢的问题。</p><p>主要工作方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(IN next)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="comment">// propagate asynchronous errors</span></span><br><span class="line">	checkErroneous();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">byte</span>[] serializedKey = schema.serializeKey(next);</span><br><span class="line">	<span class="keyword">byte</span>[] serializedValue = schema.serializeValue(next);</span><br><span class="line">	<span class="comment">// 每条元素可以自己自己要写到的topic</span></span><br><span class="line">	String targetTopic = schema.getTargetTopic(next);</span><br><span class="line">	<span class="keyword">if</span> (targetTopic == <span class="keyword">null</span>) &#123;</span><br><span class="line">		targetTopic = defaultTopicId;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span>[] partitions = <span class="keyword">this</span>.topicPartitionsMap.get(targetTopic);</span><br><span class="line">	<span class="keyword">if</span>(<span class="keyword">null</span> == partitions) &#123;</span><br><span class="line">		partitions = getPartitionsByTopic(targetTopic, producer);</span><br><span class="line">		<span class="keyword">this</span>.topicPartitionsMap.put(targetTopic, partitions);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	ProducerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; record;</span><br><span class="line">	<span class="keyword">if</span> (flinkKafkaPartitioner == <span class="keyword">null</span>) &#123;</span><br><span class="line">		record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(targetTopic, serializedKey, serializedValue);</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(</span><br><span class="line">				targetTopic,</span><br><span class="line">				flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions),</span><br><span class="line">				serializedKey,</span><br><span class="line">				serializedValue);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> (flushOnCheckpoint) &#123;</span><br><span class="line">		<span class="keyword">synchronized</span> (pendingRecordsLock) &#123;</span><br><span class="line">			pendingRecords++;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	producer.send(record, callback);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><ol><li><p>kafka恢复状态直接从状态中去获取了之前保存的partition和offset，但是如果是扩容partition的场景就不会从新的Partition消费 issue:<a href="https://issues.apache.org/jira/projects/FLINK/issues/FLINK-8869?filter=reportedbyme" target="_blank" rel="noopener">FLINK-8869</a></p></li><li><p>flink内部维护了offset，为什么向kafka提交的时候还需要在checkpoint之后再提交而不是定时提交就算了？</p><p>因为虽然从checkpoint点恢复的时候不需要从kafka broker获取消费点的位置了，但是如果是应用重启消费上次消费到的点的数据，这个offset就是flink向kafka提交的，放在checkpoint完成后去做的好处就是让应用即使不是从上个点恢复的，也能够从kafka消费正确的offset点。</p></li><li><p>如果在新的checkpoint没打之前任务失败了，重新从上次的offset点消费的话下游数据是不是重复了?</p><p><strong>是的，因为有一部分数据经过处理已经sink出去了，因此才需要0.11的一致性语义</strong></p></li></ol><p><em>以上代码：<br>kafka-connector0.10 来源于release1.3.2<br>kafka-connector0.11 来源于release1.4.0</em></p></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>谢谢支持</div><button id="rewardButton" disable="enable" onclick='var e=document.getElementById("QR");"none"===e.style.display?e.style.display="block":e.style.display="none"'><span>Donate</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="/images/wechatpay.png" alt="aitozi WeChat Pay"><p>WeChat Pay</p></div></div></div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Flink/" rel="tag"># Flink</a> <a href="/tags/Kafka/" rel="tag"># Kafka</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/flink-cep-paper.html" rel="next" title="flink-cep-paper"><i class="fa fa-chevron-left"></i> flink-cep-paper</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/advance-maven-shade-plugin.html" rel="prev" title="maven-shade-plugin插件高级用法">maven-shade-plugin插件高级用法 <i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview">站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="aitozi"><p class="site-author-name" itemprop="name">aitozi</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">31</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">10</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">19</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="gjying1314@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-globe"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://wuchong.me/" title="wuchong" target="_blank">wuchong</a></li><li class="links-of-blogroll-item"><a href="http://chenyuzhao.me/" title="yuzhao" target="_blank">yuzhao</a></li><li class="links-of-blogroll-item"><a href="http://blog.csdn.net/yanghua_kobe?viewmode=contents" title="vinoyang" target="_blank">vinoyang</a></li><li class="links-of-blogroll-item"><a href="http://blog.csdn.net/lmalds?viewmode=contents" title="Imalds" target="_blank">Imalds</a></li><li class="links-of-blogroll-item"><a href="http://blog.csdn.net/androidlushangderen" title="hadoop" target="_blank">hadoop</a></li><li class="links-of-blogroll-item"><a href="http://www.cnblogs.com/xrq730/p/5260294.html" title="java开发" target="_blank">java开发</a></li><li class="links-of-blogroll-item"><a href="http://www.hollischuang.com/" title="阿里工程师" target="_blank">阿里工程师</a></li><li class="links-of-blogroll-item"><a href="http://www.cnblogs.com/fxjwind/" title="阿里流计算工程师" target="_blank">阿里流计算工程师</a></li><li class="links-of-blogroll-item"><a href="http://jm.taobao.org/" title="阿里中间件博客" target="_blank">阿里中间件博客</a></li><li class="links-of-blogroll-item"><a href="http://armsword.com/" title="duruofei" target="_blank">duruofei</a></li><li class="links-of-blogroll-item"><a href="http://blog.yufeng.info/" title="褚霸" target="_blank">褚霸</a></li><li class="links-of-blogroll-item"><a href="https://yuzhouwan.com" title="宇宙湾" target="_blank">宇宙湾</a></li><li class="links-of-blogroll-item"><a href="http://matt33.com" title="matt" target="_blank">matt</a></li><li class="links-of-blogroll-item"><a href="http://coding-geek.com/" title="coding-geek" target="_blank">coding-geek</a></li></ul></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#引言"><span class="nav-number">1.</span> <span class="nav-text">引言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Kafka-Connector10"><span class="nav-number">2.</span> <span class="nav-text">Kafka-Connector10</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#kafkaConsumer10"><span class="nav-number">2.1.</span> <span class="nav-text">kafkaConsumer10</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FlinkKafkaConsumerBase"><span class="nav-number">2.1.1.</span> <span class="nav-text">FlinkKafkaConsumerBase</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#initializeState"><span class="nav-number">2.1.2.</span> <span class="nav-text">initializeState</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#snapshot"><span class="nav-number">2.1.3.</span> <span class="nav-text">snapshot</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#offsetCommitMode"><span class="nav-number">2.1.4.</span> <span class="nav-text">offsetCommitMode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#消费partition分配问题"><span class="nav-number">2.1.5.</span> <span class="nav-text">消费partition分配问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#消费模型kafka10"><span class="nav-number">2.1.6.</span> <span class="nav-text">消费模型kafka10</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KafkaFetcher"><span class="nav-number">2.1.7.</span> <span class="nav-text">KafkaFetcher</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KafkaConsumerThread"><span class="nav-number">2.1.8.</span> <span class="nav-text">KafkaConsumerThread</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Handover"><span class="nav-number">2.1.9.</span> <span class="nav-text">Handover</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#桥接模式"><span class="nav-number">2.1.10.</span> <span class="nav-text">桥接模式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafkaproducer10"><span class="nav-number">2.2.</span> <span class="nav-text">Kafkaproducer10</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#snapshot-1"><span class="nav-number">2.2.1.</span> <span class="nav-text">snapshot</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#问题"><span class="nav-number">3.</span> <span class="nav-text">问题</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">aitozi</span></div><div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div><div class="theme-info">主题 - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user"></i> <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="site-pv"><i class="fa fa-eye"></i> <span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/jquery.lazyload/1.9.3/jquery.lazyload.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.ui.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><script id="dsq-count-scr" src="https://aitozi.disqus.com/count.js" async></script><script type="text/javascript">var disqus_config=function(){this.page.url="http://www.aitozi.com/flink-kafka-exactly-once.html",this.page.identifier="flink-kafka-exactly-once.html",this.page.title="上篇·flink 1.4利用kafka0.11实现完整的一致性语义"},d=document,s=d.createElement("script");s.src="https://aitozi.disqus.com/embed.js",s.setAttribute("data-timestamp",""+ +new Date),(d.head||d.body).appendChild(s)</script><script>!function(){var t=document.createElement("script"),s=window.location.protocol.split(":")[0];"https"===s?t.src="https://zz.bdstatic.com/linksubmit/push.js":t.src="http://push.zhanzhang.baidu.com/push.js";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script></body></html><!-- rebuild by neat -->