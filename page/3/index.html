<!-- build time:Wed Jan 22 2020 17:55:48 GMT+0800 (China Standard Time) --><!DOCTYPE html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="google-site-verification" content="DO25iswIsaKZ5NZbYreVDjWtBKTyo1yFAROjSRcJD64"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet" type="text/css"><link href="//fonts.loli.net/css?family=Lato:300,300italic,400,400italic,700,700italic|consolas:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="//cdn.bootcss.com/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Aitozi" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2"><meta name="description" content="What makes a difference? Persistence!"><meta property="og:type" content="website"><meta property="og:title" content="Aitozi"><meta property="og:url" content="http://www.aitozi.com/page/3/index.html"><meta property="og:site_name" content="Aitozi"><meta property="og:description" content="What makes a difference? Persistence!"><meta property="og:locale" content="zh-Hans"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Aitozi"><meta name="twitter:description" content="What makes a difference? Persistence!"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post",offset:12,offset_float:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:!0,duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://www.aitozi.com/page/3/"><title>Aitozi</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Aitozi</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description"></h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/java-serialization.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/java-serialization.html" itemprop="url">Java序列化拾掇</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-28T00:51:18+08:00">2018-07-28 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/刨根问底/" itemprop="url" rel="index"><span itemprop="name">刨根问底</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/java-serialization.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="java-serialization.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">2.8k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">11</span></div></div></header><div class="post-body" itemprop="articleBody"><p>Java序列化拾掇</p><p>&lt;!-- more --&gt;</p><blockquote><p>本来想总结一下对google protobuf的用法总结，然而搜资料的过程中发现对很多java序列化的知识不足，故做了一些拾掇，在flink中关于类型序列化的地方其实也涉及很多，待以后看到，如有新的思考再来补充。</p></blockquote><h2>java序列化</h2><ol><li>在Java中，只要一个类实现了java.io.Serializable接口，那么它就可以被序列化</li><li>若父类未实现Serializable,而子类序列化了，父类属性值不会被保存，反序列化后父类属性值丢失</li><li>通过ObjectOutputStream和ObjectInputStream对对象进行序列化及反序列化</li><li>在deserialized的时候类的构造器是不会被调用的，只会调用没有实现Serializabe接口的父类的无参构造方法，如果其父类不可序列化，并且没有无参构造函数就会导致<code>InvalidClassException</code></li><li>只有non-static的成员并且没有标记为transient才会被序列化</li><li>类所包含的成员变量也必须是可序列化的</li><li>transient 关键字的作用是控制变量的序列化，在变量声明前加上该关键字，可以阻止该变量被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 型的是 0，对象型的是 null</li><li>java的serialVersionUID用来表明类的不同版本间的兼容性，必须被定义成final static long才能生效，否则会报错</li><li>在使用Externalizable进行序列化的时候，在读取对象时，会调用被序列化类的无参构造器去创建一个新的对象，然后再将被保存对象的字段的值分别填充到新对象中。所以，实现Externalizable接口的类必须要提供一个public的无参的构造器，如果一个Java类没有定义任何构造函数，编译器会帮我们自动添加一个无参的构造方法，可是，如果我们在类中定义了一个有参数的构造方法了，编译器便不会再帮我们创建无参构造方法，这点需要注意</li><li>用户自定义的 writeObject 和 readObject 方法可以允许用户控制序列化的过程</li></ol><h3>serialVersionUID</h3><p>Java的序列化机制是通过在运行时判断类的serialVersionUID来验证版本一致性的。在进行反序列化时，JVM会把传来 的字节流中的serialVersionUID与本地相应实体（类）的serialVersionUID进行比较，如果相同就认为是一致的，可以进行反序 列化，否则就会出现序列化版本不一致的异常。</p><p>当实现java.io.Serializable接口的实体（类）没有显式地定义一个名为serialVersionUID，类型为long的变 量时，Java序列化机制会根据编译的class自动生成一个serialVersionUID作序列化版本比较用，这种情况下，只有同一次编译生成的 class才会生成相同的serialVersionUID 。</p><p>如果我们不希望通过编译来强制划分软件版本，即实现序列化接口的实体能够兼容先前版本，未作更改的类，就需要显式地定义一个名为serialVersionUID，类型为long的变量，不修改这个变量值的序列化实体都可以相互进行串行化和反串行化。</p><h3>在内部类使用中带来的困扰</h3><blockquote><p>A class that is serializable with an enclosing class that is not serializable causes serialization to fail. Non-static nested classes that implement Serializable must be defined in an enclosing class that is also serializable. Non-static nested classes retain an implicit reference to an instance of their enclosing class. If the enclosing class is not serializable, the Java serialization mechanism fails with a java.io.NotSerializableException.</p></blockquote><p>一个非静态的内部类实现了Serializable接口，要求其外部类也同样实现Serializable接口。这是因为一个非静态内部类包含有一个隐式的指向外部包装类实例对象的一个指针，如上面指出的规则，序列化的时候要求类的非静态成员也需要是可序列化的，如果外部类没有声明Serializable，java序列化机制就会报错，解法通常是</p><ul><li>将内部类声明为static，这样就不包含隐式指针了</li><li>将外部类声明为Serializable</li></ul><p>在flink中采用了另一种解法，用户通过匿名内部类来定义一个userFuntion，通常userFunction需要被序列化来分发到各个task节点来执行，定义成static不如匿名类方便，外部主类定义成Serializable的代价又比较大，因此采用另一种解法：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">for (Field f: cls.getDeclaredFields()) &#123;</span><br><span class="line">	if (f.getName().startsWith(&quot;this$&quot;)) &#123;</span><br><span class="line">		// found a closure referencing field - now try to clean</span><br><span class="line">		closureAccessed |= cleanThis0(func, cls, f.getName());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private static boolean cleanThis0(Object func, Class&lt;?&gt; cls, String this0Name) &#123;</span><br><span class="line"></span><br><span class="line">	This0AccessFinder this0Finder = new This0AccessFinder(this0Name);</span><br><span class="line">	getClassReader(cls).accept(this0Finder, 0);</span><br><span class="line"></span><br><span class="line">	final boolean accessesClosure = this0Finder.isThis0Accessed();</span><br><span class="line"></span><br><span class="line">	if (LOG.isDebugEnabled()) &#123;</span><br><span class="line">		LOG.debug(this0Name + &quot; is accessed: &quot; + accessesClosure);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (!accessesClosure) &#123;</span><br><span class="line">		Field this0;</span><br><span class="line">		try &#123;</span><br><span class="line">			this0 = func.getClass().getDeclaredField(this0Name);</span><br><span class="line">		&#125; catch (NoSuchFieldException e) &#123;</span><br><span class="line">			// has no this$0, just return</span><br><span class="line">			throw new RuntimeException(&quot;Could not set &quot; + this0Name + &quot;: &quot; + e);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		try &#123;</span><br><span class="line">			this0.setAccessible(true);</span><br><span class="line">			this0.set(func, null);</span><br><span class="line">		&#125;</span><br><span class="line">		catch (Exception e) &#123;</span><br><span class="line">			// should not happen, since we use setAccessible</span><br><span class="line">			throw new RuntimeException(&quot;Could not set &quot; + this0Name + &quot; to null. &quot; + e.getMessage(), e);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	return accessesClosure;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>对每一个userFunction(可能实现自一个匿名内部类)有一个clean的机制。</p><ul><li>检查其声明字段有没有<code>this$</code>开始的，即指向外部类的引用</li><li>如果有将对应的字段通过反射置成null,这样就不会受第三条规则的困扰了</li></ul><h3>自定义序列化</h3><p>在序列化过程中，如果被序列化的类中定义了writeObject 和 readObject 方法，虚拟机会试图调用对象类里的 writeObject 和 readObject 方法，进行用户自定义的序列化和反序列化。并且这两个方法的signature必须是以下这样才会生效，否则就是默认的序列化方式</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">private void readObject(java.io.ObjectInputStream in)</span><br><span class="line">     throws IOException, ClassNotFoundException;</span><br><span class="line">private void writeObject(java.io.ObjectOutputStream out)</span><br><span class="line">     throws IOException;</span><br></pre></td></tr></table></figure><p></p><p>如果没有这样的方法，则默认调用是 ObjectOutputStream 的 defaultWriteObject 方法以及 ObjectInputStream 的 defaultReadObject 方法。这两个方法没有覆写，也没有被显式调用，为什么会生效呢？ 具体可以参见<a href="https://mp.weixin.qq.com/s/ABtxdNpr4bLpXtFiOK47hA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/ABtxdNpr4bLpXtFiOK47hA</a></p><blockquote><p>在使用ObjectOutputStream的writeObject方法和ObjectInputStream的readObject方法时，会通过反射的方式调用</p></blockquote><p>关于序列化的困惑可以在这个源码中得到解答</p><blockquote><p>ObjectOutputStream#writeObject</p><p>writeObject =&gt; writeObject0 =&gt; writeOrdinaryObject =&gt; writeSerialData =&gt; invokeWriteObject</p></blockquote><p>可以参见ArrayList使用了这种自定义序列化的方法，ArrayList实际上是动态数组，每次在放满以后自动增长设定的长度值，如果数组自动增长长度设为100，而实际只放了一个元素，那就会序列化99个null元素。为了保证在序列化的时候不会将这么多null同时进行序列化，ArrayList把元素数组设置为transient。</p><p><em>Ps：在两个方法的开始处，你会发现调用了defaultWriteObject()和defaultReadObject()。它们做的是默认的序列化进程，就像写/读所有的non-transient和 non-static字段(但他们不会去做serialVersionUID的检查).通常说来，所有我们想要自己处理的字段都应该声明为transient。这样的话，defaultWriteObject/defaultReadObject便可以专注于其余字段，而我们则可为这些特定的字段(译者：指transient)定制序列化。使用那两个默认的方法并不是强制的，而是给予了处理复杂应用时更多的灵活性</em></p><h3>关于反序列化的时候构造方法是否会被调用（反序列化是怎么做的）</h3><blockquote><p>A non-serializable, immediate superclass of a serializable class that does not itself declare an accessible, no-argument constructor causes deserialization to fail To allow subtypes of non-serializable classes to be serialized, the subtype may assume responsibility for saving and restoring the state of the supertype's public, protected, and (if accessible) package fields. The subtype may assume this responsibility only if the class it extends has an accessible no-arg constructor to initialize the class's state. It is an error to declare a class Serializable if this is not the case. The error will be detected at runtime.</p></blockquote><p>反序列化其实是将先前序列化生成的byte流重新构建成一个对象，byte流包含了所有重构对象的信息，包括class的元数据，实例的变量的类型信息，以及相应的值。然后在反序列化的时候它要求<strong>all the parent classes of instance should be Serializable; and if any super class in hirarchy is not Serializable then it must have a default constructor</strong>。在反序列化的时候会一直搜寻其父类，直到找到第一个不可序列化的类，就尝试调用其无参构造函数创建对象，如果所有的父类都是可序列化的，最终找到的就是Object类，然后首先创建一个Object对象。接着JVM就继续读取byte流，设置相关的类型信息，一个空对象创建完成后，jvm就设置相关的static字段，并调用<code>readObject</code>方法进行赋值</p><p>由于本类的构造方法不会被调用，所以你期望某个变量的初始化在构造方法中完成得到的只会是null。</p><p>在构造函数的调用上<code>Externalizable</code>和<code>Serializable</code>的表现不同，Externalizable依赖于本身类的无参构造函数</p><h3>利用序列化来做deepCopy</h3><p>主要用到了<code>ByteArrayOutputStream</code>,存储在内存中，不做持久化</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public SerializableClass deepCopy() throws Exception&#123;</span><br><span class="line">    //Serialization of object</span><br><span class="line">    ByteArrayOutputStream bos = new ByteArrayOutputStream();</span><br><span class="line">    ObjectOutputStream out = new ObjectOutputStream(bos);</span><br><span class="line">    out.writeObject(this);</span><br><span class="line"></span><br><span class="line">    //De-serialization of object</span><br><span class="line">    ByteArrayInputStream bis = new   ByteArrayInputStream(bos.toByteArray());</span><br><span class="line">    ObjectInputStream in = new ObjectInputStream(bis);</span><br><span class="line">    SerializableClass copied = (SerializableClass) in.readObject();</span><br><span class="line">    return copied;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>更多关于java clone 可参考:</p><p><a href="https://howtodoinjava.com/core-java/cloning/a-guide-to-object-cloning-in-java/" target="_blank" rel="noopener">https://howtodoinjava.com/core-java/cloning/a-guide-to-object-cloning-in-java/</a></p><h3>如果对象状态需要同步，则对象序列化也需要同步</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">private synchronized void writeObject(ObjectOutputStream s) throws IOException &#123;</span><br><span class="line">        s.defaultWriteObject();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>单例模式序列化</h3><p>参考：</p><ul><li><a href="http://www.hollischuang.com/archives/1144" target="_blank" rel="noopener">http://www.hollischuang.com/archives/1144</a></li><li>枚举实现可序列化单例 <a href="http://www.cnblogs.com/cielosun/p/6596475.html" target="_blank" rel="noopener">http://www.cnblogs.com/cielosun/p/6596475.html</a></li><li><a href="https://leokongwq.github.io/2017/08/21/why-enum-singleton-are-serialization-safe.html" target="_blank" rel="noopener">https://leokongwq.github.io/2017/08/21/why-enum-singleton-are-serialization-safe.html</a></li></ul><h3>在不同的classloader之间进行对象的序列化和反序列化</h3><p>如上所说，在同一个classloader中，利用如下的方法serializabale和deserializable对象：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ByteArrayOutputStream bo=new ByteArrayOutputStream();</span><br><span class="line">ObjectOutputStream oo=new ObjectOutputStream(bo);</span><br><span class="line">oo.writeObject(outObject);</span><br><span class="line">ByteArrayInputStream bi=new ByteArrayInputStream(bo.toByteArray());</span><br><span class="line">ObjectInputStream oi=new ObjectInputStream(bi);</span><br><span class="line">Object inObject = oi.readObject();</span><br></pre></td></tr></table></figure><p></p><p>当序列化的对象和反序列化的对象不在同一个classloader中时，以上的代码执行时，就会报无法把属性付给对象的错误，此时应当，通过设置反序列化得classloader，来解决这个问题。</p><p>首先，从ObjectInputStream继承一个自己的ObjectInputStream</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">public class CustomObjectInputStream extends ObjectInputStream &#123;</span><br><span class="line"></span><br><span class="line">    protected ClassLoader classLoader = this.getClass().getClassLoader();</span><br><span class="line"></span><br><span class="line">    public CustomObjectInputStream(InputStream in) throws IOException &#123;</span><br><span class="line">        super(in);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public CustomObjectInputStream(InputStream in, ClassLoader cl)</span><br><span class="line">            throws IOException &#123;</span><br><span class="line">        super(in);</span><br><span class="line">        this.classLoader = cl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected Class&lt;?&gt; resolveClass(ObjectStreamClass desc) throws IOException,</span><br><span class="line">            ClassNotFoundException &#123;</span><br><span class="line">        // TODO Auto-generated method stub</span><br><span class="line">        String name = desc.getName();</span><br><span class="line">        try &#123;</span><br><span class="line">            return Class.forName(name, false, this.classLoader);</span><br><span class="line">        &#125; catch (ClassNotFoundException ex) &#123;</span><br><span class="line">            return super.resolveClass(desc);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>比较重要的是这里的resolveClass方法传入classloader,反序列化时将classloader传入</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> ByteArrayOutputStream bo=new ByteArrayOutputStream();</span><br><span class="line">ObjectOutputStream oo=new ObjectOutputStream(bo);</span><br><span class="line">oo.writeObject(outObject);</span><br><span class="line">ByteArrayInputStream bi=new ByteArrayInputStream(bo.toByteArray());</span><br><span class="line">CustomObjectInputStream oi=new CustomObjectInputStream(bi, outObject.getClass().getClassLoader());</span><br><span class="line">Object = oi.readObject();</span><br><span class="line">// flink源码中也有多次类似的使用userClassloader和FlinkClassLoader的切换</span><br><span class="line">https://issues.apache.org/jira/browse/FLINK-9122 这个bug曾经就是因为这个原因引起的</span><br></pre></td></tr></table></figure><p></p><h3>序列化相关方法</h3><p>writeObject、readObject、readObjectNoData、writeReplace和readResolve 待补充</p><h2>参考</h2><ul><li><a href="https://help.semmle.com/wiki/display/JAVA/Serializable+inner+class+of+non-serializable+class," title="Serializable inner class of non-serializable class" target="_blank" rel="noopener">Serializable inner class of non-serializable class</a></li><li><a href="https://mp.weixin.qq.com/s/-D9N9_9IDqSbuIjuADJ7ZA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/-D9N9_9IDqSbuIjuADJ7ZA</a></li><li><a href="https://howtodoinjava.com/core-java/serialization/how-deserialization-process-happen-in-java/" target="_blank" rel="noopener">https://howtodoinjava.com/core-java/serialization/how-deserialization-process-happen-in-java/</a></li><li><a href="https://www.quora.com/Why-are-enum-singleton-serialization-safe" target="_blank" rel="noopener">https://www.quora.com/Why-are-enum-singleton-serialization-safe</a></li></ul></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-jobgraph-generate.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/flink-jobgraph-generate.html" itemprop="url">flink中jobgraph的生成逻辑</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-26T18:43:23+08:00">2018-05-26 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/源码分析/" itemprop="url" rel="index"><span itemprop="name">源码分析</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-jobgraph-generate.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-jobgraph-generate.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">247 </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">1</span></div></div></header><div class="post-body" itemprop="articleBody"><p>flink中jobgraph的生成逻辑，接前面的文章<a href="http://aitozi.com/2018/04/11/flink-streamGraph/" target="_blank" rel="noopener">flink图流转之StreamGraph</a></p><p>&lt;!-- more --&gt;</p><blockquote><p>今天想了一下源码分析类的文章应该是直接在源码上做注释来的直接，然后再抛开细节概括总体流程和关键点，今天这篇分析jobgraph生成的文章就以这个形式展开</p></blockquote><ol><li>先生成各个节点streamnode的hash值 主体代码在：<code>StreamGraphHasherV2.java</code></li><li>设置chaining<ul><li>找到节点中能chain和不能chain的边</li><li>生成相应的JobVertex节点，并设置StreamConfig（资源，名称，chain的节点），这个streamConfig是在部署期间比较重要的一个配置项，并拼接物理执行顺序，主要在connect函数</li></ul></li><li>设置inEdges配置项</li><li>设置slotsharingGroup</li><li>配置checkpoint，这里主要设置需要发送barrier的节点即source节点</li></ol><p><em>其实总结就是分两步：</em></p><ol><li>在createChain过程中创建JobVertex</li><li>设置各个StreamConfig需要的信息用作生成物理执行图的时候使用</li></ol><p>主要涉及的代码为两块，如下：</p><p><script src="//gist.github.com/2e021e923ed394155b191853e0975a71.js?file=StreamGraphHasherV2.java"></script><script src="//gist.github.com/2e021e923ed394155b191853e0975a71.js?file=StreamingJobGraphGenerator.java"></script></p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-cep-code.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/flink-cep-code.html" itemprop="url">flink cep源码分析</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-25T21:54:23+08:00">2018-05-25 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/源码分析/" itemprop="url" rel="index"><span itemprop="name">源码分析</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-cep-code.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-cep-code.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">1.7k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">6</span></div></div></header><div class="post-body" itemprop="articleBody"><p>关于Flink中cep实现原理的分析</p><p>&lt;!-- more --&gt;</p><blockquote><p>最近一直在搞动态cep的事情，有点焦头烂额很久没有更新博客了。其实有时候也在思考写博客的意义，因为写博客也是时间成本很高的一件事，如果得不到相应的收益其实是划不来的。那么写博客的收益到底是什么呢？两点：笔记记录和传播知识的作用，整理记录的功能一个web博客不会强于一个终端笔记例如：为知笔记。所以博客的真正意义在于传播知识观点。有时候你遇到一个百思不得其解的问题的时候，google一下找到一篇博客，竟然能够解答心中所惑的时候你是不是心中会很感谢博主呢，我认为这样的一篇文章就是有价值的文章，所以我希望我也能做好这样一件有价值的事情。</p></blockquote><h2>引言</h2><p>好了，进入本文的主题flink cep原理的深入理解，很多人可能还不知道flink cep是什么，flink cep其实实现自一篇论文，具体论文细节见我之前的一篇文章的分享<a href="http://aitozi.com/2018/02/25/flink-cep-paper/" target="_blank" rel="noopener">flink-cep-paper</a>. flink cep的全称是Complex Event Processing，在我看来它主要能做的是在一个连续不断的事件中提取出用户所关心的事件序列，他和flink的filter算子的区别在于filter只能去实现单个元素的过滤，而cep是能完成先后顺序事件的过滤。下面让我们来走进他的源码实现原理吧。以下代码基于社区1.4.2分支分析。</p><p>我们的文章以一系列问题来展开：</p><ol><li>用户定义的Pattern最后会以什么形式工作</li><li>当CEP Operator获取到上游一个算子的时候会做什么事情？</li><li>在ProcessingTime和Eventtime的语义下处理逻辑有什么不同点？</li><li>匹配成功的元素如何存储，状态机转化流程是怎么样的？</li><li>超时未匹配成功的元素会做什么？</li></ol><h2>问题一</h2><p>用户在定义Pattern和condition之后，会通过NFAcompiler将Pattern翻译成一个一个相关联的State，表明了这一组规则的状态机的走向流程。</p><p>State包括<code>Start、Normal、Final、Stop</code>，start表示一个起始状态例如<code>begin('A').followedBy('B')</code> 这里面A就是一个Start状态。Final状态表示整个序列已经匹配完成可以向下游发送了，Stop状态是用来处理否定类型的规则，一旦到达Stop状态即意味着整个匹配过程失败。各个状态之间通过<code>StateTransition</code>来连接，连接方式有：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ignore: 忽略此次匹配的元素</span><br><span class="line">proceed: 相当于forward的意思，到达下一个状态，不存储元素，继续做下一个状态的condition判断</span><br><span class="line">take： 存储本次的元素</span><br></pre></td></tr></table></figure><p></p><p>这是一段创建中间状态的代码</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">private State&lt;T&gt; createMiddleStates(final State&lt;T&gt; sinkState) &#123;</span><br><span class="line">			State&lt;T&gt; lastSink = sinkState;</span><br><span class="line">			// 不断往上遍历pattern进行state的生成</span><br><span class="line">			while (currentPattern.getPrevious() != null) &#123;</span><br><span class="line"></span><br><span class="line">				if (currentPattern.getQuantifier().getConsumingStrategy() == Quantifier.ConsumingStrategy.NOT_FOLLOW) &#123;</span><br><span class="line">					//skip notFollow patterns, they are converted into edge conditions</span><br><span class="line">				&#125; else if (currentPattern.getQuantifier().getConsumingStrategy() == Quantifier.ConsumingStrategy.NOT_NEXT) &#123;</span><br><span class="line">					final State&lt;T&gt; notNext = createState(currentPattern.getName(), State.StateType.Normal);</span><br><span class="line">					final IterativeCondition&lt;T&gt; notCondition = getTakeCondition(currentPattern);</span><br><span class="line">					// 否定类型的pattern需要创建一个stop state</span><br><span class="line">					final State&lt;T&gt; stopState = createStopState(notCondition, currentPattern.getName());</span><br><span class="line"></span><br><span class="line">					if (lastSink.isFinal()) &#123;</span><br><span class="line">						//so that the proceed to final is not fired</span><br><span class="line">						结尾状态不用proceed过去做下一次计算了，可以直接ignore到Final，然后输出结果</span><br><span class="line">						notNext.addIgnore(lastSink, new NotCondition&lt;&gt;(notCondition));</span><br><span class="line">					&#125; else &#123;</span><br><span class="line">						notNext.addProceed(lastSink, new NotCondition&lt;&gt;(notCondition));</span><br><span class="line">					&#125;</span><br><span class="line">					// 在满足Not_NEXT的条件的时候就转化成stop状态即匹配失败</span><br><span class="line">					notNext.addProceed(stopState, notCondition);</span><br><span class="line">					lastSink = notNext;</span><br><span class="line">				&#125; else &#123;</span><br><span class="line">					// 非否定类型的状态的处理逻辑都在这个方法中</span><br><span class="line">					lastSink = convertPattern(lastSink);</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				// we traverse the pattern graph backwards</span><br><span class="line">				followingPattern = currentPattern;</span><br><span class="line">				currentPattern = currentPattern.getPrevious();</span><br><span class="line"></span><br><span class="line">				final Time currentWindowTime = currentPattern.getWindowTime();</span><br><span class="line">				if (currentWindowTime != null &amp;&amp; currentWindowTime.toMilliseconds() &lt; windowTime) &#123;</span><br><span class="line">					// the window time is the global minimum of all window times of each state</span><br><span class="line">					windowTime = currentWindowTime.toMilliseconds();</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">			return lastSink;</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure><p></p><p>生成这样的state列表之后，最终会创建一个NFA，一个NFA中包含了两个重要组件： 一个是SharedBuffer用于存储中间匹配命中的数据，这是一个基于论文实现的带版本的内存共享，主要解决的事情是在同一个元素触发多个分支的时候避免存储多次。 另一个是ComputationState队列表示的是一系列当前匹配到的计算状态，每一个状态在拿到下一个元素的时候都会根据condition判断自己是能够继续往下匹配生成下一个computation state还是匹配失败。</p><h2>问题二、三</h2><p>问题二和问题三一起解释，在消费到上游一个元素之后会判断时间语义，这里主要是为了处理乱序问题，如果是processingtime的话就会直接经由nfa#process进行处理，因为processing time不需要考虑事件是否乱序，他给每个事件都打上了当前的时间戳。而event语义下，会先将该数据buffer到rocksdb中，并且注册一个比当前时间戳大1的eventimer，用以触发真正的计算，也就是说，eventtime其实是每毫秒获取过去存储的数据做一次匹配计算。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">protected void saveRegisterWatermarkTimer() &#123;</span><br><span class="line">	long currentWatermark = timerService.currentWatermark();</span><br><span class="line">	// protect against overflow</span><br><span class="line">	if (currentWatermark + 1 &gt; currentWatermark) &#123;</span><br><span class="line">		timerService.registerEventTimeTimer(VoidNamespace.INSTANCE, currentWatermark + 1);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h2>问题四、五</h2><p>nfa#process做了什么？取出前面说到的nfa中所有的当前computationState去做计算，当然计算之前会先判断时间和computation的starttime比较匹配是否超出时间，即within算子所做的时间，如果设置了超时处理的方式，就会将超时未匹配完成，已匹配到的部分元素向下游发送，并做sharebuffer的清理工作</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">if (!computationState.isStartState() &amp;&amp;</span><br><span class="line">	windowTime &gt; 0L &amp;&amp;</span><br><span class="line">	timestamp - computationState.getStartTimestamp() &gt;= windowTime) &#123;</span><br><span class="line"></span><br><span class="line">	if (handleTimeout) &#123;</span><br><span class="line">		// extract the timed out event pattern</span><br><span class="line">		Map&lt;String, List&lt;T&gt;&gt; timedOutPattern = extractCurrentMatches(computationState);</span><br><span class="line">		timeoutResult.add(Tuple2.of(timedOutPattern, timestamp));</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	eventSharedBuffer.release(</span><br><span class="line">			NFAStateNameHandler.getOriginalNameFromInternal(computationState.getPreviousState().getName()),</span><br><span class="line">			computationState.getEvent(),</span><br><span class="line">			computationState.getTimestamp(),</span><br><span class="line">			computationState.getCounter());</span><br><span class="line"></span><br><span class="line">	newComputationStates = Collections.emptyList();</span><br><span class="line">	nfaChanged = true;</span><br><span class="line">&#125; else if (event != null) &#123;</span><br><span class="line">   // 在computeNextState的时候判断成功的take条件会将元素put到eventSharedBuffer中</span><br><span class="line">	newComputationStates = computeNextStates(computationState, event, timestamp);</span><br><span class="line"></span><br><span class="line">	if (newComputationStates.size() != 1) &#123;</span><br><span class="line">		nfaChanged = true;</span><br><span class="line">	&#125; else if (!newComputationStates.iterator().next().equals(computationState)) &#123;</span><br><span class="line">		nfaChanged = true;</span><br><span class="line">	&#125;</span><br><span class="line">&#125; else &#123;</span><br><span class="line">	newComputationStates = Collections.singleton(computationState);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>在完成匹配之后达到final状态将数据提取出来向下游发送完成匹配。</p><p>以上便是cep的大致原理，说白了其实这个就是基于flink runntime开发出来的一个衍生lib，flink runtime其实是一个分布式的阻塞队列，通过这个概念可以在上面开发出很多有意思的产品，cep就是其中一个。 分析结束，欢迎拍砖~</p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-streamGraph.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/flink-streamGraph.html" itemprop="url">flink图流转之StreamGraph</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-11T07:13:58+08:00">2018-04-11 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/源码分析/" itemprop="url" rel="index"><span itemprop="name">源码分析</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-streamGraph.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-streamGraph.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">1.5k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">7</span></div></div></header><div class="post-body" itemprop="articleBody"><p>flink DAG图流转分析 &lt;!-- more --&gt;</p><h2>前言</h2><p>每次我们编写完flink作业，跑任务的时候都会在flink-ui上展示一个作业的DAG图，那么这个图是如何形成的呢？本文就和你一起来揭开flink执行图生成的神秘面纱~</p><p>##总览 在flink中的执行图可以分为4层StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图。</p><ul><li>StreamGraph：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。</li><li>JobGraph：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。</li><li>ExecutionGraph：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。</li><li>物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</li></ul><p>今天我们就来看下streamGraph的生成</p><h2>StreamGraph的生成</h2><h3>组件</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">StreamGraph：根据用户通过 Stream API 编写的代码生成的最初的图。</span><br><span class="line">StreamNode：用来代表 operator 的类，并具有所有相关的属性，如并发度、入边和出边等。</span><br><span class="line">StreamEdge：表示连接两个StreamNode的边。</span><br></pre></td></tr></table></figure><p></p><p>flink任务从定义一个运行环境开始<code>streamExecutionEnvironment</code>，流计算任务起始于addSource,我们来看这个函数，addSource之后生成了一个<code>DataStream</code>. <code>DataStream</code>的构造函数参数接收一个<code>StreamTransformation</code>类型的对象，这个对象反映了流之间的转换操作。但是这个transformation和<code>operation</code>不是一一对应的。一些分区操作：union，split/select，partition只是逻辑概念，并不会在最后的dag图上显示出来。 在生成datastream之后，经历<code>DataStream.java</code>中定义的一些api算子，完成业务逻辑的定义，在这之中可能包含以下的转化：</p><p>假设一个场景：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">addsource -&gt; map -&gt; filter -&gt; connect -&gt; flatmap </span><br><span class="line">-&gt; keyby -&gt; window -&gt; apply -&gt; addSink -&gt; excute</span><br></pre></td></tr></table></figure><p></p><ol><li>addSource创建生成一个<code>SingleOutputStreamOperator</code> 本质上是一个带有transformation=&quot;SourceTransformation&quot;的datastream</li><li>map创建生成一个<code>OneInputTransformation</code> 并调用getExecutionEnvironment().addOperator(resultTransform)将其添加入env的<code>List&lt;StreamTransformation&lt;?&gt;&gt;</code>中</li><li>filter 通过 map相同操作</li><li>connect 直接返回一个<code>ConnectedStreams</code>不是<code>Datastream</code>的子类</li><li>flatmap 生成一个<code>TwoInputTransformation</code>将其添加入env的<code>List&lt;StreamTransformation&lt;?&gt;&gt;</code>中，并返回一个<code>SingleOutputStreamOperator</code>,并且返回的Datastream中包含的是当前这个transformation</li><li>keyby 生成一个keyedStream，这里直接生成一个<code>PartitionTransformation</code> 替代了父类<code>DataStream</code>中的transformation</li><li>window 生成windowStream</li><li>apply调用将生成一个<code>OneInputTransformation</code>,增加至<code>List&lt;StreamTransformation&lt;?&gt;&gt;</code></li><li>addSink 获取 <code>SinkTransformation</code></li></ol><p>其中每一次创建<code>OneInputTransformation</code>都是基于Datastream的当前的transformation来创建的，也就是说keyby之后的PartitionTransformation信息也加入了.</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">new OneInputTransformation&lt;&gt;(</span><br><span class="line">			this.transformation,</span><br><span class="line">			operatorName,</span><br><span class="line">			operator,</span><br><span class="line">			outTypeInfo,</span><br><span class="line">			environment.getParallelism());</span><br></pre></td></tr></table></figure><p></p><p>好了到这里已经获取了各个流程的streamtransformation,最后调用execute方法，截取了流式环境下的实现：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public JobExecutionResult execute(String jobName) throws Exception &#123;</span><br><span class="line">	Preconditions.checkNotNull(&quot;Streaming Job name should not be null.&quot;);</span><br><span class="line"></span><br><span class="line">	StreamGraph streamGraph = this.getStreamGraph();</span><br><span class="line">	streamGraph.setJobName(jobName);</span><br><span class="line"></span><br><span class="line">	transformations.clear();</span><br><span class="line"></span><br><span class="line">	// execute the programs</span><br><span class="line">	if (ctx instanceof DetachedEnvironment) &#123;</span><br><span class="line">		LOG.warn(&quot;Job was executed in detached mode, the results will be available on completion.&quot;);</span><br><span class="line">		((DetachedEnvironment) ctx).setDetachedPlan(streamGraph);</span><br><span class="line">		return DetachedEnvironment.DetachedJobExecutionResult.INSTANCE;</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		return ctx</span><br><span class="line">			.getClient()</span><br><span class="line">			.run(streamGraph, ctx.getJars(), ctx.getClasspaths(), ctx.getUserCodeClassLoader(), ctx.getSavepointRestoreSettings())</span><br><span class="line">			.getJobExecutionResult();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>其实主要调用的就是</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StreamGraphGenerator.generate(this, transformations);</span><br></pre></td></tr></table></figure><p></p><p>每一个<code>OneInputTransformation</code>都会记录他的上游的input的transformation，在<code>StreamGraphGenerator.generate</code>主要针对不同的transformation进行不同的转化</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Collection&lt;Integer&gt; <span class="title">transform</span><span class="params">(StreamTransformation&lt;?&gt; transform)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">			<span class="keyword">return</span> alreadyTransformed.get(transform);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		LOG.debug(<span class="string">"Transforming "</span> + transform);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (transform.getMaxParallelism() &lt;= <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// if the max parallelism hasn't been set, then first use the job wide max parallelism</span></span><br><span class="line">			<span class="comment">// from theExecutionConfig.</span></span><br><span class="line">			<span class="keyword">int</span> globalMaxParallelismFromConfig = env.getConfig().getMaxParallelism();</span><br><span class="line">			<span class="keyword">if</span> (globalMaxParallelismFromConfig &gt; <span class="number">0</span>) &#123;</span><br><span class="line">				transform.setMaxParallelism(globalMaxParallelismFromConfig);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// call at least once to trigger exceptions about MissingTypeInfo</span></span><br><span class="line">		transform.getOutputType();</span><br><span class="line"></span><br><span class="line">		Collection&lt;Integer&gt; transformedIds;</span><br><span class="line">		<span class="keyword">if</span> (transform <span class="keyword">instanceof</span> OneInputTransformation&lt;?, ?&gt;) &#123;</span><br><span class="line">			transformedIds = transformOneInputTransform((OneInputTransformation&lt;?, ?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> TwoInputTransformation&lt;?, ?, ?&gt;) &#123;</span><br><span class="line">			transformedIds = transformTwoInputTransform((TwoInputTransformation&lt;?, ?, ?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SourceTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSource((SourceTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SinkTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSink((SinkTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> UnionTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformUnion((UnionTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SplitTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSplit((SplitTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SelectTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSelect((SelectTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> FeedbackTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformFeedback((FeedbackTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> CoFeedbackTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformCoFeedback((CoFeedbackTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> PartitionTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformPartition((PartitionTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SideOutputTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSideOutput((SideOutputTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Unknown transformation: "</span> + transform);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// need this check because the iterate transformation adds itself before</span></span><br><span class="line">		<span class="comment">// transforming the feedback edges</span></span><br><span class="line">		<span class="keyword">if</span> (!alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">			alreadyTransformed.put(transform, transformedIds);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (transform.getBufferTimeout() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">			streamGraph.setBufferTimeout(transform.getId(), transform.getBufferTimeout());</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (transform.getUid() != <span class="keyword">null</span>) &#123;</span><br><span class="line">			streamGraph.setTransformationUID(transform.getId(), transform.getUid());</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (transform.getUserProvidedNodeHash() != <span class="keyword">null</span>) &#123;</span><br><span class="line">			streamGraph.setTransformationUserHash(transform.getId(), transform.getUserProvidedNodeHash());</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (transform.getMinResources() != <span class="keyword">null</span> &amp;&amp; transform.getPreferredResources() != <span class="keyword">null</span>) &#123;</span><br><span class="line">			streamGraph.setResources(transform.getId(), transform.getMinResources(), transform.getPreferredResources());</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> transformedIds;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p>可以看到他里面的方法都是递归调用<code>transform(input)</code>方法，然后通过<code>alreadyTransformed</code>数据结构，避免重复计算，所以我们最终看的时候最先是从source处进行的，也就是从上游到下游进行转化</p><ol><li>如果已经在<code>alreadyTransformed</code>数据结构中那么就直接返回transformation的id</li><li>分别有addSource，addOperator，addSink，addCoOperator，addEdge的不同操作来生成streamGraph中的不同节点</li><li>addEdge建立每一个transformation和他所有上游输入节点的连线</li></ol><p>在streamgraph中还建立了几个虚拟的节点，这几个节点主要针对的是partition，split/select，sideoutput的操作。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">private Map&lt;Integer, Tuple2&lt;Integer, List&lt;String&gt;&gt;&gt; virtualSelectNodes;</span><br><span class="line">private Map&lt;Integer, Tuple2&lt;Integer, OutputTag&gt;&gt; virtualSideOutputNodes;</span><br><span class="line">private Map&lt;Integer, Tuple2&lt;Integer, StreamPartitioner&lt;?&gt;&gt;&gt; virtualPartitionNodes;</span><br></pre></td></tr></table></figure><p></p><p>在进行这些操作时，会添加一个唯一的虚拟节点</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//记录了上游某个transformId到下游的partition方式</span></span><br><span class="line">virtualPartitionNodes.put(virtualId, <span class="keyword">new</span> Tuple2&lt;Integer, StreamPartitioner&lt;?&gt;&gt;(originalId, partitioner));  </span><br><span class="line"><span class="comment">//记录上游的不同outputTag，用以将部分数据从该tag输出</span></span><br><span class="line">virtualSideOutputNodes.put(virtualId, <span class="keyword">new</span> Tuple2&lt;&gt;(originalId, outputTag));</span><br><span class="line"><span class="comment">//记录一个上游的select虚拟节点</span></span><br><span class="line">virtualSelectNodes.put(virtualId, <span class="keyword">new</span> Tuple2&lt;Integer, List&lt;String&gt;&gt;(originalId, selectedNames));</span><br></pre></td></tr></table></figure><p></p><p>经过一些列的<code>addNode</code>以及<code>addEdge</code>之后，streamGraph已经生成。关于其他几个graph的生成请听下回的分解</p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-backpressure-sample.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/flink-backpressure-sample.html" itemprop="url">flink-ui中的反压采样</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-11T07:02:51+08:00">2018-04-11 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/源码分析/" itemprop="url" rel="index"><span itemprop="name">源码分析</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-backpressure-sample.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-backpressure-sample.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">1.1k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">5</span></div></div></header><div class="post-body" itemprop="articleBody"><p>flink反压值采样计算原理</p><p>&lt;!-- more --&gt;</p><p>我们在点击flink ui的<code>operator-&gt;backpressure</code>之后，会触发Backpressure采样：每隔<code>BACK_PRESSURE_REFRESH_INTERVAL</code>的间隔进行一次采样。</p><h3>BackPressureStatsTracker#triggerStackTraceSample</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Triggers a stack trace sample for a operator to gather the back pressure</span></span><br><span class="line"><span class="comment"> * statistics. If there is a sample in progress for the operator, the call</span></span><br><span class="line"><span class="comment"> * is ignored.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> vertex Operator to get the stats for. 代表要采样的Operator节点</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> Flag indicating whether a sample with triggered.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">triggerStackTraceSample</span><span class="params">(ExecutionJobVertex vertex)</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 保证没有并行triggerSimple</span></span><br><span class="line">	<span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">		<span class="keyword">if</span> (shutDown) &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//排除掉已经pending在采样和结束的Operator</span></span><br><span class="line">		<span class="keyword">if</span> (!pendingStats.contains(vertex) &amp;&amp;</span><br><span class="line">				!vertex.getGraph().getState().isGloballyTerminalState()) &#123;</span><br><span class="line">				</span><br><span class="line">			<span class="comment">// 拿到对应ExecutionGraph的FutureExecutor，这是用以在相应的ExecutionGraph发起任务的</span></span><br><span class="line">			Executor executor = vertex.getGraph().getFutureExecutor();</span><br><span class="line"></span><br><span class="line">			<span class="comment">// Only trigger if still active job</span></span><br><span class="line">			<span class="keyword">if</span> (executor != <span class="keyword">null</span>) &#123;</span><br><span class="line">				pendingStats.add(vertex);</span><br><span class="line"></span><br><span class="line">				<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">					LOG.debug(<span class="string">"Triggering stack trace sample for tasks: "</span> + Arrays.toString(vertex.getTaskVertices()));</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="comment">// 核心方法 通过StackTraceSampleCoordinator 去发起相应的trigger流程, 这里的Future是Flink内部自己定义的异步结果接口 具体可查看FlinkFuture.java(可以深入了解下)</span></span><br><span class="line">				Future&lt;StackTraceSample&gt; sample = coordinator.triggerStackTraceSample(</span><br><span class="line">						vertex.getTaskVertices(),</span><br><span class="line">						numSamples,</span><br><span class="line">						delayBetweenSamples,</span><br><span class="line">						MAX_STACK_TRACE_DEPTH);</span><br><span class="line">				<span class="comment">// 指定异步回调函数（采样结果分析的函数）</span></span><br><span class="line">				sample.handleAsync(<span class="keyword">new</span> StackTraceSampleCompletionCallback(vertex), executor);</span><br><span class="line"></span><br><span class="line">				<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>StackTraceSampleCoordinator#triggerStackTraceSample</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">	 * Triggers a stack trace sample to all tasks.</span><br><span class="line">	 *</span><br><span class="line">	 * @param tasksToSample       Tasks to sample.</span><br><span class="line">	 * @param numSamples          Number of stack trace samples to collect.</span><br><span class="line">	 * @param delayBetweenSamples Delay between consecutive samples.</span><br><span class="line">	 * @param maxStackTraceDepth  Maximum depth of the stack trace. 0 indicates</span><br><span class="line">	 *                            no maximum and keeps the complete stack trace.</span><br><span class="line">	 * @return A future of the completed stack trace sample</span><br><span class="line">	 */</span><br><span class="line">	@SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">	public Future&lt;StackTraceSample&gt; triggerStackTraceSample(</span><br><span class="line">			ExecutionVertex[] tasksToSample,</span><br><span class="line">			int numSamples,</span><br><span class="line">			Time delayBetweenSamples,</span><br><span class="line">			int maxStackTraceDepth) &#123;</span><br><span class="line"></span><br><span class="line">		checkNotNull(tasksToSample, &quot;Tasks to sample&quot;);</span><br><span class="line">		checkArgument(tasksToSample.length &gt;= 1, &quot;No tasks to sample&quot;);</span><br><span class="line">		checkArgument(numSamples &gt;= 1, &quot;No number of samples&quot;);</span><br><span class="line">		checkArgument(maxStackTraceDepth &gt;= 0, &quot;Negative maximum stack trace depth&quot;);</span><br><span class="line"></span><br><span class="line">		// 通过ExecutionVertex获取ExecutionAttemptID和Execution，并最后做存活判断</span><br><span class="line">		// Execution IDs of running tasks</span><br><span class="line">		ExecutionAttemptID[] triggerIds = new ExecutionAttemptID[tasksToSample.length];</span><br><span class="line">		Execution[] executions = new Execution[tasksToSample.length];</span><br><span class="line"></span><br><span class="line">		// Check that all tasks are RUNNING before triggering anything. The</span><br><span class="line">		// triggering can still fail.</span><br><span class="line">		for (int i = 0; i &lt; triggerIds.length; i++) &#123;</span><br><span class="line">			Execution execution = tasksToSample[i].getCurrentExecutionAttempt();</span><br><span class="line">			if (execution != null &amp;&amp; execution.getState() == ExecutionState.RUNNING) &#123;</span><br><span class="line">				executions[i] = execution;</span><br><span class="line">				triggerIds[i] = execution.getAttemptId();</span><br><span class="line">			&#125; else &#123;</span><br><span class="line">				return FlinkCompletableFuture.completedExceptionally(</span><br><span class="line">					new IllegalStateException(&quot;Task &quot; + tasksToSample[i]</span><br><span class="line">					.getTaskNameWithSubtaskIndex() + &quot; is not running.&quot;));</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		synchronized (lock) &#123;</span><br><span class="line">			if (isShutDown) &#123;</span><br><span class="line">				return FlinkCompletableFuture.completedExceptionally(new IllegalStateException(&quot;Shut down&quot;));</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			final int sampleId = sampleIdCounter++;</span><br><span class="line"></span><br><span class="line">			LOG.debug(&quot;Triggering stack trace sample &#123;&#125;&quot;, sampleId);</span><br><span class="line">			</span><br><span class="line">			// 包含采样id和ExecutionAttemptID</span><br><span class="line">			final PendingStackTraceSample pending = new PendingStackTraceSample(</span><br><span class="line">					sampleId, triggerIds);</span><br><span class="line"></span><br><span class="line">			// Discard the sample if it takes too long. We don&apos;t send cancel</span><br><span class="line">			// messages to the task managers, but only wait for the responses</span><br><span class="line">			// and then ignore them.</span><br><span class="line">			long expectedDuration = numSamples * delayBetweenSamples.toMilliseconds();</span><br><span class="line">			Time timeout = Time.milliseconds(expectedDuration + sampleTimeout);</span><br><span class="line"></span><br><span class="line">			// Add the pending sample before scheduling the discard task to</span><br><span class="line">			// prevent races with removing it again.</span><br><span class="line">			pendingSamples.put(sampleId, pending);</span><br><span class="line"></span><br><span class="line">			// Trigger all samples</span><br><span class="line">			// execution是executionVertex的多次执行（recovery...）</span><br><span class="line">			for (Execution execution: executions) &#123;</span><br><span class="line">			    // 对相应的execution进行多次numSamples采样，但是都是同一个sampleId</span><br><span class="line">				final Future&lt;StackTraceSampleResponse&gt; stackTraceSampleFuture = execution.requestStackTraceSample(</span><br><span class="line">					sampleId,</span><br><span class="line">					numSamples,</span><br><span class="line">					delayBetweenSamples,</span><br><span class="line">					maxStackTraceDepth,</span><br><span class="line">					timeout);</span><br><span class="line"></span><br><span class="line">				stackTraceSampleFuture.handleAsync(new BiFunction&lt;StackTraceSampleResponse, Throwable, Void&gt;() &#123;</span><br><span class="line">					@Override</span><br><span class="line">					public Void apply(StackTraceSampleResponse stackTraceSampleResponse, Throwable throwable) &#123;</span><br><span class="line">						if (stackTraceSampleResponse != null) &#123;</span><br><span class="line">						    // 收集返回的List&lt;StackTraceElement[]&gt; 到PendingStackTraceSample</span><br><span class="line">							collectStackTraces(</span><br><span class="line">								stackTraceSampleResponse.getSampleId(),</span><br><span class="line">								stackTraceSampleResponse.getExecutionAttemptID(),</span><br><span class="line">								// 返回的堆栈信息包含所有的采样结果</span><br><span class="line">								stackTraceSampleResponse.getSamples());</span><br><span class="line">						&#125; else &#123;</span><br><span class="line">							cancelStackTraceSample(sampleId, throwable);</span><br><span class="line">						&#125;</span><br><span class="line"></span><br><span class="line">						return null;</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;, executor);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			return pending.getStackTraceSampleFuture();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><h3>BackPressureStatsTracker#StackTraceSampleCompletionCallback</h3><p>采样的结果就是<code>StackTraceSample</code></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">* java.lang.Object.wait(Native Method)</span><br><span class="line">* o.a.f.[...].LocalBufferPool.requestBuffer(LocalBufferPool.java:163)</span><br><span class="line">* o.a.f.[...].LocalBufferPool.requestBufferBlocking(LocalBufferPool.java:133) &lt;--- BLOCKING</span><br><span class="line">* request</span><br><span class="line">* [...]</span><br></pre></td></tr></table></figure><p></p><p>利用这样的线程堆栈类型来判断是否block住了。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">		 * Creates the back pressure stats from a stack trace sample.</span><br><span class="line">		 *</span><br><span class="line">		 * @param sample Stack trace sample to base stats on.</span><br><span class="line">		 *</span><br><span class="line">		 * @return Back pressure stats</span><br><span class="line">		 */</span><br><span class="line">		private OperatorBackPressureStats createStatsFromSample(StackTraceSample sample) &#123;</span><br><span class="line">			Map&lt;ExecutionAttemptID, List&lt;StackTraceElement[]&gt;&gt; traces = sample.getStackTraces();</span><br><span class="line"></span><br><span class="line">			// Map task ID to subtask index, because the web interface expects</span><br><span class="line">			// it like that.</span><br><span class="line">			// 方便下面根据executionId查询相应的并发度</span><br><span class="line">			Map&lt;ExecutionAttemptID, Integer&gt; subtaskIndexMap = Maps</span><br><span class="line">					.newHashMapWithExpectedSize(traces.size());</span><br><span class="line"></span><br><span class="line">			Set&lt;ExecutionAttemptID&gt; sampledTasks = sample.getStackTraces().keySet();</span><br><span class="line"></span><br><span class="line">			for (ExecutionVertex task : vertex.getTaskVertices()) &#123;</span><br><span class="line">				ExecutionAttemptID taskId = task.getCurrentExecutionAttempt().getAttemptId();</span><br><span class="line">				if (sampledTasks.contains(taskId)) &#123;</span><br><span class="line">					subtaskIndexMap.put(taskId, task.getParallelSubtaskIndex());</span><br><span class="line">				&#125; else &#123;</span><br><span class="line">					LOG.debug(&quot;Outdated sample. A task, which is part of the &quot; +</span><br><span class="line">							&quot;sample has been reset.&quot;);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			// Ratio of blocked samples to total samples per sub task. Array</span><br><span class="line">			// position corresponds to sub task index.</span><br><span class="line">			// 数组的index和task的并发度相绑定</span><br><span class="line">			double[] backPressureRatio = new double[traces.size()];</span><br><span class="line"></span><br><span class="line">			for (Entry&lt;ExecutionAttemptID, List&lt;StackTraceElement[]&gt;&gt; entry : traces.entrySet()) &#123;</span><br><span class="line">				int backPressureSamples = 0;</span><br><span class="line"></span><br><span class="line">				List&lt;StackTraceElement[]&gt; taskTraces = entry.getValue();</span><br><span class="line"></span><br><span class="line">				for (StackTraceElement[] trace : taskTraces) &#123;</span><br><span class="line">					for (int i = trace.length - 1; i &gt;= 0; i--) &#123;</span><br><span class="line">						StackTraceElement elem = trace[i];</span><br><span class="line"></span><br><span class="line">						if (elem.getClassName().equals(EXPECTED_CLASS_NAME) &amp;&amp;</span><br><span class="line">								elem.getMethodName().equals(EXPECTED_METHOD_NAME)) &#123;</span><br><span class="line"></span><br><span class="line">							backPressureSamples++;</span><br><span class="line">							break; // Continue with next stack trace</span><br><span class="line">						&#125;</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				int subtaskIndex = subtaskIndexMap.get(entry.getKey());</span><br><span class="line"></span><br><span class="line">				int size = taskTraces.size();</span><br><span class="line">				double ratio = (size &gt; 0)</span><br><span class="line">						? ((double) backPressureSamples) / size</span><br><span class="line">						: 0;</span><br><span class="line"></span><br><span class="line">				backPressureRatio[subtaskIndex] = ratio;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			return new OperatorBackPressureStats(</span><br><span class="line">					sample.getSampleId(),</span><br><span class="line">					sample.getEndTime(),</span><br><span class="line">					backPressureRatio);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p>至此完成采样</p><p>待学习</p><ol><li>Flink反压原理</li><li>理解清里面很多的Future使用方法</li></ol></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/advance-maven-shade-plugin.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/advance-maven-shade-plugin.html" itemprop="url">maven-shade-plugin插件高级用法</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-10T22:06:03+08:00">2018-04-10 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/编程工具/" itemprop="url" rel="index"><span itemprop="name">编程工具</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/advance-maven-shade-plugin.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="advance-maven-shade-plugin.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">638 </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">3</span></div></div></header><div class="post-body" itemprop="articleBody"><p>工作中的maven-shade-plugin插件高级用法小记</p><p>&lt;!-- more --&gt;</p><h3>问题背景</h3><p>flink-table 库中使用了org.apache.calcite 1.12版本，我在开发flink-cep的过程中引入了org.apache-calcite 1.6版本，服务端要同时引入这两个版本的包，如何解决这个版本冲突问题呢？</p><h3>maven-shade-plugin</h3><p>使用maven-shade-plugin的relocate功能可以将包名打包成一个别名</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&lt;plugin&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">    &lt;executions&gt;</span><br><span class="line">        &lt;execution&gt;</span><br><span class="line">            &lt;goals&gt;</span><br><span class="line">                &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">            &lt;/goals&gt;</span><br><span class="line">			&lt;configuration&gt;</span><br><span class="line">				&lt;filters&gt;</span><br><span class="line">					&lt;filter&gt;</span><br><span class="line">						&lt;artifact&gt;*:*&lt;/artifact&gt;</span><br><span class="line">						&lt;excludes&gt;</span><br><span class="line">							&lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;</span><br><span class="line">							&lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;</span><br><span class="line">							&lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;</span><br><span class="line">						&lt;/excludes&gt;</span><br><span class="line">					&lt;/filter&gt;</span><br><span class="line">				&lt;/filters&gt;</span><br><span class="line">				&lt;relocations&gt;</span><br><span class="line">				   &lt;!--通过relocation配置将包名进行了替换--&gt;</span><br><span class="line">					&lt;relocation&gt;</span><br><span class="line">						&lt;pattern&gt;org.apache.calcite&lt;/pattern&gt;</span><br><span class="line">						&lt;shadedPattern&gt;org.apache.flink.shaded.cep.calcite&lt;/shadedPattern&gt;</span><br><span class="line">					&lt;/relocation&gt;</span><br><span class="line">				&lt;/relocations&gt;</span><br><span class="line">				&lt;transformers&gt;</span><br><span class="line">					&lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer&quot;/&gt;</span><br><span class="line">				&lt;/transformers&gt;</span><br><span class="line">				&lt;!-- Additional configuration. --&gt;</span><br><span class="line">			&lt;/configuration&gt;</span><br><span class="line">        &lt;/execution&gt;</span><br><span class="line">    &lt;/executions&gt;</span><br><span class="line">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure><p></p><p>看起来一切都ok了，不过思考一个问题，maven-shade-plugin插件修改包名后项目中引用的包名怎么找到的呢？Class.forName(&quot;xxxx.xxx.xxx&quot;)去生成类的方式还奏不奏效呢？Stack Overflow上关于shade插件的描述也提到了对这种类加载的方式是否奏效的疑问</p><p>然而当应用一跑起来之后发现，并不奏效！！！报出异常 <code>No suitable driver found for jdbc:calcite</code> 也就是<code>DriverManager</code>生成相应的driver的时候没找到相应的类。</p><p><strong>相关的issue</strong></p><p><a href="https://github.com/xerial/sqlite-jdbc/issues/145" target="_blank" rel="noopener">https://github.com/xerial/sqlite-jdbc/issues/145</a></p><p>在里面我们看到sqlite数据库有这样的配置文件指定了相应driver的类名</p><p><a href="https://github.com/xerial/sqlite-jdbc/blob/master/src/main/resources/java.sql.Driver" target="_blank" rel="noopener">https://github.com/xerial/sqlite-jdbc/blob/master/src/main/resources/java.sql.Driver</a></p><p>查看calcite也有相应的配置文件指定</p><p><a href="https://github.com/apache/calcite/blob/master/core/src/main/resources/META-INF/services/java.sql.Driver" target="_blank" rel="noopener">https://github.com/apache/calcite/blob/master/core/src/main/resources/META-INF/services/java.sql.Driver</a></p><p>那是不是在shade的过程中将这个文件内容也进行相应的变更就可以了呢? shade也确实有这样的transformation</p><p><a href="http://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#ServicesResourceTransformer" target="_blank" rel="noopener">http://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#ServicesResourceTransformer</a></p><blockquote><p>JAR files providing implementations of some interfaces often ship with a META-INF/services/ directory that maps interfaces to their implementation classes for lookup by the service locator. To relocate the class names of these implementation classes, and to merge multiple implementations of the same interface into one service entry, the ServicesResourceTransformer can be used</p></blockquote><p>最后一个问题: maven-shade-plugin 要在3.0以上才能生效</p><p><a href="https://stackoverflow.com/questions/47476402/maven-shade-plugin-relocation-not-updating-a-entry-in-resource-file" target="_blank" rel="noopener">https://stackoverflow.com/questions/47476402/maven-shade-plugin-relocation-not-updating-a-entry-in-resource-file</a></p><p>效果: <img src="http://or0igopk2.bkt.clouddn.com/18-4-10/84724380.jpg" alt></p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-kafka-exactly-once.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/flink-kafka-exactly-once.html" itemprop="url">上篇·flink 1.4利用kafka0.11实现完整的一致性语义</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-11T22:29:44+08:00">2018-03-11 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/源码分析/" itemprop="url" rel="index"><span itemprop="name">源码分析</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-kafka-exactly-once.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-kafka-exactly-once.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">3k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">15</span></div></div></header><div class="post-body" itemprop="articleBody"><p>flink kafka-connector0.10版本分析，与1.4版本中kafka11对比</p><p>&lt;!-- more --&gt;</p><h1>引言</h1><p>官方文档在出了1.4之后特意发表了一篇blog，通过以下这两个条件实现了真正意义上的exactly once语义</p><ol><li>kafka producer0.11的事务性</li><li><a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol" target="_blank" rel="noopener">two phase commit protocol</a></li></ol><p>我们先看0.10版本的kafka-connector的行为逻辑.</p><h1>Kafka-Connector10</h1><h2>kafkaConsumer10</h2><h3>FlinkKafkaConsumerBase</h3><p>这个抽象类实现了<code>CheckpointedFunction</code>, 这个接口的描述：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">* This is the core <span class="class"><span class="keyword">interface</span> <span class="title">for</span> &lt;<span class="title">i</span>&gt;<span class="title">stateful</span> <span class="title">transformation</span> <span class="title">functions</span>&lt;/<span class="title">i</span>&gt;, <span class="title">meaning</span> <span class="title">functions</span></span></span><br><span class="line"><span class="class">* <span class="title">that</span> <span class="title">maintain</span> <span class="title">state</span> <span class="title">across</span> <span class="title">individual</span> <span class="title">stream</span> <span class="title">records</span>.</span></span><br><span class="line"><span class="class">* <span class="title">While</span> <span class="title">more</span> <span class="title">lightweight</span> <span class="title">interfaces</span> <span class="title">exist</span> <span class="title">as</span> <span class="title">shortcuts</span> <span class="title">for</span> <span class="title">various</span> <span class="title">types</span> <span class="title">of</span> <span class="title">state</span>, <span class="title">this</span> <span class="title">interface</span> <span class="title">offer</span> <span class="title">the</span></span></span><br><span class="line"><span class="class">* <span class="title">greatest</span> <span class="title">flexibility</span> <span class="title">in</span> <span class="title">managing</span> <span class="title">both</span> &lt;<span class="title">i</span>&gt;<span class="title">keyed</span> <span class="title">state</span>&lt;/<span class="title">i</span>&gt; <span class="title">and</span> &lt;<span class="title">i</span>&gt;<span class="title">operator</span> <span class="title">state</span>&lt;/<span class="title">i</span>&gt;.</span></span><br></pre></td></tr></table></figure><p></p><p>这个接口中主要要去做两件事情：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//每一次做checkpoint的时候被调用</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化每一个并发的实例的时候被调用</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br></pre></td></tr></table></figure><p></p><p>初始化函数的调用时机是在<code>open</code>之前的：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//StreamTask.java</span></span><br><span class="line">initializeState();</span><br><span class="line">openAllOperators();</span><br></pre></td></tr></table></figure><p></p><p>在初始化的函数中提供了一个<code>FunctionSnapshotContext</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">void snapshotState(FunctionSnapshotContext context) throws Exception;</span><br></pre></td></tr></table></figure><p></p><p>让你既可以注册一个KeyedStateStore，也可以注册一个OperatorStateStore</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ManagedInitializationContext</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Returns true, if state was restored from the snapshot of a previous execution. This returns always false for</span></span><br><span class="line"><span class="comment">	 * stateless tasks.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">boolean</span> <span class="title">isRestored</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Returns an interface that allows for registering operator state with the backend.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function">OperatorStateStore <span class="title">getOperatorStateStore</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Returns an interface that allows for registering keyed state with the backend.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function">KeyedStateStore <span class="title">getKeyedStateStore</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>我们可以看到kafka10是怎么利用这个<code>CheckpointedFunction</code>来管理记录内部offset的呢？</p><h3>initializeState</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始化过程</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// we might have been restored via restoreState() which restores from legacy operator state</span></span><br><span class="line">		<span class="keyword">if</span> (!restored) &#123;</span><br><span class="line">			restored = context.isRestored();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		OperatorStateStore stateStore = context.getOperatorStateStore();</span><br><span class="line">		offsetsStateForCheckpoint = stateStore.getSerializableListState(DefaultOperatorStateBackend.DEFAULT_OPERATOR_STATE_NAME);</span><br><span class="line">		<span class="comment">//如果是在重启恢复的过程中</span></span><br><span class="line">		<span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">			<span class="keyword">if</span> (restoredState == <span class="keyword">null</span>) &#123;</span><br><span class="line">				restoredState = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">				<span class="keyword">for</span> (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : offsetsStateForCheckpoint.get()) &#123;</span><br><span class="line">				<span class="comment">//partition和相应的offset数</span></span><br><span class="line">					restoredState.put(kafkaOffset.f0, kafkaOffset.f1);</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				LOG.info(<span class="string">"Setting restore state in the FlinkKafkaConsumer."</span>);</span><br><span class="line">				<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">					LOG.debug(<span class="string">"Using the following offsets: &#123;&#125;"</span>, restoredState);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			LOG.info(<span class="string">"No restore state for FlinkKafkaConsumer."</span>);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p>那么我们看到恢复或者初始化的时候将&lt;partition,offset&gt;信息保存到了一组HashMap中，但是这个Map怎么作用于消费阶段呢？</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将从状态中获取到的列表赋予给消费的列表</span></span><br><span class="line">subscribedPartitionsToStartOffsets = restoredState;</span><br></pre></td></tr></table></figure><p></p><h3>snapshot</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (!running) &#123;</span><br><span class="line">		LOG.debug(<span class="string">"snapshotState() called on closed source"</span>);</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line">		offsetsStateForCheckpoint.clear();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">final</span> AbstractFetcher&lt;?, ?&gt; fetcher = <span class="keyword">this</span>.kafkaFetcher;</span><br><span class="line">		<span class="keyword">if</span> (fetcher == <span class="keyword">null</span>) &#123;</span><br><span class="line">			<span class="comment">// fetcher还没有初始化，返回上一次恢复的partition和offset，也就是这里会有个bug。我提了个issue:[FLINK-8869][2]</span></span><br><span class="line">			<span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) &#123;</span><br><span class="line">				offsetsStateForCheckpoint.add(Tuple2.of(subscribedPartition.getKey(), subscribedPartition.getValue()));</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="comment">//</span></span><br><span class="line">			<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">				<span class="comment">// the map cannot be asynchronously updated, because only one checkpoint call can happen</span></span><br><span class="line">				<span class="comment">// on this function at a time: either snapshotState() or notifyCheckpointComplete()</span></span><br><span class="line">				pendingOffsetsToCommit.put(context.getCheckpointId(), restoredState);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			HashMap&lt;KafkaTopicPartition, Long&gt; currentOffsets = fetcher.snapshotCurrentState();</span><br><span class="line"></span><br><span class="line">			<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">				<span class="comment">// the map cannot be asynchronously updated, because only one checkpoint call can happen</span></span><br><span class="line">				<span class="comment">// on this function at a time: either snapshotState() or notifyCheckpointComplete()</span></span><br><span class="line">				pendingOffsetsToCommit.put(context.getCheckpointId(), currentOffsets);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) &#123;</span><br><span class="line">				offsetsStateForCheckpoint.add(</span><br><span class="line">						Tuple2.of(kafkaTopicPartitionLongEntry.getKey(), kafkaTopicPartitionLongEntry.getValue()));</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">			<span class="comment">// truncate the map of pending offsets to commit, to prevent infinite growth</span></span><br><span class="line">			<span class="keyword">while</span> (pendingOffsetsToCommit.size() &gt; MAX_NUM_PENDING_CHECKPOINTS) &#123;</span><br><span class="line">				pendingOffsetsToCommit.remove(<span class="number">0</span>);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>offsetCommitMode</h3><p>如上述代码中的<code>offsetCommitMode</code>,主要有以下几种</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DISABLED,</span><br><span class="line">ON_CHECKPOINTS, <span class="comment">// 完成一次checkpoint后向kafka提交消费offset，只有这种模式下才需要我们手动去提交offset到kafka</span></span><br><span class="line">KAFKA_PERIODIC;</span><br></pre></td></tr></table></figure><p></p><p>这个配置主要改变了commitOffset回kafka的时机. 首先在snapshot的时候会将对应的checkpointId和相应的offset的列表放入<code>pendingOffsetsToCommit</code>, 在checkpoint完成后回调<code>notifyCheckpointComplete</code>，这里面主要完成了offset的commit工作。</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">			<span class="comment">// only one commit operation must be in progress</span></span><br><span class="line">			<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">				LOG.debug(<span class="string">"Committing offsets to Kafka/ZooKeeper for checkpoint "</span> + checkpointId);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">try</span> &#123;</span><br><span class="line">				<span class="keyword">final</span> <span class="keyword">int</span> posInMap = pendingOffsetsToCommit.indexOf(checkpointId);</span><br><span class="line">				<span class="keyword">if</span> (posInMap == -<span class="number">1</span>) &#123;</span><br><span class="line">					LOG.warn(<span class="string">"Received confirmation for unknown checkpoint id &#123;&#125;"</span>, checkpointId);</span><br><span class="line">					<span class="keyword">return</span>;</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				<span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</span><br><span class="line">				HashMap&lt;KafkaTopicPartition, Long&gt; offsets =</span><br><span class="line">					(HashMap&lt;KafkaTopicPartition, Long&gt;) pendingOffsetsToCommit.remove(posInMap);</span><br><span class="line"></span><br><span class="line">				<span class="comment">// remove older checkpoints in map</span></span><br><span class="line">				<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; posInMap; i++) &#123;</span><br><span class="line">					pendingOffsetsToCommit.remove(<span class="number">0</span>);</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				<span class="keyword">if</span> (offsets == <span class="keyword">null</span> || offsets.size() == <span class="number">0</span>) &#123;</span><br><span class="line">					LOG.debug(<span class="string">"Checkpoint state was empty."</span>);</span><br><span class="line">					<span class="keyword">return</span>;</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				fetcher.commitInternalOffsetsToKafka(offsets, offsetCommitCallback);</span><br><span class="line">			&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">				<span class="keyword">if</span> (running) &#123;</span><br><span class="line">					<span class="keyword">throw</span> e;</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="comment">// else ignore exception if we are no longer running</span></span><br><span class="line">			&#125;</span><br></pre></td></tr></table></figure><p></p><h3>消费partition分配问题</h3><ul><li><p>在从上次点恢复的情况下是直接从state中获取应该读取哪一个partition，offset。如果并发度改变了会做出什么样的反馈呢?会正确做出rescale吗</p></li><li><p>第一次进行读取的时候会初始化处当前task（并发度）所需要订阅的partition</p></li></ul><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> Map&lt;KafkaTopicPartition, Long&gt; <span class="title">initializeSubscribedPartitionsToStartOffsets</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">			List&lt;KafkaTopicPartition&gt; kafkaTopicPartitions, //topic的所有partition</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="keyword">int</span> indexOfThisSubtask, // 当前task的维度</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="keyword">int</span> numParallelSubtasks, // 总的并发度</span></span></span><br><span class="line"><span class="function"><span class="params">			StartupMode startupMode, // 从哪个offset消费的模式（最新，最老，指定offset）</span></span></span><br><span class="line"><span class="function"><span class="params">			Map&lt;KafkaTopicPartition, Long&gt; specificStartupOffsets)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		Map&lt;KafkaTopicPartition, Long&gt; subscribedPartitionsToStartOffsets = <span class="keyword">new</span> HashMap&lt;&gt;(kafkaTopicPartitions.size());</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (KafkaTopicPartition kafkaTopicPartition : kafkaTopicPartitions) &#123;</span><br><span class="line">			<span class="comment">// only handle partitions that this subtask should subscribe to（选取当前subtask所需要订阅的partition）</span></span><br><span class="line">			<span class="keyword">if</span> (KafkaTopicPartitionAssigner.assign(kafkaTopicPartition, numParallelSubtasks) == indexOfThisSubtask) &#123;</span><br><span class="line">				<span class="keyword">if</span> (startupMode != StartupMode.SPECIFIC_OFFSETS) &#123;</span><br><span class="line">					<span class="comment">//StateSentinel都是一串随机的负数占位符(都是一个标记在KafkaConsumerThread中进行判断)</span></span><br><span class="line">					subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, startupMode.getStateSentinel());</span><br><span class="line">				&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">					<span class="keyword">if</span> (specificStartupOffsets == <span class="keyword">null</span>) &#123;</span><br><span class="line">						<span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(</span><br><span class="line">							<span class="string">"Startup mode for the consumer set to "</span> + StartupMode.SPECIFIC_OFFSETS +</span><br><span class="line">								<span class="string">", but no specific offsets were specified"</span>);</span><br><span class="line">					&#125;</span><br><span class="line"></span><br><span class="line">					Long specificOffset = specificStartupOffsets.get(kafkaTopicPartition);</span><br><span class="line">					<span class="keyword">if</span> (specificOffset != <span class="keyword">null</span>) &#123;</span><br><span class="line">						<span class="comment">// since the specified offsets represent the next record to read, we subtract</span></span><br><span class="line">						<span class="comment">// it by one so that the initial state of the consumer will be correct</span></span><br><span class="line">						<span class="comment">// 这里需要减去1</span></span><br><span class="line">						subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, specificOffset - <span class="number">1</span>);</span><br><span class="line">					&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">						subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET);</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> subscribedPartitionsToStartOffsets;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assign</span><span class="params">(KafkaTopicPartition partition, <span class="keyword">int</span> numParallelSubtasks)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> startIndex = ((partition.getTopic().hashCode() * <span class="number">31</span>) &amp; <span class="number">0x7FFFFFFF</span>) % numParallelSubtasks;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// here, the assumption is that the id of Kafka partitions are always ascending</span></span><br><span class="line">	<span class="comment">// starting from 0, and therefore can be used directly as the offset clockwise from the start index</span></span><br><span class="line">	<span class="comment">// 这里看出：每个Partition只会分配到一个subtask来消费</span></span><br><span class="line">	<span class="comment">// 1. partition &gt; parallel 一个subtask会订阅多个partition</span></span><br><span class="line">	<span class="comment">// 2. partition &lt; parallel 有subtask会是空闲的</span></span><br><span class="line">	<span class="comment">// 3. startIndex由topic名字计算得出</span></span><br><span class="line">	<span class="keyword">return</span> (startIndex + partition.getPartition()) % numParallelSubtasks;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>消费模型kafka10</h3><p>在完成partition订阅之后，就要开始真正的run方法了，<code>FlinkKafkaConsumer</code>也是实现自<code>SouceFunction</code>，因此主要的逻辑也都是在run方法中实现。 主要逻辑：</p><p>kafkaConsumerThread和Kafka10Fetcher通过<code>Handover</code>交互,我觉得这段代码写的很不错，可以好好学习下。可以形象的比作在接力跑：<code>kafkaConsumerThread</code>通过真正的消费线程消费放入一个<code>HandOver</code>，再由kafkaFetcher去poll，完成整个消费过程。</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// we need only do work, if we actually have partitions assigned</span></span><br><span class="line"><span class="keyword">if</span> (!subscribedPartitionsToStartOffsets.isEmpty()) &#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// create the fetcher that will communicate with the Kafka brokers</span></span><br><span class="line">	<span class="keyword">final</span> AbstractFetcher&lt;T, ?&gt; fetcher = createFetcher(</span><br><span class="line">			sourceContext,</span><br><span class="line">			subscribedPartitionsToStartOffsets,</span><br><span class="line">			periodicWatermarkAssigner,</span><br><span class="line">			punctuatedWatermarkAssigner,</span><br><span class="line">			(StreamingRuntimeContext) getRuntimeContext(),</span><br><span class="line">			offsetCommitMode);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// publish the reference, for snapshot-, commit-, and cancel calls</span></span><br><span class="line">	<span class="comment">// IMPORTANT: We can only do that now, because only now will calls to</span></span><br><span class="line">	<span class="comment">//            the fetchers 'snapshotCurrentState()' method return at least</span></span><br><span class="line">	<span class="comment">//            the restored offsets</span></span><br><span class="line">	<span class="keyword">this</span>.kafkaFetcher = fetcher;</span><br><span class="line">	<span class="keyword">if</span> (!running) &#123;</span><br><span class="line">		<span class="keyword">return</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// (3) run the fetcher' main work method</span></span><br><span class="line">	<span class="comment">// 主要工作方法</span></span><br><span class="line">	fetcher.runFetchLoop();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">	<span class="comment">// this source never completes, so emit a Long.MAX_VALUE watermark</span></span><br><span class="line">	<span class="comment">// to not block watermark forwarding</span></span><br><span class="line">	<span class="comment">// 发送最大的watermark就不会block住下游的watermark更新</span></span><br><span class="line">	sourceContext.emitWatermark(<span class="keyword">new</span> Watermark(Long.MAX_VALUE));</span><br><span class="line"></span><br><span class="line">	<span class="comment">// wait until this is canceled</span></span><br><span class="line">	<span class="keyword">final</span> Object waitLock = <span class="keyword">new</span> Object();</span><br><span class="line">	<span class="keyword">while</span> (running) &#123;</span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			<span class="comment">//noinspection SynchronizationOnLocalVariableOrMethodParameter</span></span><br><span class="line">			<span class="keyword">synchronized</span> (waitLock) &#123;</span><br><span class="line">				waitLock.wait();</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">			<span class="keyword">if</span> (!running) &#123;</span><br><span class="line">				<span class="comment">// restore the interrupted state, and fall through the loop</span></span><br><span class="line">				<span class="comment">// 打断当前线程</span></span><br><span class="line">				Thread.currentThread().interrupt();</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Handover的描述， Handover代码可以再好好学习下。</span><br><span class="line">* The Handover is a utility to hand over data (a buffer of records) and exception from a</span><br><span class="line">* &lt;i&gt;producer&lt;/i&gt; thread to a &lt;i&gt;consumer&lt;/i&gt; thread. It effectively behaves like a</span><br><span class="line">* &quot;size one blocking queue&quot;, with some extras around exception reporting, closing, and</span><br><span class="line">* waking up thread without &#123;@link Thread#interrupt() interrupting&#125; threads.</span><br></pre></td></tr></table></figure><p></p><h3>KafkaFetcher</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">runFetchLoop</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		<span class="keyword">final</span> Handover handover = <span class="keyword">this</span>.handover;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// kick off the actual Kafka consumer</span></span><br><span class="line">		<span class="comment">// 启动真正的消费线程</span></span><br><span class="line">		consumerThread.start();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">while</span> (running) &#123;</span><br><span class="line">			<span class="comment">// this blocks until we get the next records</span></span><br><span class="line">			<span class="comment">// it automatically re-throws exceptions encountered in the fetcher thread</span></span><br><span class="line">			<span class="comment">// 在handover中获取真正的数据，并抛出其他线程中的异常</span></span><br><span class="line">			<span class="keyword">final</span> ConsumerRecords&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; records = handover.pollNext();</span><br><span class="line"></span><br><span class="line">			<span class="comment">// get the records for each topic partition</span></span><br><span class="line">			<span class="comment">// subscribedPartitionStates维护的是每个partition的状态（partition，KPH（partition的描述，依据版本可能不同），offset, committedOffset, watermark）</span></span><br><span class="line">			<span class="keyword">for</span> (KafkaTopicPartitionState&lt;TopicPartition&gt; partition : subscribedPartitionStates()) &#123;</span><br><span class="line"></span><br><span class="line">				List&lt;ConsumerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt; partitionRecords =</span><br><span class="line">						records.records(partition.getKafkaPartitionHandle());</span><br><span class="line"></span><br><span class="line">				<span class="keyword">for</span> (ConsumerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; record : partitionRecords) &#123;</span><br><span class="line">					<span class="keyword">final</span> T value = deserializer.deserialize(</span><br><span class="line">							record.key(), record.value(),</span><br><span class="line">							record.topic(), record.partition(), record.offset());</span><br><span class="line"></span><br><span class="line">					<span class="keyword">if</span> (deserializer.isEndOfStream(value)) &#123;</span><br><span class="line">						<span class="comment">// end of stream signaled</span></span><br><span class="line">						running = <span class="keyword">false</span>;</span><br><span class="line">						<span class="keyword">break</span>;</span><br><span class="line">					&#125;</span><br><span class="line"></span><br><span class="line">					<span class="comment">// emit the actual record. this also updates offset state atomically</span></span><br><span class="line">					<span class="comment">// and deals with timestamps and watermark generation</span></span><br><span class="line">					<span class="comment">// 这里会进入真正的通过sourceContext发送数据的代码 如下</span></span><br><span class="line">					emitRecord(value, partition, record.offset(), record);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">finally</span> &#123;</span><br><span class="line">		<span class="comment">// this signals the consumer thread that no more work is to be done</span></span><br><span class="line">		consumerThread.shutdown();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// on a clean exit, wait for the runner thread</span></span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		consumerThread.join();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">		<span class="comment">// may be the result of a wake-up interruption after an exception.</span></span><br><span class="line">		<span class="comment">// we ignore this here and only restore the interruption state</span></span><br><span class="line">		Thread.currentThread().interrupt();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">emitRecordWithTimestamp</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		T record, KafkaTopicPartitionState&lt;KPH&gt; partitionState, <span class="keyword">long</span> offset, <span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (record != <span class="keyword">null</span>) &#123;</span><br><span class="line">		<span class="keyword">if</span> (timestampWatermarkMode == NO_TIMESTAMPS_WATERMARKS) &#123;</span><br><span class="line">			<span class="comment">// fast path logic, in case there are no watermarks generated in the fetcher</span></span><br><span class="line"></span><br><span class="line">			<span class="comment">// emit the record, using the checkpoint lock to guarantee</span></span><br><span class="line">			<span class="comment">// atomicity of record emission and offset state update</span></span><br><span class="line">			<span class="keyword">synchronized</span> (checkpointLock) &#123;</span><br><span class="line">				sourceContext.collectWithTimestamp(record, timestamp);</span><br><span class="line">				partitionState.setOffset(offset);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (timestampWatermarkMode == PERIODIC_WATERMARKS) &#123;</span><br><span class="line">		    <span class="comment">// 更新partitionstate中的watermark状态</span></span><br><span class="line">			emitRecordWithTimestampAndPeriodicWatermark(record, partitionState, offset, timestamp);</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			emitRecordWithTimestampAndPunctuatedWatermark(record, partitionState, offset, timestamp);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		<span class="comment">// if the record is null, simply just update the offset state for partition</span></span><br><span class="line">		<span class="keyword">synchronized</span> (checkpointLock) &#123;</span><br><span class="line">			partitionState.setOffset(offset);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>在设置了kafkaTimestampassigner之后就会进行一个定时任务向下游发送watermark，值为所有partition维护的最小值：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">long</span> minAcrossAll = Long.MAX_VALUE;</span><br><span class="line">	<span class="keyword">for</span> (KafkaTopicPartitionStateWithPeriodicWatermarks&lt;?, ?&gt; state : allPartitions) &#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// we access the current watermark for the periodic assigners under the state</span></span><br><span class="line">		<span class="comment">// lock, to prevent concurrent modification to any internal variables</span></span><br><span class="line">		<span class="keyword">final</span> <span class="keyword">long</span> curr;</span><br><span class="line">		<span class="comment">//noinspection SynchronizationOnLocalVariableOrMethodParameter</span></span><br><span class="line">		<span class="comment">// 这个锁是防止别的线程修改其他变量</span></span><br><span class="line">		<span class="keyword">synchronized</span> (state) &#123;</span><br><span class="line">			curr = state.getCurrentWatermarkTimestamp();</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		minAcrossAll = Math.min(minAcrossAll, curr);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// emit next watermark, if there is one</span></span><br><span class="line">	<span class="keyword">if</span> (minAcrossAll &gt; lastWatermarkTimestamp) &#123;</span><br><span class="line">		lastWatermarkTimestamp = minAcrossAll;</span><br><span class="line">		emitter.emitWatermark(<span class="keyword">new</span> Watermark(minAcrossAll));</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// schedule the next watermark</span></span><br><span class="line">	timerService.registerTimer(timerService.getCurrentProcessingTime() + interval, <span class="keyword">this</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>KafkaConsumerThread</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (records == <span class="keyword">null</span>) &#123;</span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					records = consumer.poll(pollTimeout);</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">catch</span> (WakeupException we) &#123;</span><br><span class="line">					<span class="keyword">continue</span>;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">try</span> &#123;</span><br><span class="line">				handover.produce(records);</span><br><span class="line">				records = <span class="keyword">null</span>;</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">catch</span> (Handover.WakeupException e) &#123;</span><br><span class="line">				<span class="comment">// fall through the loop</span></span><br><span class="line">			&#125;</span><br></pre></td></tr></table></figure><p></p><p>主要做的工作就是从consumer消费数据塞入handover，等待拉取</p><h3>Handover</h3><h3>桥接模式</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumerCallBridge</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">assignPartitions</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, List&lt;TopicPartition&gt; topicPartitions)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		consumer.assign(topicPartitions);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekPartitionToBeginning</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, TopicPartition partition)</span> </span>&#123;</span><br><span class="line">		consumer.seekToBeginning(partition);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekPartitionToEnd</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, TopicPartition partition)</span> </span>&#123;</span><br><span class="line">		consumer.seekToEnd(partition);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>解决08 09 10 版本的api不兼容问题</p><h2>Kafkaproducer10</h2><p>###initializeState</p><p>什么都不做</p><h3>snapshot</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="comment">// check for asynchronous errors and fail the checkpoint if necessary</span></span><br><span class="line">	checkErroneous();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (flushOnCheckpoint) &#123;</span><br><span class="line">		<span class="comment">// flushing is activated: We need to wait until pendingRecords is 0</span></span><br><span class="line">		flush();</span><br><span class="line">		<span class="keyword">synchronized</span> (pendingRecordsLock) &#123;</span><br><span class="line">			<span class="keyword">if</span> (pendingRecords != <span class="number">0</span>) &#123;</span><br><span class="line">				<span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Pending record count must be zero at this point: "</span> + pendingRecords);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// if the flushed requests has errors, we should propagate it also and fail the checkpoint</span></span><br><span class="line">			checkErroneous();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>这里涉及到一个flushOnCheckPoint的问题，再调用<code>producer.flush</code>期间，producer会将所有没写入的，在buffer中的数据刷盘，然后调用commitCallBack，这就保证了ckpt之后数据不会丢的问题。</p><p>主要工作方法：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(IN next)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="comment">// propagate asynchronous errors</span></span><br><span class="line">	checkErroneous();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">byte</span>[] serializedKey = schema.serializeKey(next);</span><br><span class="line">	<span class="keyword">byte</span>[] serializedValue = schema.serializeValue(next);</span><br><span class="line">	<span class="comment">// 每条元素可以自己自己要写到的topic</span></span><br><span class="line">	String targetTopic = schema.getTargetTopic(next);</span><br><span class="line">	<span class="keyword">if</span> (targetTopic == <span class="keyword">null</span>) &#123;</span><br><span class="line">		targetTopic = defaultTopicId;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span>[] partitions = <span class="keyword">this</span>.topicPartitionsMap.get(targetTopic);</span><br><span class="line">	<span class="keyword">if</span>(<span class="keyword">null</span> == partitions) &#123;</span><br><span class="line">		partitions = getPartitionsByTopic(targetTopic, producer);</span><br><span class="line">		<span class="keyword">this</span>.topicPartitionsMap.put(targetTopic, partitions);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	ProducerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; record;</span><br><span class="line">	<span class="keyword">if</span> (flinkKafkaPartitioner == <span class="keyword">null</span>) &#123;</span><br><span class="line">		record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(targetTopic, serializedKey, serializedValue);</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(</span><br><span class="line">				targetTopic,</span><br><span class="line">				flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions),</span><br><span class="line">				serializedKey,</span><br><span class="line">				serializedValue);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> (flushOnCheckpoint) &#123;</span><br><span class="line">		<span class="keyword">synchronized</span> (pendingRecordsLock) &#123;</span><br><span class="line">			pendingRecords++;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	producer.send(record, callback);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h1>问题</h1><ol><li><p>kafka恢复状态直接从状态中去获取了之前保存的partition和offset，但是如果是扩容partition的场景就不会从新的Partition消费 issue:<a href="https://issues.apache.org/jira/projects/FLINK/issues/FLINK-8869?filter=reportedbyme" target="_blank" rel="noopener">FLINK-8869</a></p></li><li><p>flink内部维护了offset，为什么向kafka提交的时候还需要在checkpoint之后再提交而不是定时提交就算了？</p><p>因为虽然从checkpoint点恢复的时候不需要从kafka broker获取消费点的位置了，但是如果是应用重启消费上次消费到的点的数据，这个offset就是flink向kafka提交的，放在checkpoint完成后去做的好处就是让应用即使不是从上个点恢复的，也能够从kafka消费正确的offset点。</p></li><li><p>如果在新的checkpoint没打之前任务失败了，重新从上次的offset点消费的话下游数据是不是重复了?</p><p><strong>是的，因为有一部分数据经过处理已经sink出去了，因此才需要0.11的一致性语义</strong></p></li></ol><p><em>以上代码： kafka-connector0.10 来源于release1.3.2 kafka-connector0.11 来源于release1.4.0</em></p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-cep-paper.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/flink-cep-paper.html" itemprop="url">flink-cep-paper</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-25T20:19:15+08:00">2018-02-25 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/论文/" itemprop="url" rel="index"><span itemprop="name">论文</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-cep-paper.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-cep-paper.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">1.4k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">6</span></div></div></header><div class="post-body" itemprop="articleBody"><p>CEP实现论文</p><p>&lt;!-- more --&gt;</p><h3>引言</h3><p>2016年1月29号，社区提了这个patch<a href="https://github.com/apache/flink/pull/1557" target="_blank" rel="noopener">[FLINK-3216]</a>引入flink cep模块，flink cep实现基于论文：<a href="https://people.cs.umass.edu/%7Eyanlei/publications/sase-sigmod08.pdf" target="_blank" rel="noopener">Efficient Pattern Matching over Event Streams</a>。我们来深入阅读以下这篇论文。</p><h3>event pattern查询表达形式</h3><p>先看三个查询 <img src="http://or0igopk2.bkt.clouddn.com/18-2-25/25532292.jpg" alt></p><p><em>Query1</em></p><p>匹配出的是：挑选从货架取出的商品，未经checkout，带离了商店的行为序列</p><ol><li><code>SEQ(Shelf a, ∼(Register b), Exit c)</code></li><li>where条件指定了a，b，c的tagid相同</li><li>within指定窗口大小12小时</li></ol><p><em>Query2</em></p><p>匹配出的是：一个污染报警和相关受污染的发货卸货流转站点序列</p><ol><li>报警类型是污染</li><li>匹配出的装载点和前一个到达点一致</li><li>within指定匹配窗口大小是3小时</li></ol><p><em>Query3</em></p><p>匹配出的是：匹配股票交易拐点</p><ol><li>初始成交量高于1000</li><li>持续增加，最后成交量突然跌为最高点的80%以下的序列</li><li>1小时的时间窗口</li></ol><p>可以看出规则是怎么描述的：</p><ol><li>指定匹配出的序列是什么（structure）</li><li>限制或筛选条件</li></ol><p>还需要匹配策略来进行规制匹配。</p><ol><li>严格的邻近匹配（Strict contiguity）：只有连续两个event能满足匹配，中间不能夹杂其他不符合条件的元素</li><li>分区邻近匹配（Partition contiguity）：这就是说被选出的两个event之间不一定需要完全紧密相连，但是在一个分区中的需要紧密相连 如<em>Query3</em>中的a[]需要连续，a与b不需要连续</li><li>skip_till_next_match: 所有的不相关的元素都会被跳过，不再考虑是不是邻近的元素</li><li>skip_till_any_match: <em>Query2</em>解释了这种用法。比如当前最后识别的shipment到达了X位置。这时候读进来一条，X-&gt;Y的Event，在<code>skip_till_any_match</code>匹配规则下系统会做两件事：1.接收X-&gt;Y事件，更新状态2.在当前状态的另一个实例中不接收他来保留一个状态。这样当下一个比如来了个X-&gt;Z事件那还是可以接受这个实例。这种策略就是返回所有的可能的序列（allowing non-deterministic actions）在接收到同一个事件后可以有不一样的走向</li></ol><hr><p>以上介绍了event pattern的查询形式，接下来我们看其内部实现算法。</p><h3>NFA 非确定性自动状态机</h3><p>非确定性自动状态机由以下结构组成。A = (Q,E,θ,q1,F),</p><pre><code>Q: 一系列状态
E： 边
θ： 计算公式
q1：起始状态
F: 结束状态
</code></pre><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/18248041.jpg" alt></p><p><strong>States</strong></p><blockquote><p>the start state,a[1], is where the matching process begins. It awaits input to start the Kleene plus and to select an event into the a[1] unit of the match buffer. At the next state a[i], it attempts to select another event into the a[i] (i &gt; 1) unit of the buffer. The subsequent state b denotes that the matching process has fulfilled the Kleene plus (for a particular match) and is ready to process the next pattern component. The final state, F, represents the completion of the process, resulting in the creation of a pattern match.</p></blockquote><p><strong>Edges</strong></p><blockquote><p>Each state is associated with a number of edges, representing the actions that can be taken at the state. As Figure 2(a) shows, each state that is a singleton state or the first state,p[1], of a pair has a forward begin edge. Each second state, p[i], of a pair has a forward proceed edge, and a looping take edge. Every state (except the start and final states) has a looping ignore edge. The start state has no edges to it as we are only interested in matches that start with selected events. Each edge at a state, q, is precisely described by a triplet:(1) a formula that specifies the condition on taking it, denoted by θq-edge, (2) an operation on the input stream (i.e.,consume an event or not), and (3) an operation on the match buffer (i.e., write to the buffer or not). Formulas of edges are compiled from pattern queries, which we explain in detail shortly. As shown in Figure 2(a), we use solid lines to denote begin and take edges that consume an event from the input and write it to the buffer, and dashed lines for ignore edges that consume an event but do not write it to the buffer. The proceed edge is a special edge: it does not consume any input event but only evaluates its formula and tries proceeding. We distinguish the proceed edge from ignore edges in the style of arrow, denoting its behavior.</p></blockquote><p><em>非确定性</em> 某些state具有两条边，但是这两条边的行为不一定是完全相反的</p><p>NFA算法过程：</p><ol><li>先根据整个structure构建state和边的行为</li><li>将condition翻译成边上的公式</li><li>将select strategy翻译成边上的公式</li><li>将时间窗口翻译成：最先匹配的元素和最后的元素时长不能超过窗口大小</li></ol><p>优化：</p><ol><li>先做过滤</li><li>将window条件前置</li><li>优化proceed边，进入下一个state的时候要检查是否满足begin的条件判断</li></ol><h3>带版本的内存共享</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/20508653.jpg" alt></p><p>通过带版本的共享内存解决多runs之间的重复事件问题。</p><h3>Computation state</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/48608682.jpg" alt></p><ol><li>version number of a run</li><li>current automaton state the run is in</li><li>a pointer to the most revent event selected into the buffer</li><li>a vector =&gt; 保留做edge判断的所需的最少的元素。例如上图中存储了a[i]的和和count为了计算平均值</li></ol><h3>Merge相同的Runs</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/87062132.jpg" alt></p><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/41465208.jpg" alt></p><p>将Query3中的avg算法改为max算法，他们在接收到e4之后达到了同样的状态，因此其之后的运算可以merge为一个。</p><ol><li>首先探测什么时候两个runs一样了，引入了算子M。每一个状态机都有一个mask，每一个mask对每一个Vector中列都有一个bit位，都相同时那么这两个run就是相等的。</li><li>创建combined的run<ol><li>时间设置为最小的那个时间</li></ol></li></ol><h3>Backtrack算法（回溯）</h3><p>每次只跑一个run，跑失败了再回退到分叉点跑另一条。也就是以广度优先搜索方式 <code>breadth first search manner</code></p><h3>待补充</h3><ol><li>性能评估方案</li><li>有哪些因素会影响计算速度和吞吐</li><li>内存优化管理措施</li></ol></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-window-source-code-analyse.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/flink-window-source-code-analyse.html" itemprop="url">flink中window实现的源码分析</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-07T02:05:43+08:00">2018-01-07 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/源码分析/" itemprop="url" rel="index"><span itemprop="name">源码分析</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-window-source-code-analyse.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-window-source-code-analyse.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">1.2k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">4</span></div></div></header><div class="post-body" itemprop="articleBody"><p>关于Flink中window的实现分析</p><p>&lt;!-- more --&gt;</p><h2>window</h2><p>window提供了一种处理无界数据的一种手段</p><h2>window的组件</h2><p>首先我们看window包含了哪些组件：触发器<code>trigger</code>，触发器上下文<code>triggerContext</code>，内部状态<code>windowState</code>，窗口分配器<code>windowassigner</code>, 内部时间服务器<code>internalTimerService</code>,初看到这么多的组件可能会有点懵，下面的分析会一点一点介绍这些组件的作用。</p><p>今天我们从flink接收流元素进行处理的角度来分析其实现，在flink的DAG中流动的有这么几种元素：<code>StreamRecord</code>,<code>LatencyRecord</code>,<code>WaterMark</code>,<code>StreamStatus</code>,我们这里只考虑这样两种元素<code>StreamRecord</code>和<code>WaterMark</code></p><h2>streamRecord</h2><p>windowOperator实现了<code>KeyContext</code>,其实就是代表每一个windowOperator处理每一个元素就会在一个Key的上下文的环境中去做处理。</p><p>当windowOperator接收到一条<code>StreamRecord</code>,windowOperator会做什么呢?</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">	numRecordsIn.inc();</span><br><span class="line">	streamOperator.setKeyContextElement1(record);</span><br><span class="line">	streamOperator.processElement(record);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><ol><li>首先设置该operator的key为当前元素</li><li>根据element所携带的时间戳（processing time或者event time）分配元素所应该属于的窗口，一个元素可能会隶属于多个窗口，比如slideWindowAssigner。</li><li>如果这个窗口是一个可merge窗口，例如session窗口，那么就会进行和原有窗口的合并和状态的更新</li></ol><h3>窗口merge原理</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-1-7/74574325.jpg" alt></p><ol><li>首先取出之前的所有未清除的窗口，和新分配到的窗口做一次merge，有重叠部分则新生成大的窗口</li><li>将各个小窗口的真实数据merge到合并后的大窗口</li><li>注册大窗口的清理时间触发器，清理原先子窗口的清理时间触发器</li></ol><p>窗口merge完之后，则会通过<code>triggerContext#OnElement</code>方法去进行判断是否能触发计算，触发方式如前文分析wartermark的那样，就是通过wartermark来判断这个窗口是否可以进行计算</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onElement</span><span class="params">(Object element, <span class="keyword">long</span> timestamp, TimeWindow window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (window.maxTimestamp() &lt;= ctx.getCurrentWatermark()) &#123;</span><br><span class="line">		<span class="comment">// if the watermark is already past the window fire immediately</span></span><br><span class="line">		<span class="keyword">return</span> TriggerResult.FIRE;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		ctx.registerEventTimeTimer(window.maxTimestamp());</span><br><span class="line">		<span class="keyword">return</span> TriggerResult.CONTINUE;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>如果不能触发计算，我们看到其实他是将<code>window.maxTimestamp</code>注册到了<code>eventTimeQueue</code>中, 这里我们先记一下，后文会提到他的作用。</p><h2>小结</h2><p>到这里windowOperator接收到一个<code>StreamRecord</code>元素的处理逻辑已经结束了，如果窗口不是可merge类型的除了不做窗口merge，其他的操作也是大同小异，到这里也许你会有几个疑问(其实是我自己看代码的一些疑问：)</p><ol><li>如果我设置了allowLateness会对我们的计算结果产生什么样的影响呢？</li><li>我是用<code>apply()</code>,<code>process()</code>函数或者<code>reduce()</code>这种聚合函数对于window的<code>cost</code>代价有多大的差别？</li></ol><h3>问题一</h3><p>上文中提到的在处理元素的最后会注册一个窗口cleanupTimer，那么这个时间是多少呢? <code>window.maxTimestamp() + allowedLateness;</code> 所以我们看到一个窗口存在时长是水位线经过<code>window的最大时间+allowLateness</code>的时间，因此当水位线大于窗口最大时间后就会触发计算，而计算之后状态不会清空，会保留<code>allowedLateness</code>的时长，而此时窗口状态还在保留，所以<strong>上游有晚到的数据来一条就会触发一次该窗口的计算</strong>，而且每次计算的数据都是该窗口的全量数据，所以业务方要慎用，或者下游要做好相应的去重或更新措施，否则可能会造成结果的不准确</p><h3>问题二</h3><p>不同的函数最终影响的其实是我们最终window保存数据的state的形式有<code>ListState</code>，也有<code>reducingState</code>...,最终影响了rocksdb和checkpoint的大小，不过肯定是能用聚合还是用聚合函数比较好</p><h2>waterMark</h2><p>当元素来的是一个watermark，window Operator又会以怎样的逻辑去处理呢?</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handleWatermark</span><span class="params">(Watermark watermark)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		<span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">			lastEmittedWatermark = watermark.getTimestamp();</span><br><span class="line">			operator.processWatermark(watermark);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">		<span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Exception occurred while processing valve output watermark: "</span>, e);		&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>这里我们将关注到开篇提到的<code>internalTimeService</code>,</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">advanceWatermark</span><span class="params">(<span class="keyword">long</span> time)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	currentWatermark = time;</span><br><span class="line"></span><br><span class="line">	InternalTimer&lt;K, N&gt; timer;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">while</span> ((timer = eventTimeTimersQueue.peek()) != <span class="keyword">null</span> &amp;&amp; timer.getTimestamp() &lt;= time) &#123;</span><br><span class="line"></span><br><span class="line">		Set&lt;InternalTimer&lt;K, N&gt;&gt; timerSet = getEventTimeTimerSetForTimer(timer);</span><br><span class="line">		timerSet.remove(timer);</span><br><span class="line">		eventTimeTimersQueue.remove();</span><br><span class="line"></span><br><span class="line">		keyContext.setCurrentKey(timer.getKey());</span><br><span class="line">		triggerTarget.onEventTime(timer);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>watermark元素处理逻辑</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-1-7/87202276.jpg" alt></p><ol><li>当流元素是watermark时主要处理逻辑集中在<code>internaltimerservice</code>上</li><li>如果<code>eventTimeTimersQueue</code>这个优先级队列中最早的时间低于了水位线，那么就会取出同一时刻的所有key的timer进行计算</li><li>处理可能意味着窗口的计算触发或者某些窗口的清理</li></ol><p>以上便是flink对于窗口的实现逻辑。</p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/why-actor-programming-model.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/why-actor-programming-model.html" itemprop="url">为什么现代系统架构需要一个新的编程模型 actor编程模型</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-17T21:32:20+08:00">2017-12-17 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/深度好文/" itemprop="url" rel="index"><span itemprop="name">深度好文</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/why-actor-programming-model.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="why-actor-programming-model.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">2.1k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">7</span></div></div></header><div class="post-body" itemprop="articleBody"><p>actor编程模型很早就由<code>Carl Hewitt</code>提出了, 最先是设计来解决在高性能的网络环境中的并发处理场景。只是当时没有这样的环境。如今，网络，硬件的基础设施的能力已经远远超越了那个时代，因此，当时提出的actor模型也有了用武之地。这篇文章讨论了关于传统编程的一些假设和现代实际的多线程，多核cpu架构不匹配的地方.</p><p>&lt;!-- more --&gt;</p><h1>封装所面临的挑战</h1><p>OOP思想的支柱是封装，封装意味着对象中的数据是不能从外界直接访问的，他仅能够通过调用内部的方法被修改。因此对象就承担起暴露<strong>正确</strong>，<strong>安全</strong>的方法来保护其封装变量的不变性。举个例子，对一个顺序二叉树来说一定不允许违反一个二叉树的排序不变性。调用者期望二叉树的顺序在查询某段数据的时候能够保持不变，从而他们能够依赖于这种约束。 当我们分析OOP应用的运行行为时，我们通常会会画一个消息序列图标来展示方法调用之间的交互，例如： <img src="http://or0igopk2.bkt.clouddn.com/17-12-14/20018675.jpg" alt> 不幸的是，上图没有精确的表达出，在执行的过程中这些对象的生命周期。实际上一个线程完成了所有这些调用。对变量不变性的要求是发生在方法调用的线程中： <img src="http://or0igopk2.bkt.clouddn.com/17-12-14/3331951.jpg" alt> 这样展示的意义在于你对多线程的情况下建模会把问题变的清晰。当有多个线程的情况下，事情变的不一样。 <img src="http://or0igopk2.bkt.clouddn.com/17-12-14/31370324.jpg" alt> 我们可以看到中间有一个环节有两个线程进入了统一个方法，但是该对象的封装并没有考虑如何处理这种场景。这两个方法的调用可能是随机交叉的，这就导致要向保证其内部变量的不变一致行必须引入两个线程之间的协调机制,通常的做法是引入锁来解决这个问题，从而确保同一时间只有一个线程在调用该方法.然而：</p><ul><li>锁机制严重限制了并发度，他们在如今cpu的架构下代价很大，需要操作系统暂停线程然后再过段时间再恢复</li><li>调用线程在block之后不能再做其他的工作</li><li>锁还引入了死锁问题。</li></ul><p><strong>导致的结果</strong></p><ul><li>没有锁状态可能会导致冲突</li><li>引入锁，性能会受损，并容易导致死锁,并且锁一般适用于单机的情况，当有多台机器协调工作时，就需要分布式锁，通常比本地锁的性能更差，更影响其可扩展性。</li></ul><h2>小结</h2><ul><li>对象仅仅能够保证在单线程情况下内部状态的一致性</li><li>锁机制在生产实际中显示并不高效</li></ul><h1>现代计算机架构内存共享的问题</h1><p>在现代计算机架构中，如果我们定义一个变量，cpu是将其写入cpu缓存，而不是将其直接写到内存中，绝大多数是写到临近该cpu核的cache中去，因此不能被别的cpu core所见，为了让一些本地的修改能让其他的核所见，cpu cache需要将其中的数据刷到其他核中。 在jvm中我们通过<code>volatile</code>关键字来明确一块内存地址，这块内存会被所有线程共享。那么为什么我们不将所有的变量能标记为<code>volatile</code>呢？因为将cpu cache的数据传递到各个cpu核是代价非常高的事情</p><h2>小结</h2><ul><li>没有真正的共享内存，cpu将数据传输到别的cpu核上，就像在网络中做的一样，cpu内部通讯和网络的通讯在实现上也有很多的共通之处。</li><li>与其通过标记为共享或使用原子数据结构的变量来隐藏消息传递，更规范和原则性的方法是将状态信息保存到一个并发实体中，并通过消息的机制显式地传播并发实体之间的数据或事件</li></ul><h1>调用栈的说明</h1><p>我们今天常常把所谓的堆栈视为理所当然，但是他们是在一个并发编程不那么重要的时代发明的，因为多cpu系统并不常见。调用堆栈不交叉线程，因此，不建模异步调用链。 当线程打算将任务委托给“后台任务”时，问题就出现了。实际上，这实际上意味着委托给另一个线程。这不能是一个简单的方法/函数调用，因为调用在线程中是严格的本地调用。通常发生的情况是，“调用者”将一个对象放入一个和被调用的工作线程共享的内存位置，这反过来调用者又可以在某个事件循环中获取它。这允许“caller”线程继续执行其他任务。 这种模式下第一个问题是，如何通知“调用者”任务已经完成了。当一个任务失败时，会出现一个更严重的问题。异常传播到什么地方?它将传播到工作线程的异常处理程序，而完全忽略实际的“调用者”是谁。就是无法将异常抛到主线程中去。 <img src="http://or0igopk2.bkt.clouddn.com/17-12-17/52377993.jpg" alt> 这是个严重的问题，工作的线程是如何处理这种情况的?它可能无法解决这个问题，因为它通常不知道失败的任务的目的“调用者”线程需要以某种方式被通知到，但是没有调用堆栈来解决异常。失败通知只能通过一个侧通道来完成，例如，在“调用者”线程希望得到结果的情况下，放置一个错误代码。如果此通知不到位，则“调用者”永远不会收到失败的通知，任务就会丢失!<strong>这与网络系统的工作方式惊人地相似，在没有任何通知的情况下，消息/请求可能会丢失/失败。</strong></p><p>这种情况再某些情况下会变得更糟，当一个由主线程发起的工作线程遇到了一个bug退出了最终会出现不可恢复的情况。一个由bug引起的内部异常会抛到线程的根，并使线程关闭。这立即引发了问题，谁应该重新启动由线程托管的服务的正常运行，以及如何恢复到已知的良好状态?乍一看，这似乎是可以管理的，但是我们突然面临一个新的、意想不到的现象。实际的任务，即线程当前正在进行的工作，不再位于任务从(通常是队列)中提取任务的共享内存位置。事实上，由于异常到达顶部，解除所有的调用堆栈，任务状态完全丢失!我们已经丢失了一条消息（即此时的工作线程已经退出），尽管这是本地通信，没有涉及到网络(消息丢失将被预期)。</p><h1>总结</h1><ul><li>为了在当前系统上实现任何有意义的并发性和性能，线程必须在不阻塞的情况下以有效的方式将任务分配给彼此。有了这种任务委托并发机制(甚至更多的是通过网络/分布式计算)基于调用堆栈的错误处理，需要引入新的显式的错误信号机制。并且失败成为领域模型的一部分。</li><li>代理模式下的并发系统需要处理服务故障，并有方法从它们中恢复，此类服务的客户端需要意识到任务/消息可能在重新启动时丢失。即使没有发生损失，也可能由于先前的排队任务(长队列)、垃圾收集导致的延迟等原因导致响应延迟。在这些情况下，并发系统应该以超时的形式处理响应期限，就像网络/分布式系统一样。</li></ul><h1>参考</h1><p>Akka 官网： <a href="https://doc.akka.io/docs/akka/2.5/guide/actors-motivation.html" target="_blank" rel="noopener">https://doc.akka.io/docs/akka/2.5/guide/actors-motivation.html</a></p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="aitozi"><p class="site-author-name" itemprop="name">aitozi</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">10</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">19</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="gjying1314@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-globe"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://wuchong.me/" title="wuchong" target="_blank">wuchong</a></li><li class="links-of-blogroll-item"><a href="http://chenyuzhao.me/" title="yuzhao" target="_blank">yuzhao</a></li><li class="links-of-blogroll-item"><a href="http://blog.csdn.net/yanghua_kobe?viewmode=contents" title="vinoyang" target="_blank">vinoyang</a></li><li class="links-of-blogroll-item"><a href="http://blog.csdn.net/lmalds?viewmode=contents" title="Imalds" target="_blank">Imalds</a></li><li class="links-of-blogroll-item"><a href="http://blog.csdn.net/androidlushangderen" title="hadoop" target="_blank">hadoop</a></li><li class="links-of-blogroll-item"><a href="http://www.cnblogs.com/xrq730/p/5260294.html" title="java开发" target="_blank">java开发</a></li><li class="links-of-blogroll-item"><a href="http://www.hollischuang.com/" title="阿里工程师" target="_blank">阿里工程师</a></li><li class="links-of-blogroll-item"><a href="http://www.cnblogs.com/fxjwind/" title="阿里流计算工程师" target="_blank">阿里流计算工程师</a></li><li class="links-of-blogroll-item"><a href="http://jm.taobao.org/" title="阿里中间件博客" target="_blank">阿里中间件博客</a></li><li class="links-of-blogroll-item"><a href="http://armsword.com/" title="duruofei" target="_blank">duruofei</a></li><li class="links-of-blogroll-item"><a href="http://blog.yufeng.info/" title="褚霸" target="_blank">褚霸</a></li><li class="links-of-blogroll-item"><a href="https://yuzhouwan.com" title="宇宙湾" target="_blank">宇宙湾</a></li><li class="links-of-blogroll-item"><a href="http://matt33.com" title="matt" target="_blank">matt</a></li><li class="links-of-blogroll-item"><a href="http://coding-geek.com/" title="coding-geek" target="_blank">coding-geek</a></li></ul></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">aitozi</span></div><div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div><div class="theme-info">主题 - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user"></i> <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="site-pv"><i class="fa fa-eye"></i> <span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/jquery.lazyload/1.9.3/jquery.lazyload.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.ui.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><script id="dsq-count-scr" src="https://aitozi.disqus.com/count.js" async></script><script>!function(){var t=document.createElement("script"),s=window.location.protocol.split(":")[0];"https"===s?t.src="https://zz.bdstatic.com/linksubmit/push.js":t.src="http://push.zhanzhang.baidu.com/push.js";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script></body></html><!-- rebuild by neat -->