<!-- build time:Sun Mar 31 2019 01:19:23 GMT+0800 (中国标准时间) --><!DOCTYPE html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="google-site-verification" content="DO25iswIsaKZ5NZbYreVDjWtBKTyo1yFAROjSRcJD64"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet" type="text/css"><link href="//fonts.loli.net/css?family=Lato:300,300italic,400,400italic,700,700italic|consolas:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="//cdn.bootcss.com/font-awesome/4.6.2/css/font-awesome.min.css" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css"><meta name="keywords" content="Hexo, NexT"><link rel="alternate" href="/atom.xml" title="Aitozi" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2"><meta name="description" content="What makes a difference? Persistence!"><meta property="og:type" content="website"><meta property="og:title" content="Aitozi"><meta property="og:url" content="http://www.aitozi.com/page/2/index.html"><meta property="og:site_name" content="Aitozi"><meta property="og:description" content="What makes a difference? Persistence!"><meta property="og:locale" content="zh-Hans"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Aitozi"><meta name="twitter:description" content="What makes a difference? Persistence!"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"left",display:"post",offset:12,offset_float:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:!0,duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://www.aitozi.com/page/2/"><title>Aitozi</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-home"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Aitozi</span> <span class="logo-line-after"><i></i></span></a></div><h1 class="site-subtitle" itemprop="description"></h1></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><section id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-state.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/flink-state.html" itemprop="url">flink中状态实现的深入理解</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-04T16:35:29+08:00">2018-08-04 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/源码分析/" itemprop="url" rel="index"><span itemprop="name">源码分析</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-state.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-state.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">3.2k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">13</span></div></div></header><div class="post-body" itemprop="articleBody"><p>本文是源于要在内部分享，所以提前整理了一些flink中的状态的一些知识，flink状态所包含的东西很多，在下面列举了一些，还有一 些在本文没有体现，后续会单独的挑出来再进行讲解</p><p>&lt;!-- more --&gt;</p><ul><li>state的层次结构</li><li>keyedState =&gt; windowState</li><li>OperatorState =&gt; kafkaOffset</li><li>stateBackend</li><li>snapshot/restore</li><li><em>internalTimerService</em></li><li><strong>RocksDB操作的初探</strong></li><li><em>state ttL</em></li><li><em>state local recovery</em></li><li><strong>QueryableState</strong></li><li><strong>increamental checkpoint</strong></li><li>state redistribution</li><li><em>broadcasting state</em></li><li><strong>CheckpointStreamFactory</strong></li></ul><hr><h3>内部和外部状态</h3><p>flink状态分为了内部和外部使用接口，但是两个层级都是一一对应，内部接口都实现了外部接口，主要是有两个目的</p><ul><li>内部接口提供了更多的方法，包括获取state中的serialize之后的byte，以及Namespace的操作方法。内部状态主要用于内部runtime实现时所需要用到的一些状态比如window中的windowState，CEP中的sharedBuffer,kafkaConsumer中offset管理的ListState,而外部State接口主要是用户自定义使用的一些状态</li><li>考虑到各个版本的兼容性，外部接口要保障跨版本之间的兼容问题，而内部接口就很少受到这个限制，因此也就比较灵活</li></ul><p>层次结构图：</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-8-2/82936981.jpg" alt></p><h3>状态的使用</h3><p>了解了flink 状态的层次结构，那么编程中和flink内部是如何使用这些状态呢？</p><p>flink中使用状态主要是两部分，一部分是函数中使用状态，另一部分是在operator中使用状态</p><p>方式：</p><ul><li>CheckpointedFunction</li><li>ListCheckpointed</li><li>RuntimeContext （DefaultKeyedStateStore）</li><li>StateContext</li></ul><p>StateContext</p><p>StateInitializationContext</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Iterable&lt;StatePartitionStreamProvider&gt; getRawOperatorStateInputs();</span><br><span class="line"></span><br><span class="line">Iterable&lt;KeyGroupStatePartitionStreamProvider&gt; getRawKeyedStateInputs();</span><br></pre></td></tr></table></figure><p></p><p>ManagedInitializationContext</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">OperatorStateStore getOperatorStateStore();</span><br><span class="line">KeyedStateStore getKeyedStateStore();</span><br></pre></td></tr></table></figure><p></p><p>举例：</p><ol><li><p>AbstractStreamOperator封装了这个方法<code>initializeState(StateInitializationContext context)</code>用以在operator中进行raw和managed的状态管理</p></li><li><p>CheckpointedFunction的用法其实也是借助于StateContext进行相关实现</p></li></ol><p><code>CheckpointedFunction#initializeState</code>方法在transformation function的各个并发实例初始化的时候被调用这个方法提供了<code>FunctionInitializationContext</code>的对象，可以通过这个<code>context</code>来获取<code>OperatorStateStore</code>或者<code>KeyedStateStore</code>，也就是说通过这个接口可以注册这两种类型的State，这也是和ListCheckpointed接口不一样的地方，只是说<code>KeyedStateStore</code>只能在keyedstream上才能注册，否则就会报错而已,以下是一个使用这两种类型状态的样例。 可以参见<code>FlinkKafkaConsumerBase</code>通过这个接口来实现offset的管理。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">public class MyFunction&lt;T&gt; implements MapFunction&lt;T, T&gt;, CheckpointedFunction &#123;</span><br><span class="line"></span><br><span class="line">     private ReducingState&lt;Long&gt; countPerKey;</span><br><span class="line">     private ListState&lt;Long&gt; countPerPartition;</span><br><span class="line"></span><br><span class="line">     private long localCount;</span><br><span class="line"></span><br><span class="line">     public void initializeState(FunctionInitializationContext context) throws Exception &#123;</span><br><span class="line">         // get the state data structure for the per-key state</span><br><span class="line">         countPerKey = context.getKeyedStateStore().getReducingState(</span><br><span class="line">                 new ReducingStateDescriptor&lt;&gt;(&quot;perKeyCount&quot;, new AddFunction&lt;&gt;(), Long.class));</span><br><span class="line"></span><br><span class="line">         // get the state data structure for the per-partition state</span><br><span class="line">         countPerPartition = context.getOperatorStateStore().getOperatorState(</span><br><span class="line">                 new ListStateDescriptor&lt;&gt;(&quot;perPartitionCount&quot;, Long.class));</span><br><span class="line"></span><br><span class="line">         // initialize the &quot;local count variable&quot; based on the operator state</span><br><span class="line">         for (Long l : countPerPartition.get()) &#123;</span><br><span class="line">             localCount += l;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     public void snapshotState(FunctionSnapshotContext context) throws Exception &#123;</span><br><span class="line">         // the keyed state is always up to date anyways</span><br><span class="line">         // just bring the per-partition state in shape</span><br><span class="line">         countPerPartition.clear();</span><br><span class="line">         countPerPartition.add(localCount);</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     public T map(T value) throws Exception &#123;</span><br><span class="line">         // update the states</span><br><span class="line">         countPerKey.add(1L);</span><br><span class="line">         localCount++;</span><br><span class="line"></span><br><span class="line">         return value;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p></p><p>这个Context的继承接口StateSnapshotContext的方法则提供了raw state的存储方法，但是其实没有对用户函数提供相应的接口，只是在引擎中有相关的使用，相比较而言这个接口提供的方法，context比较多，也有一些简单的方法去注册使用operatorstate 和 keyedState。如通过<code>RuntimeContext</code>注册keyedState:</p><p>因此使用简易化程度为:</p><blockquote><p>RuntimeContext &gt; FunctionInitializationContext &gt; StateSnapshotContext</p></blockquote><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">keyedStream.map(new RichFlatMapFunction&lt;MyType, List&lt;MyType&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">     private ListState&lt;MyType&gt; state;</span><br><span class="line"></span><br><span class="line">     public void open(Configuration cfg) &#123;</span><br><span class="line">         state = getRuntimeContext().getListState(</span><br><span class="line">                 new ListStateDescriptor&lt;&gt;(&quot;myState&quot;, MyType.class));</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     public void flatMap(MyType value, Collector&lt;MyType&gt; out) &#123;</span><br><span class="line">         if (value.isDivider()) &#123;</span><br><span class="line">             for (MyType t : state.get()) &#123;</span><br><span class="line">                 out.collect(t);</span><br><span class="line">             &#125;</span><br><span class="line">         &#125; else &#123;</span><br><span class="line">             state.add(value);</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;);</span><br></pre></td></tr></table></figure><p></p><p>通过实现<code>ListCheckpointed</code>来注册OperatorState，但是这个有限制： 一个function只能注册一个state，因为并不能像其他接口一样指定state的名字.</p><p>example：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">public class CountingFunction&lt;T&gt; implements MapFunction&lt;T, Tuple2&lt;T, Long&gt;&gt;, ListCheckpointed&lt;Long&gt; &#123;</span><br><span class="line"></span><br><span class="line">     // this count is the number of elements in the parallel subtask</span><br><span class="line">     private long count;</span><br><span class="line"></span><br><span class="line">     &#123;@literal @&#125;Override</span><br><span class="line">     public List&lt;Long&gt; snapshotState(long checkpointId, long timestamp) &#123;</span><br><span class="line">         // return a single element - our count</span><br><span class="line">         return Collections.singletonList(count);</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     &#123;@literal @&#125;Override</span><br><span class="line">     public void restoreState(List&lt;Long&gt; state) throws Exception &#123;</span><br><span class="line">         // in case of scale in, this adds up counters from different original subtasks</span><br><span class="line">         // in case of scale out, list this may be empty</span><br><span class="line">         for (Long l : state) &#123;</span><br><span class="line">             count += l;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     &#123;@literal @&#125;Override</span><br><span class="line">     public Tuple2&lt;T, Long&gt; map(T value) &#123;</span><br><span class="line">         count++;</span><br><span class="line">         return new Tuple2&lt;&gt;(value, count);</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p></p><p>下面比较一下里面的两种stateStore</p><ul><li>KeyedStateStore</li><li>OperatorStateStore</li></ul><p>查看OperatorStateStore接口可以看到OperatorState只提供了ListState一种形式的状态接口,OperatorState和KeyedState主要有以下几个区别：</p><ul><li>keyedState只能应用于KeyedStream，而operatorState都可以</li><li>keyedState可以理解成一个算子为每个subtask的每个key维护了一个状态namespace，而OperatorState是每个subtask共享一个状态</li><li>operatorState只提供了ListState，而keyedState提供了<code>ValueState</code>,<code>ListState</code>,<code>ReducingState</code>,<code>MapState</code></li><li>operatorStateStore的默认实现只有<code>DefaultOperatorStateBackend</code>可以看到他的状态都是存储在堆内存之中，而keyedState根据backend配置的不同，线上都是存储在rocksdb之中</li></ul><h3>snapshot</h3><p>这个让我们着眼于两个Operator的snapshot，<code>AbstractStreamOperator</code> 和 <code>AbstractUdfStreamOperator</code>,这两个基类几乎涵盖了所有相关operator和function在做snapshot的时候会做的处理。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">if (null != operatorStateBackend) &#123;</span><br><span class="line">				snapshotInProgress.setOperatorStateManagedFuture(</span><br><span class="line">					operatorStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions));</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			if (null != keyedStateBackend) &#123;</span><br><span class="line">				snapshotInProgress.setKeyedStateManagedFuture(</span><br><span class="line">					keyedStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><ol><li>按keyGroup去snapshot各个timerService的状态，包括processingTimer和eventTimer（RawKeyedOperatorState）</li><li>将operatorStateBackend和keyedStateBackend中的状态做snapshot</li><li>如果Operator还包含了userFunction，即是一个<code>UdfStreamOperator</code>,那么可以注意到udfStreamOperator覆写了父类的<code>snapshotState(StateSnapshotContext context)</code>方法，其主要目的就是为了将Function中的状态及时的register到相应的backend中，在第二步的时候统一由<code>CheckpointStreamFactory</code>去做快照</li></ol><h4>StreamingFunctionUtils#snapshotFunctionState</h4><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">if (userFunction instanceof CheckpointedFunction) &#123;</span><br><span class="line">			((CheckpointedFunction) userFunction).snapshotState(context);</span><br><span class="line"></span><br><span class="line">			return true;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if (userFunction instanceof ListCheckpointed) &#123;</span><br><span class="line">			@SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">			List&lt;Serializable&gt; partitionableState = ((ListCheckpointed&lt;Serializable&gt;) userFunction).</span><br><span class="line">				snapshotState(context.getCheckpointId(), context.getCheckpointTimestamp());</span><br><span class="line"></span><br><span class="line">			ListState&lt;Serializable&gt; listState = backend.</span><br><span class="line">				getSerializableListState(DefaultOperatorStateBackend.DEFAULT_OPERATOR_STATE_NAME);</span><br><span class="line"></span><br><span class="line">			listState.clear();</span><br><span class="line"></span><br><span class="line">			if (null != partitionableState) &#123;</span><br><span class="line">				try &#123;</span><br><span class="line">					for (Serializable statePartition : partitionableState) &#123;</span><br><span class="line">						listState.add(statePartition);</span><br><span class="line">					&#125;</span><br><span class="line">				&#125; catch (Exception e) &#123;</span><br><span class="line">					listState.clear();</span><br><span class="line"></span><br><span class="line">					throw new Exception(&quot;Could not write partitionable state to operator &quot; +</span><br><span class="line">						&quot;state backend.&quot;, e);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br></pre></td></tr></table></figure><p></p><p>可以看到这里就只有以上分析的两种类型的checkpoined接口，<code>CheckpointedFunction</code>，只需要执行相应的snapshot方法，相应的函数就已经将要做snapshot的数据打入了相应的state中，而<code>ListCheckpointed</code>接口由于返回的是个List，所以需要手动的通过<code>getSerializableListState</code>注册一个<code>ListState</code>(<em>这也是ListCheckpointed只能注册一个state的原因</em>),然后将List数据挨个存入ListState中。</p><h4>operatorStateBackend#snapshot</h4><ol><li>针对所有注册的state作deepCopy,为了防止在checkpoint的时候数据结构又被修改，deepcopy其实是通过序列化和反序列化的过程（参见<a href="http://aitozi.com/java-serialization.html" target="_blank" rel="noopener">http://aitozi.com/java-serialization.html</a>）</li><li>异步将state以及metainfo的数据写入到hdfs中，使用的是flink的asyncIO（这个也可以后续深入了解下），并返回相应的statehandle用作restore的过程</li><li>在StreamTask触发checkpoint的时候会将一个Task中所有的operator触发一次snapshot，触发部分就是上面1，2两个步骤，其中第二步是会返回一个RunnableFuture，在触发之后会提交一个<code>AsyncCheckpointRunnable</code>异步任务，会阻塞一直等到checkpoint的<code>Future</code>，其实就是去调用这个方法<code>AbstractAsyncIOCallable</code>, 直到完成之后OperatorState会返回一个<code>OperatorStateHandle</code>,这个地方和后文的keyedState返回的handle不一样。</li></ol><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">	public V call() throws Exception &#123;</span><br><span class="line"></span><br><span class="line">		synchronized (this) &#123;</span><br><span class="line">			if (isStopped()) &#123;</span><br><span class="line">				throw new IOException(&quot;Task was already stopped. No I/O handle opened.&quot;);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			ioHandle = openIOHandle();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		try &#123;</span><br><span class="line"></span><br><span class="line">			return performOperation();</span><br><span class="line"></span><br><span class="line">		&#125; finally &#123;</span><br><span class="line">			closeIOHandle();</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure><p></p><p>在managed keyedState、managed operatorState、raw keyedState、和raw operatorState都完成返回相应的Handle之后，会生成一个SubTaskState来ack jobmanager,这个主要是用在restore的过程中</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">SubtaskState subtaskState = createSubtaskStateFromSnapshotStateHandles(</span><br><span class="line">					chainedNonPartitionedOperatorsState,</span><br><span class="line">					chainedOperatorStateBackend,</span><br><span class="line">					chainedOperatorStateStream,</span><br><span class="line">					keyedStateHandleBackend,</span><br><span class="line">					keyedStateHandleStream);</span><br><span class="line">					</span><br><span class="line">owner.getEnvironment().acknowledgeCheckpoint(</span><br><span class="line">	checkpointMetaData.getCheckpointId(),</span><br><span class="line">	checkpointMetrics,</span><br><span class="line">	subtaskState);</span><br></pre></td></tr></table></figure><p></p><p>在jm端，ack的时候又将各个handle封装在<code>pendingCheckpoint =&gt; operatorStates =&gt; operatorState =&gt; operatorSubtaskState</code>中,最后无论是savepoint或者是externalCheckpoint都会将相应的handle序列化存储到hdfs，这也就是所谓的checkpoint元数据。这个可以起个任务观察下zk和hdfs上的文件，补充一下相关的验证。</p><p>至此完成operator state的snapshot/checkpoint阶段</p><h4>KeyedStateBackend#snapshot</h4><p>和operatorStateBackend一样，snapshot也分为了同步和异步两个部分。</p><ol><li>rocksDB的keyedStateBackend的snapshot提供了增量和全量两种方式</li><li>利用rocksdb自身的snapshot进行<code>this.snapshot = stateBackend.db.getSnapshot();</code> 这个过程是同步的，rocksdb这块是怎么snapshot还不是很了解，待后续学习</li><li>之后也是一样异步将数据写入hdfs，返回相应的keyGroupsStateHandle <code>snapshotOperation.closeCheckpointStream();</code></li></ol><p>不同的地方在于增量返回的是<code>IncrementalKeyedStateHandle</code>,而全量返回的是<code>KeyGroupsStateHandle</code>，</p><h3>restore / redistribution</h3><h4>OperatorState的rescale</h4><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">void setInitialState(TaskStateHandles taskStateHandles) throws Exception;</span><br></pre></td></tr></table></figure><p></p><p>一个task在真正的执行任务之前所需要做的事情是把状态inject到task中，如果一个任务是失败之后从上次的checkpoint点恢复的话，他的状态就是非空的。streamTask也就靠是否有这样的一个恢复状态来确认算子是不是在restore来branch他的启动逻辑</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">if (null != taskStateHandles) &#123;</span><br><span class="line">		if (invokable instanceof StatefulTask) &#123;</span><br><span class="line">			StatefulTask op = (StatefulTask) invokable;</span><br><span class="line">			op.setInitialState(taskStateHandles);</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			throw new IllegalStateException(&quot;Found operator state for a non-stateful task invokable&quot;);</span><br><span class="line">		&#125;</span><br><span class="line">		// be memory and GC friendly - since the code stays in invoke() for a potentially long time,</span><br><span class="line">		// we clear the reference to the state handle</span><br><span class="line">		//noinspection UnusedAssignment</span><br><span class="line">		taskStateHandles = null;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>那么追根究底一下这个Handle是怎么带入的呢？</p><p><code>FixedDelayRestartStrategy =&gt; triggerFullRecovery =&gt; Execution#restart =&gt; Execution#scheduleForExecution =&gt; Execution#deployToSlot =&gt; ExecutionVertex =&gt; TaskDeploymentDescriptor =&gt; taskmanger =&gt; task</code></p><p>当然还有另一个途径就是通过向jobmanager submitJob的时候带入restore的checkpoint path， 这两种方式最终都会通过<code>checkpointCoordinator#restoreLatestCheckpointedState</code>来恢复hdfs中的状态来获取到snapshot时候存入的StateHandle。</p><p>恢复的过程如何进行redistribution呢？ 也就是大家关心的并发度变了我的状态的行为是怎么样的。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// re-assign the task states</span><br><span class="line">final Map&lt;OperatorID, OperatorState&gt; operatorStates = latest.getOperatorStates();</span><br><span class="line"></span><br><span class="line">StateAssignmentOperation stateAssignmentOperation =</span><br><span class="line">		new StateAssignmentOperation(tasks, operatorStates, allowNonRestoredState);</span><br><span class="line"></span><br><span class="line">stateAssignmentOperation.assignStates();</span><br></pre></td></tr></table></figure><p></p><ol><li>如果并发度没变那么不做重新的assign，除非state的模式是broadcast，会将一个task的state广播给所有的task</li><li>对于operator state会针对每一个name的state计算出每个subtask中的element个数之和（这就要求每个element之间相互独立）进行roundrobin分配</li><li>keyedState的重新分配相对简单，就是根据新的并发度和最大并发度计算新的keygroupRange，然后根据subtaskIndex获取keyGroupRange，然后获取到相应的keyStateHandle完成状态的切分。</li></ol><p>这里补充关于raw state和managed state在rescale上的差别，由于operator state在reassign的时候是根据metaInfo来计算出所有的List&lt;element&gt;来重新分配，operatorbackend中注册的状态是会保存相应的metainfo，最终也会在snapshot的时候存入OperatorHandle，那raw state的metainfo是在哪里呢？</p><p>其实会在写入hdfs返回相应的handle的时候构建一个默认的，<code>OperatorStateCheckpointOutputStream#closeAndGetHandle</code>,其中状态各个partition的构建来自<code>startNewPartition</code>方法，引擎中我所看到的rawstate仅有timerservice的raw keyedState</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">OperatorStateHandle closeAndGetHandle() throws IOException &#123;</span><br><span class="line">		StreamStateHandle streamStateHandle = delegate.closeAndGetHandle();</span><br><span class="line"></span><br><span class="line">		if (null == streamStateHandle) &#123;</span><br><span class="line">			return null;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if (partitionOffsets.isEmpty() &amp;&amp; delegate.getPos() &gt; initialPosition) &#123;</span><br><span class="line">			startNewPartition();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		Map&lt;String, OperatorStateHandle.StateMetaInfo&gt; offsetsMap = new HashMap&lt;&gt;(1);</span><br><span class="line"></span><br><span class="line">		OperatorStateHandle.StateMetaInfo metaInfo =</span><br><span class="line">				new OperatorStateHandle.StateMetaInfo(</span><br><span class="line">						partitionOffsets.toArray(),</span><br><span class="line">						OperatorStateHandle.Mode.SPLIT_DISTRIBUTE);</span><br><span class="line"></span><br><span class="line">		offsetsMap.put(DefaultOperatorStateBackend.DEFAULT_OPERATOR_STATE_NAME, metaInfo);</span><br><span class="line"></span><br><span class="line">		return new OperatorStateHandle(offsetsMap, streamStateHandle);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><h4>KeyedState的keyGroup</h4><p>keyedState重新分配里引入了一个keyGroup的概念，那么这里为什么要引入keygroup这个概念呢？</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-8-3/53377760.jpg" alt></p><p><img src="http://or0igopk2.bkt.clouddn.com/18-8-3/18711916.jpg" alt></p><ol><li>hash(key) = key(identity)</li><li>key_group(key) = hash(key) % number_of_key_groups (等于最大并发)，默认flink任务会设置一个max parallel</li><li>subtask(key) = key_greoup(key) * parallel / number_of_key_groups</li></ol><ul><li>避免在恢复的时候带来随机IO</li><li>避免每个subtask需要将所有的状态数据读取出来pick和自己subtask相关的浪费了很多io资源</li><li>减少元数据的量，不再需要保存每次的key，每一个keygroup组只需保留一个range</li></ul><p>实际实现上的keyGroup range和上图有区别，是连续的:</p><p>比如：subtask1: [0-10], subtask2: [11-12] ...</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int start = operatorIndex == 0 ? 0 : ((operatorIndex * maxParallelism - 1) / parallelism) + 1;</span><br><span class="line">int end = ((operatorIndex + 1) * maxParallelism - 1) / parallelism;</span><br><span class="line">return new KeyGroupRange(start, end);</span><br></pre></td></tr></table></figure><p></p><ul><li>每一个backend（subtask）上只有一个keygroup range</li><li>每一个subtask在restore的时候就接收到了已经分配好的和重启后当前这个并发相绑定的keyStateHandle</li></ul><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">subManagedKeyedState = getManagedKeyedStateHandles(operatorState, keyGroupPartitions.get(subTaskIndex));</span><br><span class="line">subRawKeyedState = getRawKeyedStateHandles(operatorState, keyGroupPartitions.get(subTaskIndex));</span><br></pre></td></tr></table></figure><p></p><p>这里面关键的一步在于，根据新的subtask上的keyGroupRange，从原来的operator的keyGroupsStateHandle中求取本subtask所关心的一部分Handle，可以看到每个KeyGroupsStateHandle都维护了<code>KeyGroupRangeOffsets</code>这样一个变量，来标记这个handle所覆盖的keygrouprange，以及keygrouprange在stream中offset的位置，可以看下再snapshot的时候会记录offset到这个对象中来</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keyGroupRangeOffsets.setKeyGroupOffset(mergeIterator.keyGroup(), outStream.getPos());</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public KeyGroupRangeOffsets getIntersection(KeyGroupRange keyGroupRange) &#123;</span><br><span class="line">		Preconditions.checkNotNull(keyGroupRange);</span><br><span class="line">		KeyGroupRange intersection = this.keyGroupRange.getIntersection(keyGroupRange);</span><br><span class="line">		long[] subOffsets = new long[intersection.getNumberOfKeyGroups()];</span><br><span class="line">		if(subOffsets.length &gt; 0) &#123;</span><br><span class="line">			System.arraycopy(</span><br><span class="line">					offsets,</span><br><span class="line">					computeKeyGroupIndex(intersection.getStartKeyGroup()),</span><br><span class="line">					subOffsets,</span><br><span class="line">					0,</span><br><span class="line">					subOffsets.length);</span><br><span class="line">		&#125;</span><br><span class="line">		return new KeyGroupRangeOffsets(intersection, subOffsets);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p>KeyGroupsStateHandle是一个subtask的所有state的一个handle KeyGroupsStateHandle维护一个KeyGroupRangeOffsets， KeyGroupRangeOffsets维护一个KeyGroupRange和offsets KeyGroupRange维护多个KeyGroup KeyGroup维护多个key</p><p>KeyGroupsStateHandle和operatorStateHandle还有一个不同点，operatorStateHandle维护了metainfo中的offset信息用在restore时的reassign，原因在于KeyGroupsStateHandle的reassign不依赖这些信息，当然在restore的时候也需要keygroupOffset中的offset信息来重新构建keyGroupsStateHandle来进行各个task的状态分配。</p><p>参考：</p><p><a href="https://flink.apache.org/features/2017/07/04/flink-rescalable-state.html" target="_blank" rel="noopener">https://flink.apache.org/features/2017/07/04/flink-rescalable-state.html</a></p><p><a href="http://chenyuzhao.me/2017/12/24/Flink-%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E7%9A%84%E8%AE%BE%E8%AE%A1-%E5%AD%98%E5%82%A8/" target="_blank" rel="noopener">http://chenyuzhao.me/2017/12/24/Flink-分布式快照的设计-存储/</a></p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/dig-protobuf.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/dig-protobuf.html" itemprop="url">Protobuf深入理解</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-28T23:28:01+08:00">2018-07-28 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/刨根问底/" itemprop="url" rel="index"><span itemprop="name">刨根问底</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/dig-protobuf.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="dig-protobuf.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">3.3k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">13</span></div></div></header><div class="post-body" itemprop="articleBody"><p>本文带你深入理解和使用protobuf</p><p>&lt;!--more--&gt;</p><h2>简介</h2><p>Protocol Buffers 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python、Go 等语言的 API</p><p>使用protobuf需要这样几个步骤：</p><ul><li>在<code>.proto</code>文件中定义消息的格式</li><li>通过protoBuffer compiler编译生成相应的java类</li><li>通过Java protocol buffer Api来write和read相关的对象</li></ul><p>关于PB的操作方式见： <a href="https://developers.google.com/protocol-buffers/docs/javatutorial" target="_blank" rel="noopener">https://developers.google.com/protocol-buffers/docs/javatutorial</a></p><hr><p>protobuf的序列化快的原因主要在于其编码实现和封解包的速度</p><h2>Protobuf编码</h2><h3>Base 128 Varints 编码</h3><p>数据传输中出于IO的考虑，我们会希望尽可能的对数据进行压缩。 Varint就是一种对数字进行编码的方法，编码后二进制数据是不定长的，数值越小的数字使用的字节数越少。例如对于int32_t，采用Varint编码后需要1~5个bytes，小的数字使用1个byte，大的数字使用5个bytes。基于实际场景中小数字的使用远远多于大数字，因此通过Varint编码对于大部分场景都可以起到一个压缩的效果。Varint的主要想法就是以标志位替换掉高字节的若干个0</p><p>下图是数字131415的variant编码,通过3个字节来表示131415 <img src="http://or0igopk2.bkt.clouddn.com/18-7-28/50478975.jpg" alt></p><p>其中第一个字节的高位msb（Most Significant Bit ）为1表示下一个字节还有有效数据，msb为0表示该字节中的后7为是最后一组有效数字。踢掉最高位后的有效位组成真正的数字。注意到最终计算前将两个 byte 的位置相互交换过一次，这是因为 Google Protocol Buffer 字节序采用 little-endian（即低位字节排放在内存的低地址端） 的方式</p><p>从上面可以看出，variant编码存储比较小的整数时很节省空间，小于等于127的数字可以用一个字节存储。但缺点是对于大于</p><p>268,435,455（0xfffffff）的整数需要5个字节来存储。但是一般情况下（尤其在tag编码中）不会存储这么大的整数。</p><p>关于int32的varint编码代码</p><p></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">EncodeVarint32</span><span class="params">(<span class="keyword">char</span>* dst, <span class="keyword">uint32_t</span> v)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Operate on characters as unsigneds</span></span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">char</span>* ptr = <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">char</span>*&gt;(dst);</span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> B = <span class="number">128</span>;</span><br><span class="line">  <span class="keyword">if</span> (v &lt; (<span class="number">1</span>&lt;&lt;<span class="number">7</span>)) &#123;</span><br><span class="line">    *(ptr++) = v;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span>&lt;&lt;<span class="number">14</span>)) &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = v&gt;&gt;<span class="number">7</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span>&lt;&lt;<span class="number">21</span>)) &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = (v&gt;&gt;<span class="number">7</span>) | B;</span><br><span class="line">    *(ptr++) = v&gt;&gt;<span class="number">14</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span>&lt;&lt;<span class="number">28</span>)) &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = (v&gt;&gt;<span class="number">7</span>) | B;</span><br><span class="line">    *(ptr++) = (v&gt;&gt;<span class="number">14</span>) | B;</span><br><span class="line">    *(ptr++) = v&gt;&gt;<span class="number">21</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = (v&gt;&gt;<span class="number">7</span>) | B;</span><br><span class="line">    *(ptr++) = (v&gt;&gt;<span class="number">14</span>) | B;</span><br><span class="line">    *(ptr++) = (v&gt;&gt;<span class="number">21</span>) | B;</span><br><span class="line">    *(ptr++) = v&gt;&gt;<span class="number">28</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">char</span>*&gt;(ptr);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>Message Structure编码</h3><p>protocol buffer 中 message 是一系列键值对。message 的二进制版本只是使用字段号(field's number 和 wire_type)作为 key。每个字段的名称和声明类型只能在解码端通过引用消息类型的定义（即 .proto 文件）来确定。这一点也是人们常常说的 protocol buffer 比 JSON，XML 安全一点的原因，如果没有数据结构描述 .proto 文件，拿到数据以后是无法解释成正常的数据的。</p><p>当消息编码时，键和值被连接成一个字节流。当消息被解码时，解析器需要能够跳过它无法识别的字段。这样，可以将新字段添加到消息中，而不会破坏不知道它们的旧程序。这就是所谓的 “向后”兼容性。</p><p>为此，线性的格式消息中每对的“key”实际上是两个值，其中一个是来自.proto文件的字段编号，加上提供正好足够的信息来查找下一个值的长度。在大多数语言实现中，这个 key 被称为 tag</p><p>wireType</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-7-28/87891770.jpg" alt></p><p>key 的计算方法是 (field_number &lt;&lt; 3) | wire_type，换句话说，key 的最后 3 位表示的就是 wire_type。因此这里也涉及到前面proto文件定义的时候的宗旨，尽量将频繁使用的字段的字段号设置成1-15之间的数值，避免位数开销。这里的key的存储也是用了varint的方式</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-7-28/94576622.jpg" alt></p><p>举例，一般 message 的字段号都是 1 开始的，所以对应的 tag 可能是这样的：</p><p><code>000 1000</code></p><p>末尾3位表示的是value的类型，这里是000，即0，代表的是varint值。右移3位，即0001，这代表的就是字段号(field number)。tag的例子就举这么多，接下来举一个 value的例子，还是用varint来举例：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">96 01 = 1001 0110  0000 0001</span><br><span class="line">       → 000 0001  ++  001 0110 (drop the msb and reverse the groups of 7 bits)</span><br><span class="line">       → 10010110</span><br><span class="line">       → 128 + 16 + 4 + 2 = 150</span><br></pre></td></tr></table></figure><p></p><p>所以 96 01 代表的数据就是 150 。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">message Test1 &#123;</span><br><span class="line">  required int32 a = 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>如果存在上面这样的一个 message 的结构，如果存入 150，在 Protocol Buffer 中显示的二进制应该为 08 96 01 <code>varint(1 &lt;&lt; 3 | 0) = 0x08</code>.</p><p>注意到varint编码也应用在了key的计算上，使用非常频繁，或许是基于这个原因，pb里实现了一种性能更高的方案（coded_stream.cc）</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">inline uint8* CodedOutputStream::WriteVarint32FallbackToArrayInline(</span><br><span class="line">    uint32 value, uint8* target) &#123;</span><br><span class="line">  target[0] = static_cast&lt;uint8&gt;(value | 0x80);</span><br><span class="line">  if (value &gt;= (1 &lt;&lt; 7)) &#123;</span><br><span class="line">    target[1] = static_cast&lt;uint8&gt;((value &gt;&gt;  7) | 0x80);</span><br><span class="line">    if (value &gt;= (1 &lt;&lt; 14)) &#123;</span><br><span class="line">      target[2] = static_cast&lt;uint8&gt;((value &gt;&gt; 14) | 0x80);</span><br><span class="line">      if (value &gt;= (1 &lt;&lt; 21)) &#123;</span><br><span class="line">        target[3] = static_cast&lt;uint8&gt;((value &gt;&gt; 21) | 0x80);</span><br><span class="line">        if (value &gt;= (1 &lt;&lt; 28)) &#123;</span><br><span class="line">          target[4] = static_cast&lt;uint8&gt;(value &gt;&gt; 28);</span><br><span class="line">          return target + 5;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          target[3] &amp;= 0x7F;</span><br><span class="line">          return target + 4;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        target[2] &amp;= 0x7F;</span><br><span class="line">        return target + 3;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      target[1] &amp;= 0x7F;</span><br><span class="line">      return target + 2;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    target[0] &amp;= 0x7F;</span><br><span class="line">    return target + 1;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>测试了1kw条数据，两种方案的时间对比为 196742us vs 269806us，在pb序列化反序列化大量使用varint的前提下，这个性能提升就很有必要了(这是原作者做的测试)</p><p>type 需要注意的是 type = 2 的情况，tag 里面除了包含 field number 和 wire_type ，还需要再包含一个 length，决定 value 从那一段取出来</p><h3>负数使用varint编码的问题</h3><p>varint编码希望以标志位能够节省掉高字节的0，但是负数的最高位一定是1， 所以varint在处理32位负数时会固定的占用5个字节。比如我们修改下之前的程序test.set_a(-1)，序列化之后的数据为</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">08ff ffff ffff ffff ffff 01</span><br></pre></td></tr></table></figure><p></p><p>有11个字节之多！除了key=0x08占用的1个字节，value=-1占用了10个字节。</p><p>对应的代码（coded_stream.h）</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">inline void CodedOutputStream::WriteVarint32SignExtended(int32 value) &#123;</span><br><span class="line">  if (value &lt; 0) &#123;</span><br><span class="line">    WriteVarint64(static_cast&lt;uint64&gt;(value));</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    WriteVarint32(static_cast&lt;uint32&gt;(value));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>int32被转换成了uint64(为什么？)原作者这里问为什么== 原因在文档中有提及</p><blockquote><p>If you use int32 or int64 as the type for a negative number, the resulting varint is always ten bytes long – it is, effectively, treated like a very large unsigned integer,即uint64，也就是上面代码写的那样。但是为什么生成是10个字节呢? 因为uint64是10个?</p></blockquote><p>再经过varint编码。这就是10个字节的原因了。当然如果你使用了signed types那么产出的varint编码结果使用了Zigzag编码就会相当的高效。</p><h3>Zigzag编码</h3><p>ZigZag是将有符号数统一映射到无符号数的一种编码方案，对于无符号数0 1 2 3 4，映射前的有符号数分别为0 -1 1 -2 2，负数以及对应的正数来回映射到从0变大的数字序列里，这也是”zig-zag”的名字来源。将所有整数映射成无符号整数，然后再采用 varint 编码方式编码，这样，绝对值小的整数，编码后也会有一个较小的 varint 编码值。</p><p>Zigzag 映射函数为：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Zigzag(n) = (n &lt;&lt; 1) ^ (n &gt;&gt; 31), n 为 sint32 时</span><br><span class="line">Zigzag(n) = (n &lt;&lt; 1) ^ (n &gt;&gt; 63), n 为 sint64 时</span><br></pre></td></tr></table></figure><p></p><p>按照这种方法，-1 将会被编码成 1，1 将会被编码成 2，-2 会被编码成 3，如下表所示：</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-7-28/17159197.jpg" alt></p><p>存疑？</p><p>目前仍有一个地方不大清楚，就是对于int32类型的负数，protobuf强制编码成10个字节，理论上5个字节就够了。 （来自别人的问题，我也没懂，确实想了下int32的负数5个就够了，int64的负数才需要10个？）</p><h3>负数及大整数的解决方案</h3><p>protobuf里提供了一种sint32/sint64来使用ZigZag编码。</p><p>修改proto:optional sint32 a = 1，这样在test.set_a(-1)并序列化后只有两个字节08 01</p><p>同理对于大整数，optional int32 a = 1;，test.set_a(1 &lt;&lt; 28)序列化后可以看到占用了6个字节0880 8080 8001，解决方案也是使用不同的类型定义optional <strong>fixed32</strong> a = 1来解决，使用这种方案后int32固定的占用4个字节。这种其实就是官网中的<code>Non-varint Numbers</code></p><h3>字符串</h3><p>wire_type 类型为 2 的数据，是一种指定长度的编码方式：key + length + content，key 的编码方式是统一的，length 采用 varints 编码方式，content 就是由 length 指定长度的 Bytes</p><p>举例，假设定义如下的 message 格式：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">message Test2 &#123;</span><br><span class="line">  optional string b = 2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>设置该值为&quot;testing&quot;，二进制格式查看：<code>12 07 74 65 73 74 69 6e 67</code>, <code>74 65 73 74 69 6e 67</code> 是“testing”的 UTF8 代码。</p><p>12 -&gt; 0001 0010，后三位 010 为 wire type = 2，0001 0010 右移三位为 0000 0010，即 tag = 2。</p><p>length 此处为 7，后边跟着 7 个bytes，即我们的字符串&quot;testing&quot;。</p><p>所以 wire_type 类型为 2 的数据，编码的时候会默认转换为 T-L-V (Tag - Length - Value)的形式. TLV的模式减少了分隔符的使用，数据存储更加紧凑。需要转变为 T - L - V 形式的还有 string, bytes, embedded messages, packed repeated fields。</p><h3>小结</h3><ul><li>Protocol Buffer 利用 varint 原理压缩数据以后，二进制数据非常紧凑，option 也算是压缩体积的一个举措。所以 pb 体积更小，如果选用它作为网络数据传输，势必相同数据，消耗的网络流量更少。但是并没有压缩到极限，float、double 浮点型都没有压缩。</li><li>Protocol Buffer 比 JSON 和 XML 少了 {、}、: 这些符号，体积也减少一些。再加上 varint 压缩，gzip 压缩以后体积更小！</li><li>Protocol Buffer 是 Tag - Value (Tag - Length - Value)的编码方式的实现，减少了分隔符的使用，数据存储更加紧凑。</li><li>Protocol Buffer 另外一个核心价值在于提供了一套工具，一个编译工具，自动化生成 get/set 代码。简化了多语言交互的复杂度，使得编码解码工作有了生产力。</li><li>Protocol Buffer 不是自我描述的，离开了数据描述 .proto 文件，就无法理解二进制数据流。这点即是优点，使数据具有一定的“加密性”，也是缺点，数据可读性极差。所以 Protocol Buffer 非常适合内部服务之间 RPC 调用和传递数据。</li><li>Protocol Buffer 具有向后兼容的特性，更新数据结构以后，老版本依旧可以兼容，这也是 Protocol Buffer 诞生之初被寄予解决的问题。因为编译器对不识别的新增字段会跳过不处理</li></ul><h2>Protobuf反序列化</h2><p><a href="https://halfrost.com/protobuf_encode/" target="_blank" rel="noopener">https://halfrost.com/protobuf_encode/</a></p><p>整个解析过程需要 Protobuf 本身的框架代码和由 Protobuf 编译器生成的代码共同完成。Protobuf 提供了基类 Message 以及 Message_lite 作为通用的 Framework，，CodedInputStream 类，WireFormatLite 类等提供了对二进制数据的 decode 功能，从 5.1 节的分析来看，Protobuf 的解码可以通过几个简单的数学运算完成，无需复杂的词法语法分析，因此 ReadTag() 等方法都非常快。 在这个调用路径上的其他类和方法都非常简单，感兴趣的读者可以自行阅读。 相对于 XML 的解析过程，以上的流程图实在是非常简单吧？这也就是 Protobuf 效率高的第二个原因了</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-7-28/17159197.jpg" alt></p><h2>与json thift的性能比较</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzA3NDcyMTQyNQ==&amp;mid=2649257430&amp;idx=1&amp;sn=975b6123d8256221f6bac3b99e52af9a&amp;chksm=8767a428b0102d3e6ab7abdf797c481da570cb29e274aa4ff6ecd931f535166b776e6548941d&amp;scene=0&amp;key=399a205ce674169cbedcc1c459650908e22d6a2b81674195c3b251114acdf821dbde7bb49102c6b47f61b26a7a404d74e0e8440cea3675a7ea8f49eafd8639bfb733183a1bfb4603232d6cb8ecd230e5&amp;ascene=0&amp;uin=NTkxMDk2NjU=&amp;devicetype=iMac+MacBookPro12,1+OSX+OSX+10.12.4+build(16E195)&amp;version=12020510&amp;nettype=WIFI&amp;fontScale=100&amp;pass_ticket=wHPj0w18CV8zHl6HCfd9t9LQfs3I0ZULhUILuOHgL0E=" target="_blank" rel="noopener">Protobuf有没有比JSON快5倍？用代码来击破pb性能神话</a></p><h2>总结</h2><p>protobuf的性能来源于对存储的压缩，避免一切不必要的字节开销。flink中如过将大多数需要存储到state中的对象先转成PB格式会得到很大的性能提升。</p><p>和protobuf通常被同时提及的有Apache Thrift / Avro 本文已经没有空间介绍，待后续深入了解。</p><p>这篇文章介绍了3中数据结构如何做到对消息体格式演变的透明 <a href="https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html" target="_blank" rel="noopener">https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html</a></p><p>参考：</p><p><a href="https://developers.google.com/protocol-buffers/docs/javatutorial" target="_blank" rel="noopener">https://developers.google.com/protocol-buffers/docs/javatutorial</a> <a href="https://developers.google.com/protocol-buffers/docs/encoding#structure" target="_blank" rel="noopener">https://developers.google.com/protocol-buffers/docs/encoding#structure</a> <a href="https://halfrost.com/protobuf_encode/" target="_blank" rel="noopener">https://halfrost.com/protobuf_encode/</a> <a href="https://izualzhy.cn/protobuf-encode-varint-and-zigzag" target="_blank" rel="noopener">https://izualzhy.cn/protobuf-encode-varint-and-zigzag</a> <a href="https://izualzhy.cn/protobuf-encoding" target="_blank" rel="noopener">https://izualzhy.cn/protobuf-encoding</a> <a href="https://www.ibm.com/developerworks/cn/linux/l-cn-gpb/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/l-cn-gpb/index.html</a> <a href="https://segmentfault.com/a/1190000004891020" target="_blank" rel="noopener">https://segmentfault.com/a/1190000004891020</a> protobufstuff <a href="https://mp.weixin.qq.com/s?__biz=MzA3NDcyMTQyNQ==&amp;mid=2649257430&amp;idx=1&amp;sn=975b6123d8256221f6bac3b99e52af9a&amp;chksm=8767a428b0102d3e6ab7abdf797c481da570cb29e274aa4ff6ecd931f535166b776e6548941d&amp;scene=0&amp;key=399a205ce674169cbedcc1c459650908e22d6a2b81674195c3b251114acdf821dbde7bb49102c6b47f61b26a7a404d74e0e8440cea3675a7ea8f49eafd8639bfb733183a1bfb4603232d6cb8ecd230e5&amp;ascene=0&amp;uin=NTkxMDk2NjU=&amp;devicetype=iMac+MacBookPro12,1+OSX+OSX+10.12.4+build(16E195)&amp;version=12020510&amp;nettype=WIFI&amp;fontScale=100&amp;pass_ticket=wHPj0w18CV8zHl6HCfd9t9LQfs3I0ZULhUILuOHgL0E=" target="_blank" rel="noopener">1</a>: &quot;https://mp.weixin.qq.com/s&quot;</p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/java-serialization.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/java-serialization.html" itemprop="url">Java序列化拾掇</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-28T00:51:18+08:00">2018-07-28 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/刨根问底/" itemprop="url" rel="index"><span itemprop="name">刨根问底</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/java-serialization.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="java-serialization.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">2.8k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">11</span></div></div></header><div class="post-body" itemprop="articleBody"><p>Java序列化拾掇</p><p>&lt;!-- more --&gt;</p><blockquote><p>本来想总结一下对google protobuf的用法总结，然而搜资料的过程中发现对很多java序列化的知识不足，故做了一些拾掇，在flink中关于类型序列化的地方其实也涉及很多，待以后看到，如有新的思考再来补充。</p></blockquote><h2>java序列化</h2><ol><li>在Java中，只要一个类实现了java.io.Serializable接口，那么它就可以被序列化</li><li>若父类未实现Serializable,而子类序列化了，父类属性值不会被保存，反序列化后父类属性值丢失</li><li>通过ObjectOutputStream和ObjectInputStream对对象进行序列化及反序列化</li><li>在deserialized的时候类的构造器是不会被调用的，只会调用没有实现Serializabe接口的父类的无参构造方法，如果其父类不可序列化，并且没有无参构造函数就会导致<code>InvalidClassException</code></li><li>只有non-static的成员并且没有标记为transient才会被序列化</li><li>类所包含的成员变量也必须是可序列化的</li><li>transient 关键字的作用是控制变量的序列化，在变量声明前加上该关键字，可以阻止该变量被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 型的是 0，对象型的是 null</li><li>java的serialVersionUID用来表明类的不同版本间的兼容性，必须被定义成final static long才能生效，否则会报错</li><li>在使用Externalizable进行序列化的时候，在读取对象时，会调用被序列化类的无参构造器去创建一个新的对象，然后再将被保存对象的字段的值分别填充到新对象中。所以，实现Externalizable接口的类必须要提供一个public的无参的构造器，如果一个Java类没有定义任何构造函数，编译器会帮我们自动添加一个无参的构造方法，可是，如果我们在类中定义了一个有参数的构造方法了，编译器便不会再帮我们创建无参构造方法，这点需要注意</li><li>用户自定义的 writeObject 和 readObject 方法可以允许用户控制序列化的过程</li></ol><h3>serialVersionUID</h3><p>Java的序列化机制是通过在运行时判断类的serialVersionUID来验证版本一致性的。在进行反序列化时，JVM会把传来 的字节流中的serialVersionUID与本地相应实体（类）的serialVersionUID进行比较，如果相同就认为是一致的，可以进行反序 列化，否则就会出现序列化版本不一致的异常。</p><p>当实现java.io.Serializable接口的实体（类）没有显式地定义一个名为serialVersionUID，类型为long的变 量时，Java序列化机制会根据编译的class自动生成一个serialVersionUID作序列化版本比较用，这种情况下，只有同一次编译生成的 class才会生成相同的serialVersionUID 。</p><p>如果我们不希望通过编译来强制划分软件版本，即实现序列化接口的实体能够兼容先前版本，未作更改的类，就需要显式地定义一个名为serialVersionUID，类型为long的变量，不修改这个变量值的序列化实体都可以相互进行串行化和反串行化。</p><h3>在内部类使用中带来的困扰</h3><blockquote><p>A class that is serializable with an enclosing class that is not serializable causes serialization to fail. Non-static nested classes that implement Serializable must be defined in an enclosing class that is also serializable. Non-static nested classes retain an implicit reference to an instance of their enclosing class. If the enclosing class is not serializable, the Java serialization mechanism fails with a java.io.NotSerializableException.</p></blockquote><p>一个非静态的内部类实现了Serializable接口，要求其外部类也同样实现Serializable接口。这是因为一个非静态内部类包含有一个隐式的指向外部包装类实例对象的一个指针，如上面指出的规则，序列化的时候要求类的非静态成员也需要是可序列化的，如果外部类没有声明Serializable，java序列化机制就会报错，解法通常是</p><ul><li>将内部类声明为static，这样就不包含隐式指针了</li><li>将外部类声明为Serializable</li></ul><p>在flink中采用了另一种解法，用户通过匿名内部类来定义一个userFuntion，通常userFunction需要被序列化来分发到各个task节点来执行，定义成static不如匿名类方便，外部主类定义成Serializable的代价又比较大，因此采用另一种解法：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">for (Field f: cls.getDeclaredFields()) &#123;</span><br><span class="line">	if (f.getName().startsWith(&quot;this$&quot;)) &#123;</span><br><span class="line">		// found a closure referencing field - now try to clean</span><br><span class="line">		closureAccessed |= cleanThis0(func, cls, f.getName());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private static boolean cleanThis0(Object func, Class&lt;?&gt; cls, String this0Name) &#123;</span><br><span class="line"></span><br><span class="line">	This0AccessFinder this0Finder = new This0AccessFinder(this0Name);</span><br><span class="line">	getClassReader(cls).accept(this0Finder, 0);</span><br><span class="line"></span><br><span class="line">	final boolean accessesClosure = this0Finder.isThis0Accessed();</span><br><span class="line"></span><br><span class="line">	if (LOG.isDebugEnabled()) &#123;</span><br><span class="line">		LOG.debug(this0Name + &quot; is accessed: &quot; + accessesClosure);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (!accessesClosure) &#123;</span><br><span class="line">		Field this0;</span><br><span class="line">		try &#123;</span><br><span class="line">			this0 = func.getClass().getDeclaredField(this0Name);</span><br><span class="line">		&#125; catch (NoSuchFieldException e) &#123;</span><br><span class="line">			// has no this$0, just return</span><br><span class="line">			throw new RuntimeException(&quot;Could not set &quot; + this0Name + &quot;: &quot; + e);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		try &#123;</span><br><span class="line">			this0.setAccessible(true);</span><br><span class="line">			this0.set(func, null);</span><br><span class="line">		&#125;</span><br><span class="line">		catch (Exception e) &#123;</span><br><span class="line">			// should not happen, since we use setAccessible</span><br><span class="line">			throw new RuntimeException(&quot;Could not set &quot; + this0Name + &quot; to null. &quot; + e.getMessage(), e);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	return accessesClosure;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>对每一个userFunction(可能实现自一个匿名内部类)有一个clean的机制。</p><ul><li>检查其声明字段有没有<code>this$</code>开始的，即指向外部类的引用</li><li>如果有将对应的字段通过反射置成null,这样就不会受第三条规则的困扰了</li></ul><h3>自定义序列化</h3><p>在序列化过程中，如果被序列化的类中定义了writeObject 和 readObject 方法，虚拟机会试图调用对象类里的 writeObject 和 readObject 方法，进行用户自定义的序列化和反序列化。并且这两个方法的signature必须是以下这样才会生效，否则就是默认的序列化方式</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">private void readObject(java.io.ObjectInputStream in)</span><br><span class="line">     throws IOException, ClassNotFoundException;</span><br><span class="line">private void writeObject(java.io.ObjectOutputStream out)</span><br><span class="line">     throws IOException;</span><br></pre></td></tr></table></figure><p></p><p>如果没有这样的方法，则默认调用是 ObjectOutputStream 的 defaultWriteObject 方法以及 ObjectInputStream 的 defaultReadObject 方法。这两个方法没有覆写，也没有被显式调用，为什么会生效呢？ 具体可以参见<a href="https://mp.weixin.qq.com/s/ABtxdNpr4bLpXtFiOK47hA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/ABtxdNpr4bLpXtFiOK47hA</a></p><blockquote><p>在使用ObjectOutputStream的writeObject方法和ObjectInputStream的readObject方法时，会通过反射的方式调用</p></blockquote><p>关于序列化的困惑可以在这个源码中得到解答</p><blockquote><p>ObjectOutputStream#writeObject</p><p>writeObject =&gt; writeObject0 =&gt; writeOrdinaryObject =&gt; writeSerialData =&gt; invokeWriteObject</p></blockquote><p>可以参见ArrayList使用了这种自定义序列化的方法，ArrayList实际上是动态数组，每次在放满以后自动增长设定的长度值，如果数组自动增长长度设为100，而实际只放了一个元素，那就会序列化99个null元素。为了保证在序列化的时候不会将这么多null同时进行序列化，ArrayList把元素数组设置为transient。</p><p><em>Ps：在两个方法的开始处，你会发现调用了defaultWriteObject()和defaultReadObject()。它们做的是默认的序列化进程，就像写/读所有的non-transient和 non-static字段(但他们不会去做serialVersionUID的检查).通常说来，所有我们想要自己处理的字段都应该声明为transient。这样的话，defaultWriteObject/defaultReadObject便可以专注于其余字段，而我们则可为这些特定的字段(译者：指transient)定制序列化。使用那两个默认的方法并不是强制的，而是给予了处理复杂应用时更多的灵活性</em></p><h3>关于反序列化的时候构造方法是否会被调用（反序列化是怎么做的）</h3><blockquote><p>A non-serializable, immediate superclass of a serializable class that does not itself declare an accessible, no-argument constructor causes deserialization to fail To allow subtypes of non-serializable classes to be serialized, the subtype may assume responsibility for saving and restoring the state of the supertype's public, protected, and (if accessible) package fields. The subtype may assume this responsibility only if the class it extends has an accessible no-arg constructor to initialize the class's state. It is an error to declare a class Serializable if this is not the case. The error will be detected at runtime.</p></blockquote><p>反序列化其实是将先前序列化生成的byte流重新构建成一个对象，byte流包含了所有重构对象的信息，包括class的元数据，实例的变量的类型信息，以及相应的值。然后在反序列化的时候它要求<strong>all the parent classes of instance should be Serializable; and if any super class in hirarchy is not Serializable then it must have a default constructor</strong>。在反序列化的时候会一直搜寻其父类，直到找到第一个不可序列化的类，就尝试调用其无参构造函数创建对象，如果所有的父类都是可序列化的，最终找到的就是Object类，然后首先创建一个Object对象。接着JVM就继续读取byte流，设置相关的类型信息，一个空对象创建完成后，jvm就设置相关的static字段，并调用<code>readObject</code>方法进行赋值</p><p>由于本类的构造方法不会被调用，所以你期望某个变量的初始化在构造方法中完成得到的只会是null。</p><p>在构造函数的调用上<code>Externalizable</code>和<code>Serializable</code>的表现不同，Externalizable依赖于本身类的无参构造函数</p><h3>利用序列化来做deepCopy</h3><p>主要用到了<code>ByteArrayOutputStream</code>,存储在内存中，不做持久化</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public SerializableClass deepCopy() throws Exception&#123;</span><br><span class="line">    //Serialization of object</span><br><span class="line">    ByteArrayOutputStream bos = new ByteArrayOutputStream();</span><br><span class="line">    ObjectOutputStream out = new ObjectOutputStream(bos);</span><br><span class="line">    out.writeObject(this);</span><br><span class="line"></span><br><span class="line">    //De-serialization of object</span><br><span class="line">    ByteArrayInputStream bis = new   ByteArrayInputStream(bos.toByteArray());</span><br><span class="line">    ObjectInputStream in = new ObjectInputStream(bis);</span><br><span class="line">    SerializableClass copied = (SerializableClass) in.readObject();</span><br><span class="line">    return copied;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>更多关于java clone 可参考:</p><p><a href="https://howtodoinjava.com/core-java/cloning/a-guide-to-object-cloning-in-java/" target="_blank" rel="noopener">https://howtodoinjava.com/core-java/cloning/a-guide-to-object-cloning-in-java/</a></p><h3>如果对象状态需要同步，则对象序列化也需要同步</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">private synchronized void writeObject(ObjectOutputStream s) throws IOException &#123;</span><br><span class="line">        s.defaultWriteObject();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>单例模式序列化</h3><p>参考：</p><ul><li><a href="http://www.hollischuang.com/archives/1144" target="_blank" rel="noopener">http://www.hollischuang.com/archives/1144</a></li><li>枚举实现可序列化单例 <a href="http://www.cnblogs.com/cielosun/p/6596475.html" target="_blank" rel="noopener">http://www.cnblogs.com/cielosun/p/6596475.html</a></li><li><a href="https://leokongwq.github.io/2017/08/21/why-enum-singleton-are-serialization-safe.html" target="_blank" rel="noopener">https://leokongwq.github.io/2017/08/21/why-enum-singleton-are-serialization-safe.html</a></li></ul><h3>在不同的classloader之间进行对象的序列化和反序列化</h3><p>如上所说，在同一个classloader中，利用如下的方法serializabale和deserializable对象：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ByteArrayOutputStream bo=new ByteArrayOutputStream();</span><br><span class="line">ObjectOutputStream oo=new ObjectOutputStream(bo);</span><br><span class="line">oo.writeObject(outObject);</span><br><span class="line">ByteArrayInputStream bi=new ByteArrayInputStream(bo.toByteArray());</span><br><span class="line">ObjectInputStream oi=new ObjectInputStream(bi);</span><br><span class="line">Object inObject = oi.readObject();</span><br></pre></td></tr></table></figure><p></p><p>当序列化的对象和反序列化的对象不在同一个classloader中时，以上的代码执行时，就会报无法把属性付给对象的错误，此时应当，通过设置反序列化得classloader，来解决这个问题。</p><p>首先，从ObjectInputStream继承一个自己的ObjectInputStream</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">public class CustomObjectInputStream extends ObjectInputStream &#123;</span><br><span class="line"></span><br><span class="line">    protected ClassLoader classLoader = this.getClass().getClassLoader();</span><br><span class="line"></span><br><span class="line">    public CustomObjectInputStream(InputStream in) throws IOException &#123;</span><br><span class="line">        super(in);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public CustomObjectInputStream(InputStream in, ClassLoader cl)</span><br><span class="line">            throws IOException &#123;</span><br><span class="line">        super(in);</span><br><span class="line">        this.classLoader = cl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected Class&lt;?&gt; resolveClass(ObjectStreamClass desc) throws IOException,</span><br><span class="line">            ClassNotFoundException &#123;</span><br><span class="line">        // TODO Auto-generated method stub</span><br><span class="line">        String name = desc.getName();</span><br><span class="line">        try &#123;</span><br><span class="line">            return Class.forName(name, false, this.classLoader);</span><br><span class="line">        &#125; catch (ClassNotFoundException ex) &#123;</span><br><span class="line">            return super.resolveClass(desc);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>比较重要的是这里的resolveClass方法传入classloader,反序列化时将classloader传入</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> ByteArrayOutputStream bo=new ByteArrayOutputStream();</span><br><span class="line">ObjectOutputStream oo=new ObjectOutputStream(bo);</span><br><span class="line">oo.writeObject(outObject);</span><br><span class="line">ByteArrayInputStream bi=new ByteArrayInputStream(bo.toByteArray());</span><br><span class="line">CustomObjectInputStream oi=new CustomObjectInputStream(bi, outObject.getClass().getClassLoader());</span><br><span class="line">Object = oi.readObject();</span><br><span class="line">// flink源码中也有多次类似的使用userClassloader和FlinkClassLoader的切换</span><br><span class="line">https://issues.apache.org/jira/browse/FLINK-9122 这个bug曾经就是因为这个原因引起的</span><br></pre></td></tr></table></figure><p></p><h3>序列化相关方法</h3><p>writeObject、readObject、readObjectNoData、writeReplace和readResolve 待补充</p><h2>参考</h2><ul><li><a href="https://help.semmle.com/wiki/display/JAVA/Serializable+inner+class+of+non-serializable+class," title="Serializable inner class of non-serializable class" target="_blank" rel="noopener">Serializable inner class of non-serializable class</a></li><li><a href="https://mp.weixin.qq.com/s/-D9N9_9IDqSbuIjuADJ7ZA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/-D9N9_9IDqSbuIjuADJ7ZA</a></li><li><a href="https://howtodoinjava.com/core-java/serialization/how-deserialization-process-happen-in-java/" target="_blank" rel="noopener">https://howtodoinjava.com/core-java/serialization/how-deserialization-process-happen-in-java/</a></li><li><a href="https://www.quora.com/Why-are-enum-singleton-serialization-safe" target="_blank" rel="noopener">https://www.quora.com/Why-are-enum-singleton-serialization-safe</a></li></ul></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-jobgraph-generate.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/flink-jobgraph-generate.html" itemprop="url">flink中jobgraph的生成逻辑</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-26T18:43:23+08:00">2018-05-26 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/源码分析/" itemprop="url" rel="index"><span itemprop="name">源码分析</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-jobgraph-generate.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-jobgraph-generate.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">247 </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">1</span></div></div></header><div class="post-body" itemprop="articleBody"><p>flink中jobgraph的生成逻辑，接前面的文章<a href="http://aitozi.com/2018/04/11/flink-streamGraph/" target="_blank" rel="noopener">flink图流转之StreamGraph</a></p><p>&lt;!-- more --&gt;</p><blockquote><p>今天想了一下源码分析类的文章应该是直接在源码上做注释来的直接，然后再抛开细节概括总体流程和关键点，今天这篇分析jobgraph生成的文章就以这个形式展开</p></blockquote><ol><li>先生成各个节点streamnode的hash值 主体代码在：<code>StreamGraphHasherV2.java</code></li><li>设置chaining<ul><li>找到节点中能chain和不能chain的边</li><li>生成相应的JobVertex节点，并设置StreamConfig（资源，名称，chain的节点），这个streamConfig是在部署期间比较重要的一个配置项，并拼接物理执行顺序，主要在connect函数</li></ul></li><li>设置inEdges配置项</li><li>设置slotsharingGroup</li><li>配置checkpoint，这里主要设置需要发送barrier的节点即source节点</li></ol><p><em>其实总结就是分两步：</em></p><ol><li>在createChain过程中创建JobVertex</li><li>设置各个StreamConfig需要的信息用作生成物理执行图的时候使用</li></ol><p>主要涉及的代码为两块，如下：</p><p><script src="//gist.github.com/2e021e923ed394155b191853e0975a71.js?file=StreamGraphHasherV2.java"></script><script src="//gist.github.com/2e021e923ed394155b191853e0975a71.js?file=StreamingJobGraphGenerator.java"></script></p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-cep-code.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/flink-cep-code.html" itemprop="url">flink cep源码分析</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-25T21:54:23+08:00">2018-05-25 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/源码分析/" itemprop="url" rel="index"><span itemprop="name">源码分析</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-cep-code.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-cep-code.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">1.7k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">6</span></div></div></header><div class="post-body" itemprop="articleBody"><p>关于Flink中cep实现原理的分析</p><p>&lt;!-- more --&gt;</p><blockquote><p>最近一直在搞动态cep的事情，有点焦头烂额很久没有更新博客了。其实有时候也在思考写博客的意义，因为写博客也是时间成本很高的一件事，如果得不到相应的收益其实是划不来的。那么写博客的收益到底是什么呢？两点：笔记记录和传播知识的作用，整理记录的功能一个web博客不会强于一个终端笔记例如：为知笔记。所以博客的真正意义在于传播知识观点。有时候你遇到一个百思不得其解的问题的时候，google一下找到一篇博客，竟然能够解答心中所惑的时候你是不是心中会很感谢博主呢，我认为这样的一篇文章就是有价值的文章，所以我希望我也能做好这样一件有价值的事情。</p></blockquote><h2>引言</h2><p>好了，进入本文的主题flink cep原理的深入理解，很多人可能还不知道flink cep是什么，flink cep其实实现自一篇论文，具体论文细节见我之前的一篇文章的分享<a href="http://aitozi.com/2018/02/25/flink-cep-paper/" target="_blank" rel="noopener">flink-cep-paper</a>. flink cep的全称是Complex Event Processing，在我看来它主要能做的是在一个连续不断的事件中提取出用户所关心的事件序列，他和flink的filter算子的区别在于filter只能去实现单个元素的过滤，而cep是能完成先后顺序事件的过滤。下面让我们来走进他的源码实现原理吧。以下代码基于社区1.4.2分支分析。</p><p>我们的文章以一系列问题来展开：</p><ol><li>用户定义的Pattern最后会以什么形式工作</li><li>当CEP Operator获取到上游一个算子的时候会做什么事情？</li><li>在ProcessingTime和Eventtime的语义下处理逻辑有什么不同点？</li><li>匹配成功的元素如何存储，状态机转化流程是怎么样的？</li><li>超时未匹配成功的元素会做什么？</li></ol><h2>问题一</h2><p>用户在定义Pattern和condition之后，会通过NFAcompiler将Pattern翻译成一个一个相关联的State，表明了这一组规则的状态机的走向流程。</p><p>State包括<code>Start、Normal、Final、Stop</code>，start表示一个起始状态例如<code>begin('A').followedBy('B')</code> 这里面A就是一个Start状态。Final状态表示整个序列已经匹配完成可以向下游发送了，Stop状态是用来处理否定类型的规则，一旦到达Stop状态即意味着整个匹配过程失败。各个状态之间通过<code>StateTransition</code>来连接，连接方式有：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ignore: 忽略此次匹配的元素</span><br><span class="line">proceed: 相当于forward的意思，到达下一个状态，不存储元素，继续做下一个状态的condition判断</span><br><span class="line">take： 存储本次的元素</span><br></pre></td></tr></table></figure><p></p><p>这是一段创建中间状态的代码</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">private State&lt;T&gt; createMiddleStates(final State&lt;T&gt; sinkState) &#123;</span><br><span class="line">			State&lt;T&gt; lastSink = sinkState;</span><br><span class="line">			// 不断往上遍历pattern进行state的生成</span><br><span class="line">			while (currentPattern.getPrevious() != null) &#123;</span><br><span class="line"></span><br><span class="line">				if (currentPattern.getQuantifier().getConsumingStrategy() == Quantifier.ConsumingStrategy.NOT_FOLLOW) &#123;</span><br><span class="line">					//skip notFollow patterns, they are converted into edge conditions</span><br><span class="line">				&#125; else if (currentPattern.getQuantifier().getConsumingStrategy() == Quantifier.ConsumingStrategy.NOT_NEXT) &#123;</span><br><span class="line">					final State&lt;T&gt; notNext = createState(currentPattern.getName(), State.StateType.Normal);</span><br><span class="line">					final IterativeCondition&lt;T&gt; notCondition = getTakeCondition(currentPattern);</span><br><span class="line">					// 否定类型的pattern需要创建一个stop state</span><br><span class="line">					final State&lt;T&gt; stopState = createStopState(notCondition, currentPattern.getName());</span><br><span class="line"></span><br><span class="line">					if (lastSink.isFinal()) &#123;</span><br><span class="line">						//so that the proceed to final is not fired</span><br><span class="line">						结尾状态不用proceed过去做下一次计算了，可以直接ignore到Final，然后输出结果</span><br><span class="line">						notNext.addIgnore(lastSink, new NotCondition&lt;&gt;(notCondition));</span><br><span class="line">					&#125; else &#123;</span><br><span class="line">						notNext.addProceed(lastSink, new NotCondition&lt;&gt;(notCondition));</span><br><span class="line">					&#125;</span><br><span class="line">					// 在满足Not_NEXT的条件的时候就转化成stop状态即匹配失败</span><br><span class="line">					notNext.addProceed(stopState, notCondition);</span><br><span class="line">					lastSink = notNext;</span><br><span class="line">				&#125; else &#123;</span><br><span class="line">					// 非否定类型的状态的处理逻辑都在这个方法中</span><br><span class="line">					lastSink = convertPattern(lastSink);</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				// we traverse the pattern graph backwards</span><br><span class="line">				followingPattern = currentPattern;</span><br><span class="line">				currentPattern = currentPattern.getPrevious();</span><br><span class="line"></span><br><span class="line">				final Time currentWindowTime = currentPattern.getWindowTime();</span><br><span class="line">				if (currentWindowTime != null &amp;&amp; currentWindowTime.toMilliseconds() &lt; windowTime) &#123;</span><br><span class="line">					// the window time is the global minimum of all window times of each state</span><br><span class="line">					windowTime = currentWindowTime.toMilliseconds();</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">			return lastSink;</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure><p></p><p>生成这样的state列表之后，最终会创建一个NFA，一个NFA中包含了两个重要组件： 一个是SharedBuffer用于存储中间匹配命中的数据，这是一个基于论文实现的带版本的内存共享，主要解决的事情是在同一个元素触发多个分支的时候避免存储多次。 另一个是ComputationState队列表示的是一系列当前匹配到的计算状态，每一个状态在拿到下一个元素的时候都会根据condition判断自己是能够继续往下匹配生成下一个computation state还是匹配失败。</p><h2>问题二、三</h2><p>问题二和问题三一起解释，在消费到上游一个元素之后会判断时间语义，这里主要是为了处理乱序问题，如果是processingtime的话就会直接经由nfa#process进行处理，因为processing time不需要考虑事件是否乱序，他给每个事件都打上了当前的时间戳。而event语义下，会先将该数据buffer到rocksdb中，并且注册一个比当前时间戳大1的eventimer，用以触发真正的计算，也就是说，eventtime其实是每毫秒获取过去存储的数据做一次匹配计算。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">protected void saveRegisterWatermarkTimer() &#123;</span><br><span class="line">	long currentWatermark = timerService.currentWatermark();</span><br><span class="line">	// protect against overflow</span><br><span class="line">	if (currentWatermark + 1 &gt; currentWatermark) &#123;</span><br><span class="line">		timerService.registerEventTimeTimer(VoidNamespace.INSTANCE, currentWatermark + 1);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h2>问题四、五</h2><p>nfa#process做了什么？取出前面说到的nfa中所有的当前computationState去做计算，当然计算之前会先判断时间和computation的starttime比较匹配是否超出时间，即within算子所做的时间，如果设置了超时处理的方式，就会将超时未匹配完成，已匹配到的部分元素向下游发送，并做sharebuffer的清理工作</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">if (!computationState.isStartState() &amp;&amp;</span><br><span class="line">	windowTime &gt; 0L &amp;&amp;</span><br><span class="line">	timestamp - computationState.getStartTimestamp() &gt;= windowTime) &#123;</span><br><span class="line"></span><br><span class="line">	if (handleTimeout) &#123;</span><br><span class="line">		// extract the timed out event pattern</span><br><span class="line">		Map&lt;String, List&lt;T&gt;&gt; timedOutPattern = extractCurrentMatches(computationState);</span><br><span class="line">		timeoutResult.add(Tuple2.of(timedOutPattern, timestamp));</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	eventSharedBuffer.release(</span><br><span class="line">			NFAStateNameHandler.getOriginalNameFromInternal(computationState.getPreviousState().getName()),</span><br><span class="line">			computationState.getEvent(),</span><br><span class="line">			computationState.getTimestamp(),</span><br><span class="line">			computationState.getCounter());</span><br><span class="line"></span><br><span class="line">	newComputationStates = Collections.emptyList();</span><br><span class="line">	nfaChanged = true;</span><br><span class="line">&#125; else if (event != null) &#123;</span><br><span class="line">   // 在computeNextState的时候判断成功的take条件会将元素put到eventSharedBuffer中</span><br><span class="line">	newComputationStates = computeNextStates(computationState, event, timestamp);</span><br><span class="line"></span><br><span class="line">	if (newComputationStates.size() != 1) &#123;</span><br><span class="line">		nfaChanged = true;</span><br><span class="line">	&#125; else if (!newComputationStates.iterator().next().equals(computationState)) &#123;</span><br><span class="line">		nfaChanged = true;</span><br><span class="line">	&#125;</span><br><span class="line">&#125; else &#123;</span><br><span class="line">	newComputationStates = Collections.singleton(computationState);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>在完成匹配之后达到final状态将数据提取出来向下游发送完成匹配。</p><p>以上便是cep的大致原理，说白了其实这个就是基于flink runntime开发出来的一个衍生lib，flink runtime其实是一个分布式的阻塞队列，通过这个概念可以在上面开发出很多有意思的产品，cep就是其中一个。 分析结束，欢迎拍砖~</p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-streamGraph.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/flink-streamGraph.html" itemprop="url">flink图流转之StreamGraph</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-11T07:13:58+08:00">2018-04-11 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/源码分析/" itemprop="url" rel="index"><span itemprop="name">源码分析</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-streamGraph.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-streamGraph.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">1.5k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">7</span></div></div></header><div class="post-body" itemprop="articleBody"><p>flink DAG图流转分析 &lt;!-- more --&gt;</p><h2>前言</h2><p>每次我们编写完flink作业，跑任务的时候都会在flink-ui上展示一个作业的DAG图，那么这个图是如何形成的呢？本文就和你一起来揭开flink执行图生成的神秘面纱~</p><p>##总览 在flink中的执行图可以分为4层StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图。</p><ul><li>StreamGraph：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。</li><li>JobGraph：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。</li><li>ExecutionGraph：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。</li><li>物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</li></ul><p>今天我们就来看下streamGraph的生成</p><h2>StreamGraph的生成</h2><h3>组件</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">StreamGraph：根据用户通过 Stream API 编写的代码生成的最初的图。</span><br><span class="line">StreamNode：用来代表 operator 的类，并具有所有相关的属性，如并发度、入边和出边等。</span><br><span class="line">StreamEdge：表示连接两个StreamNode的边。</span><br></pre></td></tr></table></figure><p></p><p>flink任务从定义一个运行环境开始<code>streamExecutionEnvironment</code>，流计算任务起始于addSource,我们来看这个函数，addSource之后生成了一个<code>DataStream</code>. <code>DataStream</code>的构造函数参数接收一个<code>StreamTransformation</code>类型的对象，这个对象反映了流之间的转换操作。但是这个transformation和<code>operation</code>不是一一对应的。一些分区操作：union，split/select，partition只是逻辑概念，并不会在最后的dag图上显示出来。 在生成datastream之后，经历<code>DataStream.java</code>中定义的一些api算子，完成业务逻辑的定义，在这之中可能包含以下的转化：</p><p>假设一个场景：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">addsource -&gt; map -&gt; filter -&gt; connect -&gt; flatmap </span><br><span class="line">-&gt; keyby -&gt; window -&gt; apply -&gt; addSink -&gt; excute</span><br></pre></td></tr></table></figure><p></p><ol><li>addSource创建生成一个<code>SingleOutputStreamOperator</code> 本质上是一个带有transformation=&quot;SourceTransformation&quot;的datastream</li><li>map创建生成一个<code>OneInputTransformation</code> 并调用getExecutionEnvironment().addOperator(resultTransform)将其添加入env的<code>List&lt;StreamTransformation&lt;?&gt;&gt;</code>中</li><li>filter 通过 map相同操作</li><li>connect 直接返回一个<code>ConnectedStreams</code>不是<code>Datastream</code>的子类</li><li>flatmap 生成一个<code>TwoInputTransformation</code>将其添加入env的<code>List&lt;StreamTransformation&lt;?&gt;&gt;</code>中，并返回一个<code>SingleOutputStreamOperator</code>,并且返回的Datastream中包含的是当前这个transformation</li><li>keyby 生成一个keyedStream，这里直接生成一个<code>PartitionTransformation</code> 替代了父类<code>DataStream</code>中的transformation</li><li>window 生成windowStream</li><li>apply调用将生成一个<code>OneInputTransformation</code>,增加至<code>List&lt;StreamTransformation&lt;?&gt;&gt;</code></li><li>addSink 获取 <code>SinkTransformation</code></li></ol><p>其中每一次创建<code>OneInputTransformation</code>都是基于Datastream的当前的transformation来创建的，也就是说keyby之后的PartitionTransformation信息也加入了.</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">new OneInputTransformation&lt;&gt;(</span><br><span class="line">			this.transformation,</span><br><span class="line">			operatorName,</span><br><span class="line">			operator,</span><br><span class="line">			outTypeInfo,</span><br><span class="line">			environment.getParallelism());</span><br></pre></td></tr></table></figure><p></p><p>好了到这里已经获取了各个流程的streamtransformation,最后调用execute方法，截取了流式环境下的实现：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public JobExecutionResult execute(String jobName) throws Exception &#123;</span><br><span class="line">	Preconditions.checkNotNull(&quot;Streaming Job name should not be null.&quot;);</span><br><span class="line"></span><br><span class="line">	StreamGraph streamGraph = this.getStreamGraph();</span><br><span class="line">	streamGraph.setJobName(jobName);</span><br><span class="line"></span><br><span class="line">	transformations.clear();</span><br><span class="line"></span><br><span class="line">	// execute the programs</span><br><span class="line">	if (ctx instanceof DetachedEnvironment) &#123;</span><br><span class="line">		LOG.warn(&quot;Job was executed in detached mode, the results will be available on completion.&quot;);</span><br><span class="line">		((DetachedEnvironment) ctx).setDetachedPlan(streamGraph);</span><br><span class="line">		return DetachedEnvironment.DetachedJobExecutionResult.INSTANCE;</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		return ctx</span><br><span class="line">			.getClient()</span><br><span class="line">			.run(streamGraph, ctx.getJars(), ctx.getClasspaths(), ctx.getUserCodeClassLoader(), ctx.getSavepointRestoreSettings())</span><br><span class="line">			.getJobExecutionResult();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>其实主要调用的就是</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StreamGraphGenerator.generate(this, transformations);</span><br></pre></td></tr></table></figure><p></p><p>每一个<code>OneInputTransformation</code>都会记录他的上游的input的transformation，在<code>StreamGraphGenerator.generate</code>主要针对不同的transformation进行不同的转化</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Collection&lt;Integer&gt; <span class="title">transform</span><span class="params">(StreamTransformation&lt;?&gt; transform)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">			<span class="keyword">return</span> alreadyTransformed.get(transform);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		LOG.debug(<span class="string">"Transforming "</span> + transform);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (transform.getMaxParallelism() &lt;= <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// if the max parallelism hasn't been set, then first use the job wide max parallelism</span></span><br><span class="line">			<span class="comment">// from theExecutionConfig.</span></span><br><span class="line">			<span class="keyword">int</span> globalMaxParallelismFromConfig = env.getConfig().getMaxParallelism();</span><br><span class="line">			<span class="keyword">if</span> (globalMaxParallelismFromConfig &gt; <span class="number">0</span>) &#123;</span><br><span class="line">				transform.setMaxParallelism(globalMaxParallelismFromConfig);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// call at least once to trigger exceptions about MissingTypeInfo</span></span><br><span class="line">		transform.getOutputType();</span><br><span class="line"></span><br><span class="line">		Collection&lt;Integer&gt; transformedIds;</span><br><span class="line">		<span class="keyword">if</span> (transform <span class="keyword">instanceof</span> OneInputTransformation&lt;?, ?&gt;) &#123;</span><br><span class="line">			transformedIds = transformOneInputTransform((OneInputTransformation&lt;?, ?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> TwoInputTransformation&lt;?, ?, ?&gt;) &#123;</span><br><span class="line">			transformedIds = transformTwoInputTransform((TwoInputTransformation&lt;?, ?, ?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SourceTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSource((SourceTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SinkTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSink((SinkTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> UnionTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformUnion((UnionTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SplitTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSplit((SplitTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SelectTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSelect((SelectTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> FeedbackTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformFeedback((FeedbackTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> CoFeedbackTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformCoFeedback((CoFeedbackTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> PartitionTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformPartition((PartitionTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SideOutputTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSideOutput((SideOutputTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Unknown transformation: "</span> + transform);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// need this check because the iterate transformation adds itself before</span></span><br><span class="line">		<span class="comment">// transforming the feedback edges</span></span><br><span class="line">		<span class="keyword">if</span> (!alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">			alreadyTransformed.put(transform, transformedIds);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (transform.getBufferTimeout() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">			streamGraph.setBufferTimeout(transform.getId(), transform.getBufferTimeout());</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (transform.getUid() != <span class="keyword">null</span>) &#123;</span><br><span class="line">			streamGraph.setTransformationUID(transform.getId(), transform.getUid());</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (transform.getUserProvidedNodeHash() != <span class="keyword">null</span>) &#123;</span><br><span class="line">			streamGraph.setTransformationUserHash(transform.getId(), transform.getUserProvidedNodeHash());</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (transform.getMinResources() != <span class="keyword">null</span> &amp;&amp; transform.getPreferredResources() != <span class="keyword">null</span>) &#123;</span><br><span class="line">			streamGraph.setResources(transform.getId(), transform.getMinResources(), transform.getPreferredResources());</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> transformedIds;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p>可以看到他里面的方法都是递归调用<code>transform(input)</code>方法，然后通过<code>alreadyTransformed</code>数据结构，避免重复计算，所以我们最终看的时候最先是从source处进行的，也就是从上游到下游进行转化</p><ol><li>如果已经在<code>alreadyTransformed</code>数据结构中那么就直接返回transformation的id</li><li>分别有addSource，addOperator，addSink，addCoOperator，addEdge的不同操作来生成streamGraph中的不同节点</li><li>addEdge建立每一个transformation和他所有上游输入节点的连线</li></ol><p>在streamgraph中还建立了几个虚拟的节点，这几个节点主要针对的是partition，split/select，sideoutput的操作。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">private Map&lt;Integer, Tuple2&lt;Integer, List&lt;String&gt;&gt;&gt; virtualSelectNodes;</span><br><span class="line">private Map&lt;Integer, Tuple2&lt;Integer, OutputTag&gt;&gt; virtualSideOutputNodes;</span><br><span class="line">private Map&lt;Integer, Tuple2&lt;Integer, StreamPartitioner&lt;?&gt;&gt;&gt; virtualPartitionNodes;</span><br></pre></td></tr></table></figure><p></p><p>在进行这些操作时，会添加一个唯一的虚拟节点</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//记录了上游某个transformId到下游的partition方式</span></span><br><span class="line">virtualPartitionNodes.put(virtualId, <span class="keyword">new</span> Tuple2&lt;Integer, StreamPartitioner&lt;?&gt;&gt;(originalId, partitioner));  </span><br><span class="line"><span class="comment">//记录上游的不同outputTag，用以将部分数据从该tag输出</span></span><br><span class="line">virtualSideOutputNodes.put(virtualId, <span class="keyword">new</span> Tuple2&lt;&gt;(originalId, outputTag));</span><br><span class="line"><span class="comment">//记录一个上游的select虚拟节点</span></span><br><span class="line">virtualSelectNodes.put(virtualId, <span class="keyword">new</span> Tuple2&lt;Integer, List&lt;String&gt;&gt;(originalId, selectedNames));</span><br></pre></td></tr></table></figure><p></p><p>经过一些列的<code>addNode</code>以及<code>addEdge</code>之后，streamGraph已经生成。关于其他几个graph的生成请听下回的分解</p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-backpressure-sample.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/flink-backpressure-sample.html" itemprop="url">flink-ui中的反压采样</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-11T07:02:51+08:00">2018-04-11 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/源码分析/" itemprop="url" rel="index"><span itemprop="name">源码分析</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-backpressure-sample.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-backpressure-sample.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">1.1k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">5</span></div></div></header><div class="post-body" itemprop="articleBody"><p>flink反压值采样计算原理</p><p>&lt;!-- more --&gt;</p><p>我们在点击flink ui的<code>operator-&gt;backpressure</code>之后，会触发Backpressure采样：每隔<code>BACK_PRESSURE_REFRESH_INTERVAL</code>的间隔进行一次采样。</p><h3>BackPressureStatsTracker#triggerStackTraceSample</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Triggers a stack trace sample for a operator to gather the back pressure</span></span><br><span class="line"><span class="comment"> * statistics. If there is a sample in progress for the operator, the call</span></span><br><span class="line"><span class="comment"> * is ignored.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> vertex Operator to get the stats for. 代表要采样的Operator节点</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> Flag indicating whether a sample with triggered.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">triggerStackTraceSample</span><span class="params">(ExecutionJobVertex vertex)</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 保证没有并行triggerSimple</span></span><br><span class="line">	<span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">		<span class="keyword">if</span> (shutDown) &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//排除掉已经pending在采样和结束的Operator</span></span><br><span class="line">		<span class="keyword">if</span> (!pendingStats.contains(vertex) &amp;&amp;</span><br><span class="line">				!vertex.getGraph().getState().isGloballyTerminalState()) &#123;</span><br><span class="line">				</span><br><span class="line">			<span class="comment">// 拿到对应ExecutionGraph的FutureExecutor，这是用以在相应的ExecutionGraph发起任务的</span></span><br><span class="line">			Executor executor = vertex.getGraph().getFutureExecutor();</span><br><span class="line"></span><br><span class="line">			<span class="comment">// Only trigger if still active job</span></span><br><span class="line">			<span class="keyword">if</span> (executor != <span class="keyword">null</span>) &#123;</span><br><span class="line">				pendingStats.add(vertex);</span><br><span class="line"></span><br><span class="line">				<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">					LOG.debug(<span class="string">"Triggering stack trace sample for tasks: "</span> + Arrays.toString(vertex.getTaskVertices()));</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="comment">// 核心方法 通过StackTraceSampleCoordinator 去发起相应的trigger流程, 这里的Future是Flink内部自己定义的异步结果接口 具体可查看FlinkFuture.java(可以深入了解下)</span></span><br><span class="line">				Future&lt;StackTraceSample&gt; sample = coordinator.triggerStackTraceSample(</span><br><span class="line">						vertex.getTaskVertices(),</span><br><span class="line">						numSamples,</span><br><span class="line">						delayBetweenSamples,</span><br><span class="line">						MAX_STACK_TRACE_DEPTH);</span><br><span class="line">				<span class="comment">// 指定异步回调函数（采样结果分析的函数）</span></span><br><span class="line">				sample.handleAsync(<span class="keyword">new</span> StackTraceSampleCompletionCallback(vertex), executor);</span><br><span class="line"></span><br><span class="line">				<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>StackTraceSampleCoordinator#triggerStackTraceSample</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">	 * Triggers a stack trace sample to all tasks.</span><br><span class="line">	 *</span><br><span class="line">	 * @param tasksToSample       Tasks to sample.</span><br><span class="line">	 * @param numSamples          Number of stack trace samples to collect.</span><br><span class="line">	 * @param delayBetweenSamples Delay between consecutive samples.</span><br><span class="line">	 * @param maxStackTraceDepth  Maximum depth of the stack trace. 0 indicates</span><br><span class="line">	 *                            no maximum and keeps the complete stack trace.</span><br><span class="line">	 * @return A future of the completed stack trace sample</span><br><span class="line">	 */</span><br><span class="line">	@SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">	public Future&lt;StackTraceSample&gt; triggerStackTraceSample(</span><br><span class="line">			ExecutionVertex[] tasksToSample,</span><br><span class="line">			int numSamples,</span><br><span class="line">			Time delayBetweenSamples,</span><br><span class="line">			int maxStackTraceDepth) &#123;</span><br><span class="line"></span><br><span class="line">		checkNotNull(tasksToSample, &quot;Tasks to sample&quot;);</span><br><span class="line">		checkArgument(tasksToSample.length &gt;= 1, &quot;No tasks to sample&quot;);</span><br><span class="line">		checkArgument(numSamples &gt;= 1, &quot;No number of samples&quot;);</span><br><span class="line">		checkArgument(maxStackTraceDepth &gt;= 0, &quot;Negative maximum stack trace depth&quot;);</span><br><span class="line"></span><br><span class="line">		// 通过ExecutionVertex获取ExecutionAttemptID和Execution，并最后做存活判断</span><br><span class="line">		// Execution IDs of running tasks</span><br><span class="line">		ExecutionAttemptID[] triggerIds = new ExecutionAttemptID[tasksToSample.length];</span><br><span class="line">		Execution[] executions = new Execution[tasksToSample.length];</span><br><span class="line"></span><br><span class="line">		// Check that all tasks are RUNNING before triggering anything. The</span><br><span class="line">		// triggering can still fail.</span><br><span class="line">		for (int i = 0; i &lt; triggerIds.length; i++) &#123;</span><br><span class="line">			Execution execution = tasksToSample[i].getCurrentExecutionAttempt();</span><br><span class="line">			if (execution != null &amp;&amp; execution.getState() == ExecutionState.RUNNING) &#123;</span><br><span class="line">				executions[i] = execution;</span><br><span class="line">				triggerIds[i] = execution.getAttemptId();</span><br><span class="line">			&#125; else &#123;</span><br><span class="line">				return FlinkCompletableFuture.completedExceptionally(</span><br><span class="line">					new IllegalStateException(&quot;Task &quot; + tasksToSample[i]</span><br><span class="line">					.getTaskNameWithSubtaskIndex() + &quot; is not running.&quot;));</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		synchronized (lock) &#123;</span><br><span class="line">			if (isShutDown) &#123;</span><br><span class="line">				return FlinkCompletableFuture.completedExceptionally(new IllegalStateException(&quot;Shut down&quot;));</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			final int sampleId = sampleIdCounter++;</span><br><span class="line"></span><br><span class="line">			LOG.debug(&quot;Triggering stack trace sample &#123;&#125;&quot;, sampleId);</span><br><span class="line">			</span><br><span class="line">			// 包含采样id和ExecutionAttemptID</span><br><span class="line">			final PendingStackTraceSample pending = new PendingStackTraceSample(</span><br><span class="line">					sampleId, triggerIds);</span><br><span class="line"></span><br><span class="line">			// Discard the sample if it takes too long. We don&apos;t send cancel</span><br><span class="line">			// messages to the task managers, but only wait for the responses</span><br><span class="line">			// and then ignore them.</span><br><span class="line">			long expectedDuration = numSamples * delayBetweenSamples.toMilliseconds();</span><br><span class="line">			Time timeout = Time.milliseconds(expectedDuration + sampleTimeout);</span><br><span class="line"></span><br><span class="line">			// Add the pending sample before scheduling the discard task to</span><br><span class="line">			// prevent races with removing it again.</span><br><span class="line">			pendingSamples.put(sampleId, pending);</span><br><span class="line"></span><br><span class="line">			// Trigger all samples</span><br><span class="line">			// execution是executionVertex的多次执行（recovery...）</span><br><span class="line">			for (Execution execution: executions) &#123;</span><br><span class="line">			    // 对相应的execution进行多次numSamples采样，但是都是同一个sampleId</span><br><span class="line">				final Future&lt;StackTraceSampleResponse&gt; stackTraceSampleFuture = execution.requestStackTraceSample(</span><br><span class="line">					sampleId,</span><br><span class="line">					numSamples,</span><br><span class="line">					delayBetweenSamples,</span><br><span class="line">					maxStackTraceDepth,</span><br><span class="line">					timeout);</span><br><span class="line"></span><br><span class="line">				stackTraceSampleFuture.handleAsync(new BiFunction&lt;StackTraceSampleResponse, Throwable, Void&gt;() &#123;</span><br><span class="line">					@Override</span><br><span class="line">					public Void apply(StackTraceSampleResponse stackTraceSampleResponse, Throwable throwable) &#123;</span><br><span class="line">						if (stackTraceSampleResponse != null) &#123;</span><br><span class="line">						    // 收集返回的List&lt;StackTraceElement[]&gt; 到PendingStackTraceSample</span><br><span class="line">							collectStackTraces(</span><br><span class="line">								stackTraceSampleResponse.getSampleId(),</span><br><span class="line">								stackTraceSampleResponse.getExecutionAttemptID(),</span><br><span class="line">								// 返回的堆栈信息包含所有的采样结果</span><br><span class="line">								stackTraceSampleResponse.getSamples());</span><br><span class="line">						&#125; else &#123;</span><br><span class="line">							cancelStackTraceSample(sampleId, throwable);</span><br><span class="line">						&#125;</span><br><span class="line"></span><br><span class="line">						return null;</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;, executor);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			return pending.getStackTraceSampleFuture();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><h3>BackPressureStatsTracker#StackTraceSampleCompletionCallback</h3><p>采样的结果就是<code>StackTraceSample</code></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">* java.lang.Object.wait(Native Method)</span><br><span class="line">* o.a.f.[...].LocalBufferPool.requestBuffer(LocalBufferPool.java:163)</span><br><span class="line">* o.a.f.[...].LocalBufferPool.requestBufferBlocking(LocalBufferPool.java:133) &lt;--- BLOCKING</span><br><span class="line">* request</span><br><span class="line">* [...]</span><br></pre></td></tr></table></figure><p></p><p>利用这样的线程堆栈类型来判断是否block住了。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">		 * Creates the back pressure stats from a stack trace sample.</span><br><span class="line">		 *</span><br><span class="line">		 * @param sample Stack trace sample to base stats on.</span><br><span class="line">		 *</span><br><span class="line">		 * @return Back pressure stats</span><br><span class="line">		 */</span><br><span class="line">		private OperatorBackPressureStats createStatsFromSample(StackTraceSample sample) &#123;</span><br><span class="line">			Map&lt;ExecutionAttemptID, List&lt;StackTraceElement[]&gt;&gt; traces = sample.getStackTraces();</span><br><span class="line"></span><br><span class="line">			// Map task ID to subtask index, because the web interface expects</span><br><span class="line">			// it like that.</span><br><span class="line">			// 方便下面根据executionId查询相应的并发度</span><br><span class="line">			Map&lt;ExecutionAttemptID, Integer&gt; subtaskIndexMap = Maps</span><br><span class="line">					.newHashMapWithExpectedSize(traces.size());</span><br><span class="line"></span><br><span class="line">			Set&lt;ExecutionAttemptID&gt; sampledTasks = sample.getStackTraces().keySet();</span><br><span class="line"></span><br><span class="line">			for (ExecutionVertex task : vertex.getTaskVertices()) &#123;</span><br><span class="line">				ExecutionAttemptID taskId = task.getCurrentExecutionAttempt().getAttemptId();</span><br><span class="line">				if (sampledTasks.contains(taskId)) &#123;</span><br><span class="line">					subtaskIndexMap.put(taskId, task.getParallelSubtaskIndex());</span><br><span class="line">				&#125; else &#123;</span><br><span class="line">					LOG.debug(&quot;Outdated sample. A task, which is part of the &quot; +</span><br><span class="line">							&quot;sample has been reset.&quot;);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			// Ratio of blocked samples to total samples per sub task. Array</span><br><span class="line">			// position corresponds to sub task index.</span><br><span class="line">			// 数组的index和task的并发度相绑定</span><br><span class="line">			double[] backPressureRatio = new double[traces.size()];</span><br><span class="line"></span><br><span class="line">			for (Entry&lt;ExecutionAttemptID, List&lt;StackTraceElement[]&gt;&gt; entry : traces.entrySet()) &#123;</span><br><span class="line">				int backPressureSamples = 0;</span><br><span class="line"></span><br><span class="line">				List&lt;StackTraceElement[]&gt; taskTraces = entry.getValue();</span><br><span class="line"></span><br><span class="line">				for (StackTraceElement[] trace : taskTraces) &#123;</span><br><span class="line">					for (int i = trace.length - 1; i &gt;= 0; i--) &#123;</span><br><span class="line">						StackTraceElement elem = trace[i];</span><br><span class="line"></span><br><span class="line">						if (elem.getClassName().equals(EXPECTED_CLASS_NAME) &amp;&amp;</span><br><span class="line">								elem.getMethodName().equals(EXPECTED_METHOD_NAME)) &#123;</span><br><span class="line"></span><br><span class="line">							backPressureSamples++;</span><br><span class="line">							break; // Continue with next stack trace</span><br><span class="line">						&#125;</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				int subtaskIndex = subtaskIndexMap.get(entry.getKey());</span><br><span class="line"></span><br><span class="line">				int size = taskTraces.size();</span><br><span class="line">				double ratio = (size &gt; 0)</span><br><span class="line">						? ((double) backPressureSamples) / size</span><br><span class="line">						: 0;</span><br><span class="line"></span><br><span class="line">				backPressureRatio[subtaskIndex] = ratio;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			return new OperatorBackPressureStats(</span><br><span class="line">					sample.getSampleId(),</span><br><span class="line">					sample.getEndTime(),</span><br><span class="line">					backPressureRatio);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p>至此完成采样</p><p>待学习</p><ol><li>Flink反压原理</li><li>理解清里面很多的Future使用方法</li></ol></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/advance-maven-shade-plugin.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/advance-maven-shade-plugin.html" itemprop="url">maven-shade-plugin插件高级用法</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-10T22:06:03+08:00">2018-04-10 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/编程工具/" itemprop="url" rel="index"><span itemprop="name">编程工具</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/advance-maven-shade-plugin.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="advance-maven-shade-plugin.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">638 </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">3</span></div></div></header><div class="post-body" itemprop="articleBody"><p>工作中的maven-shade-plugin插件高级用法小记</p><p>&lt;!-- more --&gt;</p><h3>问题背景</h3><p>flink-table 库中使用了org.apache.calcite 1.12版本，我在开发flink-cep的过程中引入了org.apache-calcite 1.6版本，服务端要同时引入这两个版本的包，如何解决这个版本冲突问题呢？</p><h3>maven-shade-plugin</h3><p>使用maven-shade-plugin的relocate功能可以将包名打包成一个别名</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&lt;plugin&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">    &lt;executions&gt;</span><br><span class="line">        &lt;execution&gt;</span><br><span class="line">            &lt;goals&gt;</span><br><span class="line">                &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">            &lt;/goals&gt;</span><br><span class="line">			&lt;configuration&gt;</span><br><span class="line">				&lt;filters&gt;</span><br><span class="line">					&lt;filter&gt;</span><br><span class="line">						&lt;artifact&gt;*:*&lt;/artifact&gt;</span><br><span class="line">						&lt;excludes&gt;</span><br><span class="line">							&lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;</span><br><span class="line">							&lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;</span><br><span class="line">							&lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;</span><br><span class="line">						&lt;/excludes&gt;</span><br><span class="line">					&lt;/filter&gt;</span><br><span class="line">				&lt;/filters&gt;</span><br><span class="line">				&lt;relocations&gt;</span><br><span class="line">				   &lt;!--通过relocation配置将包名进行了替换--&gt;</span><br><span class="line">					&lt;relocation&gt;</span><br><span class="line">						&lt;pattern&gt;org.apache.calcite&lt;/pattern&gt;</span><br><span class="line">						&lt;shadedPattern&gt;org.apache.flink.shaded.cep.calcite&lt;/shadedPattern&gt;</span><br><span class="line">					&lt;/relocation&gt;</span><br><span class="line">				&lt;/relocations&gt;</span><br><span class="line">				&lt;transformers&gt;</span><br><span class="line">					&lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer&quot;/&gt;</span><br><span class="line">				&lt;/transformers&gt;</span><br><span class="line">				&lt;!-- Additional configuration. --&gt;</span><br><span class="line">			&lt;/configuration&gt;</span><br><span class="line">        &lt;/execution&gt;</span><br><span class="line">    &lt;/executions&gt;</span><br><span class="line">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure><p></p><p>看起来一切都ok了，不过思考一个问题，maven-shade-plugin插件修改包名后项目中引用的包名怎么找到的呢？Class.forName(&quot;xxxx.xxx.xxx&quot;)去生成类的方式还奏不奏效呢？Stack Overflow上关于shade插件的描述也提到了对这种类加载的方式是否奏效的疑问</p><p>然而当应用一跑起来之后发现，并不奏效！！！报出异常 <code>No suitable driver found for jdbc:calcite</code> 也就是<code>DriverManager</code>生成相应的driver的时候没找到相应的类。</p><p><strong>相关的issue</strong></p><p><a href="https://github.com/xerial/sqlite-jdbc/issues/145" target="_blank" rel="noopener">https://github.com/xerial/sqlite-jdbc/issues/145</a></p><p>在里面我们看到sqlite数据库有这样的配置文件指定了相应driver的类名</p><p><a href="https://github.com/xerial/sqlite-jdbc/blob/master/src/main/resources/java.sql.Driver" target="_blank" rel="noopener">https://github.com/xerial/sqlite-jdbc/blob/master/src/main/resources/java.sql.Driver</a></p><p>查看calcite也有相应的配置文件指定</p><p><a href="https://github.com/apache/calcite/blob/master/core/src/main/resources/META-INF/services/java.sql.Driver" target="_blank" rel="noopener">https://github.com/apache/calcite/blob/master/core/src/main/resources/META-INF/services/java.sql.Driver</a></p><p>那是不是在shade的过程中将这个文件内容也进行相应的变更就可以了呢? shade也确实有这样的transformation</p><p><a href="http://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#ServicesResourceTransformer" target="_blank" rel="noopener">http://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#ServicesResourceTransformer</a></p><blockquote><p>JAR files providing implementations of some interfaces often ship with a META-INF/services/ directory that maps interfaces to their implementation classes for lookup by the service locator. To relocate the class names of these implementation classes, and to merge multiple implementations of the same interface into one service entry, the ServicesResourceTransformer can be used</p></blockquote><p>最后一个问题: maven-shade-plugin 要在3.0以上才能生效</p><p><a href="https://stackoverflow.com/questions/47476402/maven-shade-plugin-relocation-not-updating-a-entry-in-resource-file" target="_blank" rel="noopener">https://stackoverflow.com/questions/47476402/maven-shade-plugin-relocation-not-updating-a-entry-in-resource-file</a></p><p>效果: <img src="http://or0igopk2.bkt.clouddn.com/18-4-10/84724380.jpg" alt></p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-kafka-exactly-once.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/flink-kafka-exactly-once.html" itemprop="url">上篇·flink 1.4利用kafka0.11实现完整的一致性语义</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-11T22:29:44+08:00">2018-03-11 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/源码分析/" itemprop="url" rel="index"><span itemprop="name">源码分析</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-kafka-exactly-once.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-kafka-exactly-once.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">3k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">15</span></div></div></header><div class="post-body" itemprop="articleBody"><p>flink kafka-connector0.10版本分析，与1.4版本中kafka11对比</p><p>&lt;!-- more --&gt;</p><h1>引言</h1><p>官方文档在出了1.4之后特意发表了一篇blog，通过以下这两个条件实现了真正意义上的exactly once语义</p><ol><li>kafka producer0.11的事务性</li><li><a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol" target="_blank" rel="noopener">two phase commit protocol</a></li></ol><p>我们先看0.10版本的kafka-connector的行为逻辑.</p><h1>Kafka-Connector10</h1><h2>kafkaConsumer10</h2><h3>FlinkKafkaConsumerBase</h3><p>这个抽象类实现了<code>CheckpointedFunction</code>, 这个接口的描述：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">* This is the core <span class="class"><span class="keyword">interface</span> <span class="title">for</span> &lt;<span class="title">i</span>&gt;<span class="title">stateful</span> <span class="title">transformation</span> <span class="title">functions</span>&lt;/<span class="title">i</span>&gt;, <span class="title">meaning</span> <span class="title">functions</span></span></span><br><span class="line"><span class="class">* <span class="title">that</span> <span class="title">maintain</span> <span class="title">state</span> <span class="title">across</span> <span class="title">individual</span> <span class="title">stream</span> <span class="title">records</span>.</span></span><br><span class="line"><span class="class">* <span class="title">While</span> <span class="title">more</span> <span class="title">lightweight</span> <span class="title">interfaces</span> <span class="title">exist</span> <span class="title">as</span> <span class="title">shortcuts</span> <span class="title">for</span> <span class="title">various</span> <span class="title">types</span> <span class="title">of</span> <span class="title">state</span>, <span class="title">this</span> <span class="title">interface</span> <span class="title">offer</span> <span class="title">the</span></span></span><br><span class="line"><span class="class">* <span class="title">greatest</span> <span class="title">flexibility</span> <span class="title">in</span> <span class="title">managing</span> <span class="title">both</span> &lt;<span class="title">i</span>&gt;<span class="title">keyed</span> <span class="title">state</span>&lt;/<span class="title">i</span>&gt; <span class="title">and</span> &lt;<span class="title">i</span>&gt;<span class="title">operator</span> <span class="title">state</span>&lt;/<span class="title">i</span>&gt;.</span></span><br></pre></td></tr></table></figure><p></p><p>这个接口中主要要去做两件事情：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//每一次做checkpoint的时候被调用</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化每一个并发的实例的时候被调用</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br></pre></td></tr></table></figure><p></p><p>初始化函数的调用时机是在<code>open</code>之前的：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//StreamTask.java</span></span><br><span class="line">initializeState();</span><br><span class="line">openAllOperators();</span><br></pre></td></tr></table></figure><p></p><p>在初始化的函数中提供了一个<code>FunctionSnapshotContext</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">void snapshotState(FunctionSnapshotContext context) throws Exception;</span><br></pre></td></tr></table></figure><p></p><p>让你既可以注册一个KeyedStateStore，也可以注册一个OperatorStateStore</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ManagedInitializationContext</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Returns true, if state was restored from the snapshot of a previous execution. This returns always false for</span></span><br><span class="line"><span class="comment">	 * stateless tasks.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">boolean</span> <span class="title">isRestored</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Returns an interface that allows for registering operator state with the backend.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function">OperatorStateStore <span class="title">getOperatorStateStore</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Returns an interface that allows for registering keyed state with the backend.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function">KeyedStateStore <span class="title">getKeyedStateStore</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>我们可以看到kafka10是怎么利用这个<code>CheckpointedFunction</code>来管理记录内部offset的呢？</p><h3>initializeState</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始化过程</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// we might have been restored via restoreState() which restores from legacy operator state</span></span><br><span class="line">		<span class="keyword">if</span> (!restored) &#123;</span><br><span class="line">			restored = context.isRestored();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		OperatorStateStore stateStore = context.getOperatorStateStore();</span><br><span class="line">		offsetsStateForCheckpoint = stateStore.getSerializableListState(DefaultOperatorStateBackend.DEFAULT_OPERATOR_STATE_NAME);</span><br><span class="line">		<span class="comment">//如果是在重启恢复的过程中</span></span><br><span class="line">		<span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">			<span class="keyword">if</span> (restoredState == <span class="keyword">null</span>) &#123;</span><br><span class="line">				restoredState = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">				<span class="keyword">for</span> (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : offsetsStateForCheckpoint.get()) &#123;</span><br><span class="line">				<span class="comment">//partition和相应的offset数</span></span><br><span class="line">					restoredState.put(kafkaOffset.f0, kafkaOffset.f1);</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				LOG.info(<span class="string">"Setting restore state in the FlinkKafkaConsumer."</span>);</span><br><span class="line">				<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">					LOG.debug(<span class="string">"Using the following offsets: &#123;&#125;"</span>, restoredState);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			LOG.info(<span class="string">"No restore state for FlinkKafkaConsumer."</span>);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p>那么我们看到恢复或者初始化的时候将&lt;partition,offset&gt;信息保存到了一组HashMap中，但是这个Map怎么作用于消费阶段呢？</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将从状态中获取到的列表赋予给消费的列表</span></span><br><span class="line">subscribedPartitionsToStartOffsets = restoredState;</span><br></pre></td></tr></table></figure><p></p><h3>snapshot</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (!running) &#123;</span><br><span class="line">		LOG.debug(<span class="string">"snapshotState() called on closed source"</span>);</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line">		offsetsStateForCheckpoint.clear();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">final</span> AbstractFetcher&lt;?, ?&gt; fetcher = <span class="keyword">this</span>.kafkaFetcher;</span><br><span class="line">		<span class="keyword">if</span> (fetcher == <span class="keyword">null</span>) &#123;</span><br><span class="line">			<span class="comment">// fetcher还没有初始化，返回上一次恢复的partition和offset，也就是这里会有个bug。我提了个issue:[FLINK-8869][2]</span></span><br><span class="line">			<span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) &#123;</span><br><span class="line">				offsetsStateForCheckpoint.add(Tuple2.of(subscribedPartition.getKey(), subscribedPartition.getValue()));</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="comment">//</span></span><br><span class="line">			<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">				<span class="comment">// the map cannot be asynchronously updated, because only one checkpoint call can happen</span></span><br><span class="line">				<span class="comment">// on this function at a time: either snapshotState() or notifyCheckpointComplete()</span></span><br><span class="line">				pendingOffsetsToCommit.put(context.getCheckpointId(), restoredState);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			HashMap&lt;KafkaTopicPartition, Long&gt; currentOffsets = fetcher.snapshotCurrentState();</span><br><span class="line"></span><br><span class="line">			<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">				<span class="comment">// the map cannot be asynchronously updated, because only one checkpoint call can happen</span></span><br><span class="line">				<span class="comment">// on this function at a time: either snapshotState() or notifyCheckpointComplete()</span></span><br><span class="line">				pendingOffsetsToCommit.put(context.getCheckpointId(), currentOffsets);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) &#123;</span><br><span class="line">				offsetsStateForCheckpoint.add(</span><br><span class="line">						Tuple2.of(kafkaTopicPartitionLongEntry.getKey(), kafkaTopicPartitionLongEntry.getValue()));</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">			<span class="comment">// truncate the map of pending offsets to commit, to prevent infinite growth</span></span><br><span class="line">			<span class="keyword">while</span> (pendingOffsetsToCommit.size() &gt; MAX_NUM_PENDING_CHECKPOINTS) &#123;</span><br><span class="line">				pendingOffsetsToCommit.remove(<span class="number">0</span>);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>offsetCommitMode</h3><p>如上述代码中的<code>offsetCommitMode</code>,主要有以下几种</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DISABLED,</span><br><span class="line">ON_CHECKPOINTS, <span class="comment">// 完成一次checkpoint后向kafka提交消费offset，只有这种模式下才需要我们手动去提交offset到kafka</span></span><br><span class="line">KAFKA_PERIODIC;</span><br></pre></td></tr></table></figure><p></p><p>这个配置主要改变了commitOffset回kafka的时机. 首先在snapshot的时候会将对应的checkpointId和相应的offset的列表放入<code>pendingOffsetsToCommit</code>, 在checkpoint完成后回调<code>notifyCheckpointComplete</code>，这里面主要完成了offset的commit工作。</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">			<span class="comment">// only one commit operation must be in progress</span></span><br><span class="line">			<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">				LOG.debug(<span class="string">"Committing offsets to Kafka/ZooKeeper for checkpoint "</span> + checkpointId);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">try</span> &#123;</span><br><span class="line">				<span class="keyword">final</span> <span class="keyword">int</span> posInMap = pendingOffsetsToCommit.indexOf(checkpointId);</span><br><span class="line">				<span class="keyword">if</span> (posInMap == -<span class="number">1</span>) &#123;</span><br><span class="line">					LOG.warn(<span class="string">"Received confirmation for unknown checkpoint id &#123;&#125;"</span>, checkpointId);</span><br><span class="line">					<span class="keyword">return</span>;</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				<span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</span><br><span class="line">				HashMap&lt;KafkaTopicPartition, Long&gt; offsets =</span><br><span class="line">					(HashMap&lt;KafkaTopicPartition, Long&gt;) pendingOffsetsToCommit.remove(posInMap);</span><br><span class="line"></span><br><span class="line">				<span class="comment">// remove older checkpoints in map</span></span><br><span class="line">				<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; posInMap; i++) &#123;</span><br><span class="line">					pendingOffsetsToCommit.remove(<span class="number">0</span>);</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				<span class="keyword">if</span> (offsets == <span class="keyword">null</span> || offsets.size() == <span class="number">0</span>) &#123;</span><br><span class="line">					LOG.debug(<span class="string">"Checkpoint state was empty."</span>);</span><br><span class="line">					<span class="keyword">return</span>;</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				fetcher.commitInternalOffsetsToKafka(offsets, offsetCommitCallback);</span><br><span class="line">			&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">				<span class="keyword">if</span> (running) &#123;</span><br><span class="line">					<span class="keyword">throw</span> e;</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="comment">// else ignore exception if we are no longer running</span></span><br><span class="line">			&#125;</span><br></pre></td></tr></table></figure><p></p><h3>消费partition分配问题</h3><ul><li><p>在从上次点恢复的情况下是直接从state中获取应该读取哪一个partition，offset。如果并发度改变了会做出什么样的反馈呢?会正确做出rescale吗</p></li><li><p>第一次进行读取的时候会初始化处当前task（并发度）所需要订阅的partition</p></li></ul><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> Map&lt;KafkaTopicPartition, Long&gt; <span class="title">initializeSubscribedPartitionsToStartOffsets</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">			List&lt;KafkaTopicPartition&gt; kafkaTopicPartitions, //topic的所有partition</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="keyword">int</span> indexOfThisSubtask, // 当前task的维度</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="keyword">int</span> numParallelSubtasks, // 总的并发度</span></span></span><br><span class="line"><span class="function"><span class="params">			StartupMode startupMode, // 从哪个offset消费的模式（最新，最老，指定offset）</span></span></span><br><span class="line"><span class="function"><span class="params">			Map&lt;KafkaTopicPartition, Long&gt; specificStartupOffsets)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		Map&lt;KafkaTopicPartition, Long&gt; subscribedPartitionsToStartOffsets = <span class="keyword">new</span> HashMap&lt;&gt;(kafkaTopicPartitions.size());</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (KafkaTopicPartition kafkaTopicPartition : kafkaTopicPartitions) &#123;</span><br><span class="line">			<span class="comment">// only handle partitions that this subtask should subscribe to（选取当前subtask所需要订阅的partition）</span></span><br><span class="line">			<span class="keyword">if</span> (KafkaTopicPartitionAssigner.assign(kafkaTopicPartition, numParallelSubtasks) == indexOfThisSubtask) &#123;</span><br><span class="line">				<span class="keyword">if</span> (startupMode != StartupMode.SPECIFIC_OFFSETS) &#123;</span><br><span class="line">					<span class="comment">//StateSentinel都是一串随机的负数占位符(都是一个标记在KafkaConsumerThread中进行判断)</span></span><br><span class="line">					subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, startupMode.getStateSentinel());</span><br><span class="line">				&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">					<span class="keyword">if</span> (specificStartupOffsets == <span class="keyword">null</span>) &#123;</span><br><span class="line">						<span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(</span><br><span class="line">							<span class="string">"Startup mode for the consumer set to "</span> + StartupMode.SPECIFIC_OFFSETS +</span><br><span class="line">								<span class="string">", but no specific offsets were specified"</span>);</span><br><span class="line">					&#125;</span><br><span class="line"></span><br><span class="line">					Long specificOffset = specificStartupOffsets.get(kafkaTopicPartition);</span><br><span class="line">					<span class="keyword">if</span> (specificOffset != <span class="keyword">null</span>) &#123;</span><br><span class="line">						<span class="comment">// since the specified offsets represent the next record to read, we subtract</span></span><br><span class="line">						<span class="comment">// it by one so that the initial state of the consumer will be correct</span></span><br><span class="line">						<span class="comment">// 这里需要减去1</span></span><br><span class="line">						subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, specificOffset - <span class="number">1</span>);</span><br><span class="line">					&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">						subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET);</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> subscribedPartitionsToStartOffsets;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assign</span><span class="params">(KafkaTopicPartition partition, <span class="keyword">int</span> numParallelSubtasks)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> startIndex = ((partition.getTopic().hashCode() * <span class="number">31</span>) &amp; <span class="number">0x7FFFFFFF</span>) % numParallelSubtasks;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// here, the assumption is that the id of Kafka partitions are always ascending</span></span><br><span class="line">	<span class="comment">// starting from 0, and therefore can be used directly as the offset clockwise from the start index</span></span><br><span class="line">	<span class="comment">// 这里看出：每个Partition只会分配到一个subtask来消费</span></span><br><span class="line">	<span class="comment">// 1. partition &gt; parallel 一个subtask会订阅多个partition</span></span><br><span class="line">	<span class="comment">// 2. partition &lt; parallel 有subtask会是空闲的</span></span><br><span class="line">	<span class="comment">// 3. startIndex由topic名字计算得出</span></span><br><span class="line">	<span class="keyword">return</span> (startIndex + partition.getPartition()) % numParallelSubtasks;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>消费模型kafka10</h3><p>在完成partition订阅之后，就要开始真正的run方法了，<code>FlinkKafkaConsumer</code>也是实现自<code>SouceFunction</code>，因此主要的逻辑也都是在run方法中实现。 主要逻辑：</p><p>kafkaConsumerThread和Kafka10Fetcher通过<code>Handover</code>交互,我觉得这段代码写的很不错，可以好好学习下。可以形象的比作在接力跑：<code>kafkaConsumerThread</code>通过真正的消费线程消费放入一个<code>HandOver</code>，再由kafkaFetcher去poll，完成整个消费过程。</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// we need only do work, if we actually have partitions assigned</span></span><br><span class="line"><span class="keyword">if</span> (!subscribedPartitionsToStartOffsets.isEmpty()) &#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// create the fetcher that will communicate with the Kafka brokers</span></span><br><span class="line">	<span class="keyword">final</span> AbstractFetcher&lt;T, ?&gt; fetcher = createFetcher(</span><br><span class="line">			sourceContext,</span><br><span class="line">			subscribedPartitionsToStartOffsets,</span><br><span class="line">			periodicWatermarkAssigner,</span><br><span class="line">			punctuatedWatermarkAssigner,</span><br><span class="line">			(StreamingRuntimeContext) getRuntimeContext(),</span><br><span class="line">			offsetCommitMode);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// publish the reference, for snapshot-, commit-, and cancel calls</span></span><br><span class="line">	<span class="comment">// IMPORTANT: We can only do that now, because only now will calls to</span></span><br><span class="line">	<span class="comment">//            the fetchers 'snapshotCurrentState()' method return at least</span></span><br><span class="line">	<span class="comment">//            the restored offsets</span></span><br><span class="line">	<span class="keyword">this</span>.kafkaFetcher = fetcher;</span><br><span class="line">	<span class="keyword">if</span> (!running) &#123;</span><br><span class="line">		<span class="keyword">return</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// (3) run the fetcher' main work method</span></span><br><span class="line">	<span class="comment">// 主要工作方法</span></span><br><span class="line">	fetcher.runFetchLoop();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">	<span class="comment">// this source never completes, so emit a Long.MAX_VALUE watermark</span></span><br><span class="line">	<span class="comment">// to not block watermark forwarding</span></span><br><span class="line">	<span class="comment">// 发送最大的watermark就不会block住下游的watermark更新</span></span><br><span class="line">	sourceContext.emitWatermark(<span class="keyword">new</span> Watermark(Long.MAX_VALUE));</span><br><span class="line"></span><br><span class="line">	<span class="comment">// wait until this is canceled</span></span><br><span class="line">	<span class="keyword">final</span> Object waitLock = <span class="keyword">new</span> Object();</span><br><span class="line">	<span class="keyword">while</span> (running) &#123;</span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			<span class="comment">//noinspection SynchronizationOnLocalVariableOrMethodParameter</span></span><br><span class="line">			<span class="keyword">synchronized</span> (waitLock) &#123;</span><br><span class="line">				waitLock.wait();</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">			<span class="keyword">if</span> (!running) &#123;</span><br><span class="line">				<span class="comment">// restore the interrupted state, and fall through the loop</span></span><br><span class="line">				<span class="comment">// 打断当前线程</span></span><br><span class="line">				Thread.currentThread().interrupt();</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Handover的描述， Handover代码可以再好好学习下。</span><br><span class="line">* The Handover is a utility to hand over data (a buffer of records) and exception from a</span><br><span class="line">* &lt;i&gt;producer&lt;/i&gt; thread to a &lt;i&gt;consumer&lt;/i&gt; thread. It effectively behaves like a</span><br><span class="line">* &quot;size one blocking queue&quot;, with some extras around exception reporting, closing, and</span><br><span class="line">* waking up thread without &#123;@link Thread#interrupt() interrupting&#125; threads.</span><br></pre></td></tr></table></figure><p></p><h3>KafkaFetcher</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">runFetchLoop</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		<span class="keyword">final</span> Handover handover = <span class="keyword">this</span>.handover;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// kick off the actual Kafka consumer</span></span><br><span class="line">		<span class="comment">// 启动真正的消费线程</span></span><br><span class="line">		consumerThread.start();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">while</span> (running) &#123;</span><br><span class="line">			<span class="comment">// this blocks until we get the next records</span></span><br><span class="line">			<span class="comment">// it automatically re-throws exceptions encountered in the fetcher thread</span></span><br><span class="line">			<span class="comment">// 在handover中获取真正的数据，并抛出其他线程中的异常</span></span><br><span class="line">			<span class="keyword">final</span> ConsumerRecords&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; records = handover.pollNext();</span><br><span class="line"></span><br><span class="line">			<span class="comment">// get the records for each topic partition</span></span><br><span class="line">			<span class="comment">// subscribedPartitionStates维护的是每个partition的状态（partition，KPH（partition的描述，依据版本可能不同），offset, committedOffset, watermark）</span></span><br><span class="line">			<span class="keyword">for</span> (KafkaTopicPartitionState&lt;TopicPartition&gt; partition : subscribedPartitionStates()) &#123;</span><br><span class="line"></span><br><span class="line">				List&lt;ConsumerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt; partitionRecords =</span><br><span class="line">						records.records(partition.getKafkaPartitionHandle());</span><br><span class="line"></span><br><span class="line">				<span class="keyword">for</span> (ConsumerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; record : partitionRecords) &#123;</span><br><span class="line">					<span class="keyword">final</span> T value = deserializer.deserialize(</span><br><span class="line">							record.key(), record.value(),</span><br><span class="line">							record.topic(), record.partition(), record.offset());</span><br><span class="line"></span><br><span class="line">					<span class="keyword">if</span> (deserializer.isEndOfStream(value)) &#123;</span><br><span class="line">						<span class="comment">// end of stream signaled</span></span><br><span class="line">						running = <span class="keyword">false</span>;</span><br><span class="line">						<span class="keyword">break</span>;</span><br><span class="line">					&#125;</span><br><span class="line"></span><br><span class="line">					<span class="comment">// emit the actual record. this also updates offset state atomically</span></span><br><span class="line">					<span class="comment">// and deals with timestamps and watermark generation</span></span><br><span class="line">					<span class="comment">// 这里会进入真正的通过sourceContext发送数据的代码 如下</span></span><br><span class="line">					emitRecord(value, partition, record.offset(), record);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">finally</span> &#123;</span><br><span class="line">		<span class="comment">// this signals the consumer thread that no more work is to be done</span></span><br><span class="line">		consumerThread.shutdown();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// on a clean exit, wait for the runner thread</span></span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		consumerThread.join();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">		<span class="comment">// may be the result of a wake-up interruption after an exception.</span></span><br><span class="line">		<span class="comment">// we ignore this here and only restore the interruption state</span></span><br><span class="line">		Thread.currentThread().interrupt();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">emitRecordWithTimestamp</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		T record, KafkaTopicPartitionState&lt;KPH&gt; partitionState, <span class="keyword">long</span> offset, <span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (record != <span class="keyword">null</span>) &#123;</span><br><span class="line">		<span class="keyword">if</span> (timestampWatermarkMode == NO_TIMESTAMPS_WATERMARKS) &#123;</span><br><span class="line">			<span class="comment">// fast path logic, in case there are no watermarks generated in the fetcher</span></span><br><span class="line"></span><br><span class="line">			<span class="comment">// emit the record, using the checkpoint lock to guarantee</span></span><br><span class="line">			<span class="comment">// atomicity of record emission and offset state update</span></span><br><span class="line">			<span class="keyword">synchronized</span> (checkpointLock) &#123;</span><br><span class="line">				sourceContext.collectWithTimestamp(record, timestamp);</span><br><span class="line">				partitionState.setOffset(offset);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (timestampWatermarkMode == PERIODIC_WATERMARKS) &#123;</span><br><span class="line">		    <span class="comment">// 更新partitionstate中的watermark状态</span></span><br><span class="line">			emitRecordWithTimestampAndPeriodicWatermark(record, partitionState, offset, timestamp);</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			emitRecordWithTimestampAndPunctuatedWatermark(record, partitionState, offset, timestamp);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		<span class="comment">// if the record is null, simply just update the offset state for partition</span></span><br><span class="line">		<span class="keyword">synchronized</span> (checkpointLock) &#123;</span><br><span class="line">			partitionState.setOffset(offset);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>在设置了kafkaTimestampassigner之后就会进行一个定时任务向下游发送watermark，值为所有partition维护的最小值：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">long</span> minAcrossAll = Long.MAX_VALUE;</span><br><span class="line">	<span class="keyword">for</span> (KafkaTopicPartitionStateWithPeriodicWatermarks&lt;?, ?&gt; state : allPartitions) &#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// we access the current watermark for the periodic assigners under the state</span></span><br><span class="line">		<span class="comment">// lock, to prevent concurrent modification to any internal variables</span></span><br><span class="line">		<span class="keyword">final</span> <span class="keyword">long</span> curr;</span><br><span class="line">		<span class="comment">//noinspection SynchronizationOnLocalVariableOrMethodParameter</span></span><br><span class="line">		<span class="comment">// 这个锁是防止别的线程修改其他变量</span></span><br><span class="line">		<span class="keyword">synchronized</span> (state) &#123;</span><br><span class="line">			curr = state.getCurrentWatermarkTimestamp();</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		minAcrossAll = Math.min(minAcrossAll, curr);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// emit next watermark, if there is one</span></span><br><span class="line">	<span class="keyword">if</span> (minAcrossAll &gt; lastWatermarkTimestamp) &#123;</span><br><span class="line">		lastWatermarkTimestamp = minAcrossAll;</span><br><span class="line">		emitter.emitWatermark(<span class="keyword">new</span> Watermark(minAcrossAll));</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// schedule the next watermark</span></span><br><span class="line">	timerService.registerTimer(timerService.getCurrentProcessingTime() + interval, <span class="keyword">this</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>KafkaConsumerThread</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (records == <span class="keyword">null</span>) &#123;</span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					records = consumer.poll(pollTimeout);</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">catch</span> (WakeupException we) &#123;</span><br><span class="line">					<span class="keyword">continue</span>;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">try</span> &#123;</span><br><span class="line">				handover.produce(records);</span><br><span class="line">				records = <span class="keyword">null</span>;</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">catch</span> (Handover.WakeupException e) &#123;</span><br><span class="line">				<span class="comment">// fall through the loop</span></span><br><span class="line">			&#125;</span><br></pre></td></tr></table></figure><p></p><p>主要做的工作就是从consumer消费数据塞入handover，等待拉取</p><h3>Handover</h3><h3>桥接模式</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumerCallBridge</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">assignPartitions</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, List&lt;TopicPartition&gt; topicPartitions)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		consumer.assign(topicPartitions);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekPartitionToBeginning</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, TopicPartition partition)</span> </span>&#123;</span><br><span class="line">		consumer.seekToBeginning(partition);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekPartitionToEnd</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, TopicPartition partition)</span> </span>&#123;</span><br><span class="line">		consumer.seekToEnd(partition);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>解决08 09 10 版本的api不兼容问题</p><h2>Kafkaproducer10</h2><p>###initializeState</p><p>什么都不做</p><h3>snapshot</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="comment">// check for asynchronous errors and fail the checkpoint if necessary</span></span><br><span class="line">	checkErroneous();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (flushOnCheckpoint) &#123;</span><br><span class="line">		<span class="comment">// flushing is activated: We need to wait until pendingRecords is 0</span></span><br><span class="line">		flush();</span><br><span class="line">		<span class="keyword">synchronized</span> (pendingRecordsLock) &#123;</span><br><span class="line">			<span class="keyword">if</span> (pendingRecords != <span class="number">0</span>) &#123;</span><br><span class="line">				<span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Pending record count must be zero at this point: "</span> + pendingRecords);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// if the flushed requests has errors, we should propagate it also and fail the checkpoint</span></span><br><span class="line">			checkErroneous();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>这里涉及到一个flushOnCheckPoint的问题，再调用<code>producer.flush</code>期间，producer会将所有没写入的，在buffer中的数据刷盘，然后调用commitCallBack，这就保证了ckpt之后数据不会丢的问题。</p><p>主要工作方法：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(IN next)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="comment">// propagate asynchronous errors</span></span><br><span class="line">	checkErroneous();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">byte</span>[] serializedKey = schema.serializeKey(next);</span><br><span class="line">	<span class="keyword">byte</span>[] serializedValue = schema.serializeValue(next);</span><br><span class="line">	<span class="comment">// 每条元素可以自己自己要写到的topic</span></span><br><span class="line">	String targetTopic = schema.getTargetTopic(next);</span><br><span class="line">	<span class="keyword">if</span> (targetTopic == <span class="keyword">null</span>) &#123;</span><br><span class="line">		targetTopic = defaultTopicId;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span>[] partitions = <span class="keyword">this</span>.topicPartitionsMap.get(targetTopic);</span><br><span class="line">	<span class="keyword">if</span>(<span class="keyword">null</span> == partitions) &#123;</span><br><span class="line">		partitions = getPartitionsByTopic(targetTopic, producer);</span><br><span class="line">		<span class="keyword">this</span>.topicPartitionsMap.put(targetTopic, partitions);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	ProducerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; record;</span><br><span class="line">	<span class="keyword">if</span> (flinkKafkaPartitioner == <span class="keyword">null</span>) &#123;</span><br><span class="line">		record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(targetTopic, serializedKey, serializedValue);</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(</span><br><span class="line">				targetTopic,</span><br><span class="line">				flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions),</span><br><span class="line">				serializedKey,</span><br><span class="line">				serializedValue);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> (flushOnCheckpoint) &#123;</span><br><span class="line">		<span class="keyword">synchronized</span> (pendingRecordsLock) &#123;</span><br><span class="line">			pendingRecords++;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	producer.send(record, callback);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h1>问题</h1><ol><li><p>kafka恢复状态直接从状态中去获取了之前保存的partition和offset，但是如果是扩容partition的场景就不会从新的Partition消费 issue:<a href="https://issues.apache.org/jira/projects/FLINK/issues/FLINK-8869?filter=reportedbyme" target="_blank" rel="noopener">FLINK-8869</a></p></li><li><p>flink内部维护了offset，为什么向kafka提交的时候还需要在checkpoint之后再提交而不是定时提交就算了？</p><p>因为虽然从checkpoint点恢复的时候不需要从kafka broker获取消费点的位置了，但是如果是应用重启消费上次消费到的点的数据，这个offset就是flink向kafka提交的，放在checkpoint完成后去做的好处就是让应用即使不是从上个点恢复的，也能够从kafka消费正确的offset点。</p></li><li><p>如果在新的checkpoint没打之前任务失败了，重新从上次的offset点消费的话下游数据是不是重复了?</p><p><strong>是的，因为有一部分数据经过处理已经sink出去了，因此才需要0.11的一致性语义</strong></p></li></ol><p><em>以上代码： kafka-connector0.10 来源于release1.3.2 kafka-connector0.11 来源于release1.4.0</em></p></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://www.aitozi.com/flink-cep-paper.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="aitozi"><meta itemprop="description" content><meta itemprop="image" content="/images/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Aitozi"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"><a class="post-title-link" href="/flink-cep-paper.html" itemprop="url">flink-cep-paper</a></h2><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-25T20:19:15+08:00">2018-02-25 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/论文/" itemprop="url" rel="index"><span itemprop="name">论文</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a href="/flink-cep-paper.html#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="flink-cep-paper.html" itemprop="commentCount"></span> </a><span class="post-meta-divider">|</span></span><div class="post-wordcount"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i> </span><span class="post-meta-item-text">字数统计</span> <span title="字数统计">1.4k </span><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i> </span><span class="post-meta-item-text">阅读时长</span> <span title="阅读时长">6</span></div></div></header><div class="post-body" itemprop="articleBody"><p>CEP实现论文</p><p>&lt;!-- more --&gt;</p><h3>引言</h3><p>2016年1月29号，社区提了这个patch<a href="https://github.com/apache/flink/pull/1557" target="_blank" rel="noopener">[FLINK-3216]</a>引入flink cep模块，flink cep实现基于论文：<a href="https://people.cs.umass.edu/%7Eyanlei/publications/sase-sigmod08.pdf" target="_blank" rel="noopener">Efficient Pattern Matching over Event Streams</a>。我们来深入阅读以下这篇论文。</p><h3>event pattern查询表达形式</h3><p>先看三个查询 <img src="http://or0igopk2.bkt.clouddn.com/18-2-25/25532292.jpg" alt></p><p><em>Query1</em></p><p>匹配出的是：挑选从货架取出的商品，未经checkout，带离了商店的行为序列</p><ol><li><code>SEQ(Shelf a, ∼(Register b), Exit c)</code></li><li>where条件指定了a，b，c的tagid相同</li><li>within指定窗口大小12小时</li></ol><p><em>Query2</em></p><p>匹配出的是：一个污染报警和相关受污染的发货卸货流转站点序列</p><ol><li>报警类型是污染</li><li>匹配出的装载点和前一个到达点一致</li><li>within指定匹配窗口大小是3小时</li></ol><p><em>Query3</em></p><p>匹配出的是：匹配股票交易拐点</p><ol><li>初始成交量高于1000</li><li>持续增加，最后成交量突然跌为最高点的80%以下的序列</li><li>1小时的时间窗口</li></ol><p>可以看出规则是怎么描述的：</p><ol><li>指定匹配出的序列是什么（structure）</li><li>限制或筛选条件</li></ol><p>还需要匹配策略来进行规制匹配。</p><ol><li>严格的邻近匹配（Strict contiguity）：只有连续两个event能满足匹配，中间不能夹杂其他不符合条件的元素</li><li>分区邻近匹配（Partition contiguity）：这就是说被选出的两个event之间不一定需要完全紧密相连，但是在一个分区中的需要紧密相连 如<em>Query3</em>中的a[]需要连续，a与b不需要连续</li><li>skip_till_next_match: 所有的不相关的元素都会被跳过，不再考虑是不是邻近的元素</li><li>skip_till_any_match: <em>Query2</em>解释了这种用法。比如当前最后识别的shipment到达了X位置。这时候读进来一条，X-&gt;Y的Event，在<code>skip_till_any_match</code>匹配规则下系统会做两件事：1.接收X-&gt;Y事件，更新状态2.在当前状态的另一个实例中不接收他来保留一个状态。这样当下一个比如来了个X-&gt;Z事件那还是可以接受这个实例。这种策略就是返回所有的可能的序列（allowing non-deterministic actions）在接收到同一个事件后可以有不一样的走向</li></ol><hr><p>以上介绍了event pattern的查询形式，接下来我们看其内部实现算法。</p><h3>NFA 非确定性自动状态机</h3><p>非确定性自动状态机由以下结构组成。A = (Q,E,θ,q1,F),</p><pre><code>Q: 一系列状态
E： 边
θ： 计算公式
q1：起始状态
F: 结束状态
</code></pre><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/18248041.jpg" alt></p><p><strong>States</strong></p><blockquote><p>the start state,a[1], is where the matching process begins. It awaits input to start the Kleene plus and to select an event into the a[1] unit of the match buffer. At the next state a[i], it attempts to select another event into the a[i] (i &gt; 1) unit of the buffer. The subsequent state b denotes that the matching process has fulfilled the Kleene plus (for a particular match) and is ready to process the next pattern component. The final state, F, represents the completion of the process, resulting in the creation of a pattern match.</p></blockquote><p><strong>Edges</strong></p><blockquote><p>Each state is associated with a number of edges, representing the actions that can be taken at the state. As Figure 2(a) shows, each state that is a singleton state or the first state,p[1], of a pair has a forward begin edge. Each second state, p[i], of a pair has a forward proceed edge, and a looping take edge. Every state (except the start and final states) has a looping ignore edge. The start state has no edges to it as we are only interested in matches that start with selected events. Each edge at a state, q, is precisely described by a triplet:(1) a formula that specifies the condition on taking it, denoted by θq-edge, (2) an operation on the input stream (i.e.,consume an event or not), and (3) an operation on the match buffer (i.e., write to the buffer or not). Formulas of edges are compiled from pattern queries, which we explain in detail shortly. As shown in Figure 2(a), we use solid lines to denote begin and take edges that consume an event from the input and write it to the buffer, and dashed lines for ignore edges that consume an event but do not write it to the buffer. The proceed edge is a special edge: it does not consume any input event but only evaluates its formula and tries proceeding. We distinguish the proceed edge from ignore edges in the style of arrow, denoting its behavior.</p></blockquote><p><em>非确定性</em> 某些state具有两条边，但是这两条边的行为不一定是完全相反的</p><p>NFA算法过程：</p><ol><li>先根据整个structure构建state和边的行为</li><li>将condition翻译成边上的公式</li><li>将select strategy翻译成边上的公式</li><li>将时间窗口翻译成：最先匹配的元素和最后的元素时长不能超过窗口大小</li></ol><p>优化：</p><ol><li>先做过滤</li><li>将window条件前置</li><li>优化proceed边，进入下一个state的时候要检查是否满足begin的条件判断</li></ol><h3>带版本的内存共享</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/20508653.jpg" alt></p><p>通过带版本的共享内存解决多runs之间的重复事件问题。</p><h3>Computation state</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/48608682.jpg" alt></p><ol><li>version number of a run</li><li>current automaton state the run is in</li><li>a pointer to the most revent event selected into the buffer</li><li>a vector =&gt; 保留做edge判断的所需的最少的元素。例如上图中存储了a[i]的和和count为了计算平均值</li></ol><h3>Merge相同的Runs</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/87062132.jpg" alt></p><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/41465208.jpg" alt></p><p>将Query3中的avg算法改为max算法，他们在接收到e4之后达到了同样的状态，因此其之后的运算可以merge为一个。</p><ol><li>首先探测什么时候两个runs一样了，引入了算子M。每一个状态机都有一个mask，每一个mask对每一个Vector中列都有一个bit位，都相同时那么这两个run就是相等的。</li><li>创建combined的run<ol><li>时间设置为最小的那个时间</li></ol></li></ol><h3>Backtrack算法（回溯）</h3><p>每次只跑一个run，跑失败了再回退到分叉点跑另一条。也就是以广度优先搜索方式 <code>breadth first search manner</code></p><h3>待补充</h3><ol><li>性能评估方案</li><li>有哪些因素会影响计算速度和吞吐</li><li>内存优化管理措施</li></ol></div><footer class="post-footer"><div class="post-eof"></div></footer></div></article></section><nav class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a></nav></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="aitozi"><p class="site-author-name" itemprop="name">aitozi</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">27</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">9</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">15</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="gjying1314@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-globe"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://wuchong.me/" title="wuchong" target="_blank">wuchong</a></li><li class="links-of-blogroll-item"><a href="http://chenyuzhao.me/" title="yuzhao" target="_blank">yuzhao</a></li><li class="links-of-blogroll-item"><a href="http://blog.csdn.net/yanghua_kobe?viewmode=contents" title="vinoyang" target="_blank">vinoyang</a></li><li class="links-of-blogroll-item"><a href="http://blog.csdn.net/lmalds?viewmode=contents" title="Imalds" target="_blank">Imalds</a></li><li class="links-of-blogroll-item"><a href="http://blog.csdn.net/androidlushangderen" title="hadoop" target="_blank">hadoop</a></li><li class="links-of-blogroll-item"><a href="http://www.cnblogs.com/xrq730/p/5260294.html" title="java开发" target="_blank">java开发</a></li><li class="links-of-blogroll-item"><a href="http://www.hollischuang.com/" title="阿里工程师" target="_blank">阿里工程师</a></li><li class="links-of-blogroll-item"><a href="http://www.cnblogs.com/fxjwind/" title="阿里流计算工程师" target="_blank">阿里流计算工程师</a></li><li class="links-of-blogroll-item"><a href="http://jm.taobao.org/" title="阿里中间件博客" target="_blank">阿里中间件博客</a></li><li class="links-of-blogroll-item"><a href="http://armsword.com/" title="duruofei" target="_blank">duruofei</a></li><li class="links-of-blogroll-item"><a href="http://blog.yufeng.info/" title="褚霸" target="_blank">褚霸</a></li><li class="links-of-blogroll-item"><a href="https://yuzhouwan.com" title="宇宙湾" target="_blank">宇宙湾</a></li><li class="links-of-blogroll-item"><a href="http://matt33.com" title="matt" target="_blank">matt</a></li><li class="links-of-blogroll-item"><a href="http://coding-geek.com/" title="coding-geek" target="_blank">coding-geek</a></li></ul></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2019</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">aitozi</span></div><div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div><div class="theme-info">主题 - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user"></i> <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="site-pv"><i class="fa fa-eye"></i> <span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/jquery.lazyload/1.9.3/jquery.lazyload.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/velocity/1.2.3/velocity.ui.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><script id="dsq-count-scr" src="https://aitozi.disqus.com/count.js" async></script><script>!function(){var t=document.createElement("script"),s=window.location.protocol.split(":")[0];"https"===s?t.src="https://zz.bdstatic.com/linksubmit/push.js":t.src="http://push.zhanzhang.baidu.com/push.js";var e=document.getElementsByTagName("script")[0];e.parentNode.insertBefore(t,e)}()</script></body></html><!-- rebuild by neat -->