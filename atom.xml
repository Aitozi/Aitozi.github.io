<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Aitozi</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.aitozi.com/"/>
  <updated>2019-03-30T17:16:50.325Z</updated>
  <id>http://www.aitozi.com/</id>
  
  <author>
    <name>aitozi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Binary Row数据结构的实现</title>
    <link href="http://www.aitozi.com/%5BBlink%5DBinary%20Row%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%9A%84%E5%AE%9E%E7%8E%B0.html"/>
    <id>http://www.aitozi.com/[Blink]Binary Row数据结构的实现.html</id>
    <published>2019-03-26T01:17:30.000Z</published>
    <updated>2019-03-30T17:16:50.325Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>[toc]</p><p>Binary Row是blink开源版本<a href="https://github.com/apache/flink/tree/blink" target="_blank" rel="noopener">https://github.com/apache/flink/tree/blink</a>中提到的一个runtime层面优化的特性，主要是应用于sql模块，简单来说，由于sql本身自带schema，在上下游数据传输的时候就可以利用这个schema信息来简化序列化和反序列化的过程，本文就来具体分析这个特性的实现。</p><p>&lt;!--more--&gt;</p><p>主要实现代码在</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flink-table org.apache.flink.table.typeutils</span><br><span class="line">flink-table-common org.apache.flink.table.dataformat</span><br><span class="line">flink-table-common org.apache.flink.table.typeutils</span><br></pre></td></tr></table></figure><p></p><h3>Binary Row</h3><p>我们要理解sql层的数据传输是用的什么结构，只需要去观察runtime层实现的算子的传输数据类型即可，通过查看代码可以发现中间算子传输的均为<code>BaseRow</code></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">public TwoInputSelection processElement1(StreamRecord&lt;BaseRow&gt; element) throws Exception &#123;&#125;</span><br></pre></td></tr></table></figure><p></p><p>而之前版本的数据传输的是一个Row,内部是一个<code>Object[]</code>，在传输的过程中使用<code>RowSerializer</code>进行每一个字段的序列化和反序列化</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public void serialize(Row record, DataOutputView target) throws IOException &#123;</span><br><span class="line">	int len = fieldSerializers.length;</span><br><span class="line"></span><br><span class="line">	if (record.getArity() != len) &#123;</span><br><span class="line">		throw new RuntimeException(&quot;Row arity of from does not match serializers.&quot;);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	// write a null mask</span><br><span class="line">	writeNullMask(len, record, target);</span><br><span class="line"></span><br><span class="line">	// serialize non-null fields</span><br><span class="line">	for (int i = 0; i &lt; len; i++) &#123;</span><br><span class="line">		Object o = record.getField(i);</span><br><span class="line">		if (o != null) &#123;</span><br><span class="line">			fieldSerializers[i].serialize(o, target);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>新的实现中以BaseRow代替了Row，baserow是一个基类，在不同的场景下有不同的子类去实现相应的功能。</p><h4>GenericRow</h4><p>能够方便的用以更新字段，其内部实现也是一个Object数组，</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// kafka source deserialization schema 从source处就解析成一个BaseRow</span><br><span class="line">public GenericRow deserialize(byte[] messageKey, byte[] message, String topic, int partition, long offset) throws IOException &#123;</span><br><span class="line">	GenericRow row = new GenericRow(5);</span><br><span class="line">	row.update(0, messageKey);</span><br><span class="line">	row.update(1, message);</span><br><span class="line">	row.update(2, BinaryString.fromString(topic));</span><br><span class="line">	row.update(3, partition);</span><br><span class="line">	row.update(4, offset);</span><br><span class="line">	return row;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4>JoinedRow</h4><p>主要能够方便的将两个row进行拼接成一个baserow</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// windowoperator中发送一个key和aggRes的组合的row到下游</span><br><span class="line">reuseOutput.replace((BaseRow) getCurrentKey(), aggResult);</span><br></pre></td></tr></table></figure><p></p><h4>BinaryRow</h4><p>BaseRow序列化是先转化成BinaryRow，然后再通过BinaryRowSerializer进行序列化</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">// BaseRowSerializer.java</span><br><span class="line">public void serialize(BaseRow row, DataOutputView target) throws IOException &#123;</span><br><span class="line">	BinaryRow binaryRow;</span><br><span class="line">	if (row.getClass() == BinaryRow.class) &#123;</span><br><span class="line">		binaryRow = (BinaryRow) row;</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		binaryRow = baseRowToBinary(row);</span><br><span class="line">	&#125;</span><br><span class="line">	binarySerializer.serialize(binaryRow, target);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public BinaryRow baseRowToBinary(BaseRow baseRow) throws IOException &#123;</span><br><span class="line">	BinaryRow row = getProjection().apply(baseRow);</span><br><span class="line">	row.setHeader(baseRow.getHeader());</span><br><span class="line">	return row;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>可以看到baseToRow的过程中首先是会通过codeGen生成映射函数，然后将baserow转成binaryrow，测试将以下的GenericRow转化成BinaryRow生成如下代码</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GenericRow gR = new GenericRow(3);</span><br><span class="line">gR.update(0, 1);</span><br><span class="line">gR.update(1, 2L);</span><br><span class="line">gR.update(2, &quot;test&quot;);</span><br><span class="line"></span><br><span class="line">BaseRowSerializer&lt;GenericRow&gt; serializer = new BaseRowSerializer&lt;&gt;(Types.INT, Types.LONG, Types.STRING);</span><br><span class="line">BinaryRow row = serializer.baseRowToBinary(gR);</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">public class BaseRowSerializerProjection$0 extends org.apache.flink.table.codegen.Projection&lt;org.apache.flink.table.dataformat.BaseRow, org.apache.flink.table.dataformat.BinaryRow&gt; &#123;</span><br><span class="line"></span><br><span class="line">        org.apache.flink.table.dataformat.BinaryString reuseBString$3 = new org.apache.flink.table.dataformat.BinaryString();</span><br><span class="line">        final org.apache.flink.table.dataformat.BinaryRow out = new org.apache.flink.table.dataformat.BinaryRow(3);</span><br><span class="line">        // 先构建一个BinaryRowWriter</span><br><span class="line">        final org.apache.flink.table.dataformat.BinaryRowWriter outWriter = new org.apache.flink.table.dataformat.BinaryRowWriter(out);</span><br><span class="line"></span><br><span class="line">        public BaseRowSerializerProjection$0() throws Exception &#123;</span><br><span class="line">          </span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public org.apache.flink.table.dataformat.BinaryRow apply(org.apache.flink.table.dataformat.BaseRow in1) &#123;</span><br><span class="line">          int field$1;</span><br><span class="line">          boolean isNull$1;</span><br><span class="line">          long field$2;</span><br><span class="line">          boolean isNull$2;</span><br><span class="line">          org.apache.flink.table.dataformat.BinaryString field$4;</span><br><span class="line">          boolean isNull$4;</span><br><span class="line">          outWriter.reset();</span><br><span class="line">          isNull$1 = in1.isNullAt(0);</span><br><span class="line">          field$1 = -1;</span><br><span class="line">          if (!isNull$1) &#123;</span><br><span class="line">            field$1 = in1.getInt(0);</span><br><span class="line">          &#125;</span><br><span class="line">          if (isNull$1) &#123;</span><br><span class="line">            outWriter.setNullAt(0);</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            outWriter.writeInt(0, field$1);</span><br><span class="line">          &#125;</span><br><span class="line">          isNull$2 = in1.isNullAt(1);</span><br><span class="line">          field$2 = -1L;</span><br><span class="line">          if (!isNull$2) &#123;</span><br><span class="line">            field$2 = in1.getLong(1);</span><br><span class="line">          &#125;</span><br><span class="line">          if (isNull$2) &#123;</span><br><span class="line">            outWriter.setNullAt(1);</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            outWriter.writeLong(1, field$2);</span><br><span class="line">          &#125;</span><br><span class="line">          isNull$4 = in1.isNullAt(2);</span><br><span class="line">          field$4 = org.apache.flink.table.dataformat.BinaryString.EMPTY_UTF8;</span><br><span class="line">          if (!isNull$4) &#123;</span><br><span class="line">            field$4 = in1.getBinaryString(2, reuseBString$3);</span><br><span class="line">          &#125;</span><br><span class="line">          if (isNull$4) &#123;</span><br><span class="line">            outWriter.setNullAt(2);</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            outWriter.writeBinaryString(2, field$4);</span><br><span class="line">          &#125;</span><br><span class="line">          outWriter.complete();</span><br><span class="line">          return out;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><p></p><p>通过codeGen的代码可以得出，对于普通的baserow，通过BinaryRowWriter，将baserow的每个字段写入到BinaryRow中，写入完成后，序列化的工作就都通过BinaryRowSerializer来完成。这样的好处有以下几个:</p><ul><li>如果某个中间算子只需要获取上游传输下来的某几个字段的值，那么只需要通过getXXX来直接获取，减少反序列化的量</li><li>如果中间结果不发生改变，只需要将binaryRow直接拷贝出去，也减少了序列化的量</li></ul><p>BinaryRow序列化的过程，可以看到就是直接的内存拷贝的过程</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public void serialize(BinaryRow record, DataOutputView target) throws IOException &#123;</span><br><span class="line">	int sizeInBytes = record.getSizeInBytes();</span><br><span class="line">	target.writeInt(sizeInBytes);</span><br><span class="line">	int offset = record.getBaseOffset();</span><br><span class="line">	for (MemorySegment segment : record.getAllSegments()) &#123;</span><br><span class="line">		int remain = segment.size() - offset;</span><br><span class="line">		int copySize = remain &gt; sizeInBytes ? sizeInBytes : remain;</span><br><span class="line">		target.write(segment, offset, copySize);</span><br><span class="line"></span><br><span class="line">		sizeInBytes -= copySize;</span><br><span class="line">		offset = 0;</span><br><span class="line">	&#125;</span><br><span class="line">	if (sizeInBytes != 0) &#123;</span><br><span class="line">		throw new RuntimeException(&quot;No copy finished, this should be a bug, &quot; +</span><br><span class="line">				&quot;The remaining length is: &quot; + sizeInBytes);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4>BinaryString</h4><p>在上面codegen的一段代码中，关于string的处理引入了一个概念BinaryString</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">field$4 = org.apache.flink.table.dataformat.BinaryString.EMPTY_UTF8;</span><br><span class="line">if (!isNull$4) &#123;</span><br><span class="line">  field$4 = in1.getBinaryString(2, reuseBString$3);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// GenericRow的getBinaryString的实现</span><br><span class="line">public BinaryString getBinaryString(int ordinal) &#123;</span><br><span class="line">	Object value = this.fields[ordinal];</span><br><span class="line">	if (value instanceof BinaryString) &#123;</span><br><span class="line">		return (BinaryString) value;</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		return BinaryString.fromString((String) value);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>那么BinaryString是什么作用呢? 看注释</p><blockquote><p>A utf8 string which is backed by {@link MemorySegment} instead of String. Its data may span multiple {@link MemorySegment}s. 一个直接存储在MemorySegment上的utf8的字符串，一个字符串数据可能会跨segment</p></blockquote><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">private MemorySegment[] segments;</span><br><span class="line">private int offset;</span><br><span class="line">private int numBytes;</span><br><span class="line"></span><br><span class="line">/** Cache the java string for the binary string to avoid redundant decode. */</span><br><span class="line">private String javaString;</span><br></pre></td></tr></table></figure><p></p><p>针对string类型，会在codegen阶段，将其转化成一个binaryString。从binarystring初始化的时候没有存储在MemorySegment之上，而是仅仅只是保存string的字符串信息，等到有对string的操作的时候,才会通过这个方法将其序列化，并包装成memorysegments</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public void ensureEncoded() &#123;</span><br><span class="line">	if (!isEncoded()) &#123;</span><br><span class="line">		encodeToBytes();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private void encodeToBytes() &#123;</span><br><span class="line">	if (javaString != null) &#123;</span><br><span class="line">		byte[] bytes = StringUtf8Utils.encodeUTF8(javaString);</span><br><span class="line">		pointTo(bytes, 0, bytes.length, javaString);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>序列化</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">public void serialize(BinaryString record, DataOutputView target) throws IOException &#123;</span><br><span class="line">	byte[] bytes = record.getBytes();</span><br><span class="line">	target.writeInt(bytes.length);</span><br><span class="line">	target.write(bytes);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Maybe not copied, if want copy, please use copyTo.</span><br><span class="line"> */</span><br><span class="line">public static byte[] getBytes(MemorySegment[] segments, int baseOffset, int sizeInBytes) &#123;</span><br><span class="line">	// avoid copy if `base` is `byte[]`</span><br><span class="line">	if (segments.length == 1) &#123;</span><br><span class="line">		byte[] heapMemory = segments[0].getHeapMemory();</span><br><span class="line">		// 基于byte[]数组的memorysegment</span><br><span class="line">		if (baseOffset == 0</span><br><span class="line">				&amp;&amp; heapMemory != null</span><br><span class="line">				&amp;&amp; heapMemory.length == sizeInBytes) &#123;</span><br><span class="line">			return heapMemory;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			// 将内存从堆外内存拷贝出来</span><br><span class="line">			byte[] bytes = new byte[sizeInBytes];</span><br><span class="line">			segments[0].get(baseOffset, bytes, 0, sizeInBytes);</span><br><span class="line">			return bytes;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		byte[] bytes = new byte[sizeInBytes];</span><br><span class="line">		BinaryRowUtil.copySlow(segments, baseOffset, bytes, 0, sizeInBytes);</span><br><span class="line">		return bytes;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>BinaryString有什么好处呢？</p><ol><li>仅仅序列化一次</li><li>是可以修改的string，而不会产生中间对象</li><li>序列化的时候仅仅是内存的拷贝</li></ol><h4>BinaryArray</h4><p>同样的基于memorysegment实现的还有BinaryMap和BinaryArray，这两者都有一个Generic的实现用以快速的更新, GenericArray要求<strong>数据类型都是相同的类型</strong></p><p>BinaryArray的存储格式:</p><blockquote><p>[numElements(int)] + [null bits(4-byte word boundaries)] + [values or offset&amp;length] + [variable length part].</p></blockquote><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">public void serialize(BaseArray record, DataOutputView target) throws IOException &#123;</span><br><span class="line">	BinaryArray binaryArray = baseArrayToBinary(record);</span><br><span class="line">	target.write(binaryArray.getBytes());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public BinaryArray baseArrayToBinary(BaseArray from) &#123;</span><br><span class="line">	if (from instanceof BinaryArray) &#123;</span><br><span class="line">		return (BinaryArray) from;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	int numElements = from.numElements();</span><br><span class="line">	if (reuseBinaryArray == null) &#123;</span><br><span class="line">		reuseBinaryArray = new BinaryArray();</span><br><span class="line">	&#125;</span><br><span class="line">	if (reuseBinaryWriter == null || reuseBinaryWriter.getNumElements() != numElements) &#123;</span><br><span class="line">		reuseBinaryWriter = new BinaryArrayWriter(</span><br><span class="line">			reuseBinaryArray, numElements, BinaryArray.calculateElementSize(eleType));</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		reuseBinaryWriter.reset();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	for (int i = 0; i &lt; numElements; i++) &#123;</span><br><span class="line">		if (from.isNullAt(i)) &#123;</span><br><span class="line">			reuseBinaryWriter.setNullAt(i, eleType);</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			BaseRowUtil.write(reuseBinaryWriter, i,</span><br><span class="line">					TypeGetterSetters.get(from, i, eleType), eleType, elementSerializer);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	reuseBinaryWriter.complete();</span><br><span class="line"></span><br><span class="line">	return reuseBinaryArray;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>从BinaryArraySerializer可以看到，处理方式和BaseRow很像，先baseToBinary，然后直接从segments拷贝byte。</p><h4>DataStructureConverters</h4><p>以上的类型和这个类型convert搭配使用才发挥出相应的效果，这个工具类的作用在于在codegen的阶段，根据输入的类型去转化为相对应的InternalType</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def genToInternal(ctx: CodeGeneratorContext, t: DataType): String =&gt; String = &#123;</span><br><span class="line">    val iTerm = boxedTypeTermForType(t.toInternalType)</span><br><span class="line">    val eTerm = externalBoxedTermForType(t)</span><br><span class="line">    if (isIdentity(t)) &#123;</span><br><span class="line">      term =&gt; s&quot;($iTerm) $term&quot;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      val scalarFuncTerm = classOf[BuildInScalarFunctions].getCanonicalName</span><br><span class="line">      TypeConverters.createExternalTypeInfoFromDataType(t) match &#123;</span><br><span class="line">        case Types.STRING =&gt; term =&gt; s&quot;$BINARY_STRING.fromString($term)&quot;</span><br><span class="line">        case Types.SQL_DATE | Types.SQL_TIME =&gt;</span><br><span class="line">          term =&gt; s&quot;$scalarFuncTerm.safeToInt(($eTerm) $term)&quot;</span><br><span class="line">        case Types.SQL_TIMESTAMP =&gt; term =&gt; s&quot;$scalarFuncTerm.safeToLong(($eTerm) $term)&quot;</span><br><span class="line">        case _ =&gt;</span><br><span class="line">          val converter = genConvertField(ctx, createToInternalConverter(t))</span><br><span class="line">          term =&gt; s&quot;($iTerm) $converter.apply($term)&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">// 生成的类型转化函数</span><br><span class="line">val internal = genToInternalIfNeeded(ctx, resultExternalType, resultClass, javaTerm)</span><br><span class="line">        s&quot;&quot;&quot;</span><br><span class="line">            |$javaTypeTerm $javaTerm = ($javaTypeTerm) $evalResult;</span><br><span class="line">            |$resultTerm = $javaTerm == null ? null : ($internal);</span><br><span class="line">            &quot;&quot;&quot;.stripMargin</span><br></pre></td></tr></table></figure><p></p><p>这样在codegen阶段就完成了相应类型的替换</p><h4>BinaryRow和BinaryArray的底层存储</h4><p>上面我们看到了binaryRow的使用方式，通过writeXXX的方式将数据写入到一个row中，一个row中可以写入基本类型，也可以写入binaryString, binaryArray，binaryMap等等变长的数据结构，其存储方式如下所示：</p><p><img src="https://github.com/Aitozi/images/blob/master/flink/flink-binaryrow.jpg?raw=true" alt="BinaryRow"></p><blockquote><p>A Row has two part: Fixed-length part and variable-length part. Fixed-length part contains null bit set and field values. Null bit set is used for null tracking and is aligned to 8-byte word boundaries. <code>Field values</code> holds fixed-length primitive types and variable-length values which can be stored in 8 bytes inside. If it do not fit the variable-length field, then store the length and offset of variable-length part. Fixed-length part will certainly fall into a MemorySegment, which will speed up the read and write of field. Variable-length part may fall into multiple MemorySegments.</p></blockquote><p>在将其他格式通过baseRowToBinaryRow的时候，确定了BinaryRow包含的field个数，</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public BinaryRowWriter(BinaryRow row, int initialSize) &#123;</span><br><span class="line">	this.nullBitsSizeInBytes = BinaryRow.calculateBitSetWidthInBytes(row.getArity());</span><br><span class="line">	this.fixedSize = row.getFixedLengthPartSize();</span><br><span class="line">	this.cursor = fixedSize;</span><br><span class="line"></span><br><span class="line">	this.segment = MemorySegmentFactory.wrap(new byte[fixedSize + initialSize]);</span><br><span class="line">	this.row = row;</span><br><span class="line">	this.row.pointTo(segment, 0, segment.size());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public static int calculateBitSetWidthInBytes(int arity) &#123;</span><br><span class="line">	// add 8 bit header</span><br><span class="line">	return ((arity + 63 + 8) / 64) * 8;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>这个函数是计算出null值得Flag位需要多少Byte来表示，这里的+63是将其对其到8的倍数（向上去整的意思），+8和spark代码相比其实是因为flink多了一个header存储回撤消息的标志位,null bits中第一个byte存储了header位的信息，这里的null bits的作用主要是用在runtime处理时可以快速判断一条数据是不是null值。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public boolean anyNull() &#123;</span><br><span class="line">	// 这里有一个疑问，判断null的时候不是应该跳过第一个header位吗</span><br><span class="line">	for (int i = 0; i &lt; nullBitsSizeInBytes; i += 8) &#123;</span><br><span class="line">		if (segment.getLong(i) != 0) &#123;</span><br><span class="line">			return true;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	return false;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public int getFixedLengthPartSize() &#123;</span><br><span class="line">	return nullBitsSizeInBytes + 8 * arity;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>获取整个固定长度字段的长度，再写变长区时就从这个offset写起。</p><p><strong>写定长数据</strong></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public void writeByte(int pos, byte value) &#123;</span><br><span class="line">	segment.put(getFieldOffset(pos), value);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p><strong>写不定长的数据</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public void writeString(int pos, String input) &#123;</span><br><span class="line">	byte[] bytes = StringUtf8Utils.allocateBytes(input.length() * MAX_BYTES_PER_CHAR);</span><br><span class="line">	int len = StringUtf8Utils.encodeUTF8(input, bytes);</span><br><span class="line">	if (len &lt;= 7) &#123;</span><br><span class="line">		// 小于记录length的长度时直接写在固定长度区</span><br><span class="line">		writeLittleBytes(segment, getFieldOffset(pos), bytes, len);</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		writeBigBytes(pos, bytes, len);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>BinaryRow/BinaryRowWriter的实现和Spark中UnsafeRow/UnsafeRowWriter的实现非常相似，spark中的对此数据结构解释更为清晰一些。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * An Unsafe implementation of Row which is backed by raw memory instead of Java objects.</span><br><span class="line"> *</span><br><span class="line"> * Each tuple has three parts: [null bit set] [values] [variable length portion]</span><br><span class="line"> *</span><br><span class="line"> * The bit set is used for null tracking and is aligned to 8-byte word boundaries.  It stores</span><br><span class="line"> * one bit per field.</span><br><span class="line"> *</span><br><span class="line"> * In the `values` region, we store one 8-byte word per field. For fields that hold fixed-length</span><br><span class="line"> * primitive types, such as long, double, or int, we store the value directly in the word. For</span><br><span class="line"> * fields with non-primitive or variable-length values, we store a relative offset (w.r.t. the</span><br><span class="line"> * base address of the row) that points to the beginning of the variable-length field, and length</span><br><span class="line"> * (they are combined into a long).</span><br><span class="line"> *</span><br><span class="line"> */</span><br></pre></td></tr></table></figure><p></p><h3>题外话</h3><p>看代码的时候遵循的是一种推理加源码追踪的手段，但是代码在初始设计的时候应该是另一种维度的思考，因此应该换一种思路去想如果自己来实现这个feature需要考虑什么地方，多多思考。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;[toc]&lt;/p&gt;&lt;p&gt;Binary Row是blink开源版本&lt;a href=&quot;https://github.com/apache/flink/tr
    
    </summary>
    
      <category term="源码解析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"/>
    
    
      <category term="Blink" scheme="http://www.aitozi.com/tags/Blink/"/>
    
  </entry>
  
  <entry>
    <title>flink sql与calcite</title>
    <link href="http://www.aitozi.com/flink-sql-tutorial.html"/>
    <id>http://www.aitozi.com/flink-sql-tutorial.html</id>
    <published>2019-03-25T08:45:30.000Z</published>
    <updated>2019-03-26T01:51:33.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>[toc]</p><p>基于Flink1.4.2版本分析flink与calcite结合构建的flink sql模块。</p><p>&lt;!--more--&gt;</p><p>Flink SQL是现在Flink社区中着重发展的一个模块，我理解主要原因是因为</p><ol><li>SQL是一门发展很有的通用的描述性语言，接入门槛较低</li><li>有希望在sql层面实现流批计算的统一</li><li>能够通过sql优化器内置优化能力，避免需要每个用户方需要理解低阶任务的调优，屏蔽实现细节</li></ol><h3>概述</h3><p>Flink SQL的总体执行流程为：</p><ul><li><em>SELECT</em>查询语句经过caclite parse成SqlNode</li><li>SqlNode经过validate校验</li><li>SqlNode经过calcite转化为relNode</li><li><em>Insert</em>语句将relNode经过calcite的优化和转化成FlinkRelNode</li><li>将相应的FlinkRelNode和codeGen生成的Function结合生成相应的执行算子</li></ul><p>下面以一个查询sql来讲解整体流程:</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">val stream = env</span><br><span class="line">        .fromCollection(data)</span><br><span class="line">        .assignTimestampsAndWatermarks(</span><br><span class="line">          new TimestampAndWatermarkWithOffset[(Long, String, String)](0L))</span><br><span class="line">val table = stream.toTable(tEnv, &apos;a, &apos;b, &apos;c, &apos;rowtime.rowtime)</span><br><span class="line"></span><br><span class="line">tEnv.registerTable(&quot;T1&quot;, table)</span><br><span class="line"></span><br><span class="line">val sqlQuery = &quot;SELECT c, COUNT(*), COUNT(1), COUNT(b) FROM T1 &quot; +</span><br><span class="line">  &quot;GROUP BY TUMBLE(rowtime, interval &apos;5&apos; SECOND), c&quot;</span><br><span class="line"></span><br><span class="line">val result = tEnv.sqlQuery(sqlQuery).toAppendStream[Row]</span><br><span class="line">result.addSink(new StreamITCase.StringSink[Row])</span><br><span class="line"></span><br><span class="line">env.execute()</span><br></pre></td></tr></table></figure><p></p><h3>parse</h3><h4>SqlNode</h4><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val parser: SqlParser = SqlParser.create(sql, parserConfig)</span><br><span class="line">val sqlNode: SqlNode = parser.parseStmt</span><br><span class="line">sqlNode</span><br></pre></td></tr></table></figure><p></p><p><img src="https://github.com/Aitozi/images/blob/master/flink/flink-sql-node.png?raw=true" alt="SqlNode" title="SqlNode"></p><p>SqlNode表示的是一颗sql解析树，由于Flink暂时只支持SELECT查询，所以我们这里得到的其实是一个<code>SqlSelect</code>实例，SqlSelect是一个SqlCall，SqlCall继承自SqlNode，每一个无叶子节点的节点就是一个Sqlcall，常见的SqlNode的子类就是，SqlKind是所有SqlNode类型的枚举类:</p><ul><li>SqlCall 表示一个树的无叶子节点的调用，例如图中的Count(*)</li><li>SqlNodeList 表示SqlNode的集合</li><li>SqlIdentifer 表示某个标识符</li></ul><h4>SqlOperator</h4><p>SqlNode的成员方法：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public List&lt;SqlNode&gt; getOperandList() &#123;</span><br><span class="line">  return ImmutableNullableList.of(keywordList, selectList, from, where,</span><br><span class="line">      groupBy, having, windowDecls, orderBy, offset, fetch);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public SqlOperator getOperator() &#123;</span><br><span class="line">  return SqlSelectOperator.INSTANCE;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>getOperator返回的是这个是个什么操作，operands得到的运算对象。每一个SqlNode是由作用于一系列SqlNode的SqlOperator组成，SqlFunction也是一种SqlOperator. 这里SqlSelect node是SqlSelectOperator作用于以下的SqlNode节点</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">SqlNodeList keywordList;</span><br><span class="line">SqlNodeList selectList;</span><br><span class="line">SqlNode from;</span><br><span class="line">SqlNode where;</span><br><span class="line">SqlNodeList groupBy;</span><br><span class="line">SqlNode having;</span><br><span class="line">SqlNodeList windowDecls;</span><br><span class="line">SqlNodeList orderBy;</span><br><span class="line">SqlNode offset;</span><br><span class="line">SqlNode fetch;</span><br><span class="line">SqlMatchRecognize matchRecognize;</span><br></pre></td></tr></table></figure><p></p><h3>rel</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val rexBuilder: RexBuilder = createRexBuilder</span><br><span class="line">val cluster: RelOptCluster = FlinkRelOptClusterFactory.create(planner, rexBuilder)</span><br><span class="line">val config = SqlToRelConverter.configBuilder()</span><br><span class="line">  .withTrimUnusedFields(false).withConvertTableAccess(false).build()</span><br><span class="line">val sqlToRelConverter: SqlToRelConverter = new SqlToRelConverter(</span><br><span class="line">  new ViewExpanderImpl, validator, createCatalogReader, cluster, convertletTable, config)</span><br><span class="line">root = sqlToRelConverter.convertQuery(validatedSqlNode, false, true)</span><br></pre></td></tr></table></figure><p></p><p>这个过程是将SqlNode转化为RelNode的过程，RelNode表示关系型表达式,代表的是对数据的一个操作常见的有Project,Scan,Filter,Join等。通过explain可以看到相应的逻辑执行计划，以下还包括优化后的物理执行计划的一部分</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"> == Abstract Syntax Tree ==</span><br><span class="line">LogicalProject(c=[$1], EXPR$1=[$2], EXPR$2=[$2], EXPR$3=[$3])</span><br><span class="line">  LogicalAggregate(group=[&#123;0, 1&#125;], EXPR$1=[COUNT()], EXPR$3=[COUNT($3)])</span><br><span class="line">    LogicalProject($f0=[TUMBLE($3, 5000)], c=[$2], $f2=[1], b=[$1])</span><br><span class="line">      LogicalTableScan(table=[[T1]])</span><br><span class="line"></span><br><span class="line">== Optimized Logical Plan ==</span><br><span class="line">DataStreamCalc(select=[c, EXPR$1, EXPR$1 AS EXPR$2, EXPR$3])</span><br><span class="line">  DataStreamGroupWindowAggregate(groupBy=[c], window=[TumblingGroupWindow(&apos;w$, &apos;rowtime, 5000.millis)], select=[c, COUNT(*) AS EXPR$1, COUNT(b) AS EXPR$3])</span><br><span class="line">    DataStreamCalc(select=[rowtime, c, 1 AS $f2, b])</span><br><span class="line">      DataStreamScan(table=[[_DataStreamTable_0]])</span><br><span class="line"></span><br><span class="line">== Physical Execution Plan ==</span><br><span class="line">Stage 1 : Data Source</span><br><span class="line">	content : collect elements with CollectionInputFormat</span><br><span class="line"></span><br><span class="line">	Stage 2 : Operator</span><br><span class="line">		content : Timestamps/Watermarks</span><br><span class="line">		ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">		Stage 3 : Operator</span><br><span class="line">			content : from: (a, b, c, rowtime)</span><br><span class="line">			ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">			Stage 4 : Operator</span><br><span class="line">				content : select: (rowtime, c, 1 AS $f2, b)</span><br><span class="line">				ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">				Stage 5 : Operator</span><br><span class="line">					content : time attribute: (rowtime)</span><br><span class="line">					ship_strategy : FORWARD</span><br><span class="line"></span><br><span class="line">					Stage 7 : Operator</span><br><span class="line">						content : groupBy: (c), window: (TumblingGroupWindow(&apos;w$, &apos;rowtime, 5000.millis)), select: (c, COUNT(*) AS EXPR$1, COUNT(b) AS EXPR$3)</span><br><span class="line">						ship_strategy : HASH</span><br><span class="line"></span><br><span class="line">						Stage 8 : Operator</span><br><span class="line">							content : select: (c, EXPR$1, EXPR$1 AS EXPR$2, EXPR$3)</span><br><span class="line">							ship_strategy : FORWARD</span><br></pre></td></tr></table></figure><p></p><p>Flink sql查询的时候只做到这里的LogicalNode生成之后就完成了，等待sink才会触发下一步优化和转化逻辑。</p><h4>RelNode，RexNode</h4><p>RelNode的实现类有LogicalProject，LogicalScan等表示的是数据处理方式，rexnode表示的是行表达式，是包含在一个RelNode中的,RexNode类中的exps字段就存储了相应的数据操作所需要的行表达式</p><p>参考以下讨论:</p><blockquote><p>Difference between sqlnode and relnode and rexnode https://www.mail-archive.com/dev@calcite.apache.org/msg01674.html</p></blockquote><h4>RexTraits, RelTraitDef</h4><p>这个表示的是一个RelNode的物理特性，用于在convertRule中使用</p><p>在一个<code>ConverterRule</code>中的convert</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val scan: FlinkLogicalNativeTableScan = rel.asInstanceOf[FlinkLogicalNativeTableScan]</span><br><span class="line">val traitSet: RelTraitSet = rel.getTraitSet.replace(FlinkConventions.DATASTREAM)</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public RelTraitSet replace(</span><br><span class="line">    RelTrait trait) &#123;</span><br><span class="line">  // Quick check for common case</span><br><span class="line">  if (containsShallow(traits, trait)) &#123;</span><br><span class="line">    return this;</span><br><span class="line">  &#125;</span><br><span class="line">  final RelTraitDef traitDef = trait.getTraitDef();</span><br><span class="line">  int index = findIndex(traitDef);</span><br><span class="line">  if (index &lt; 0) &#123;</span><br><span class="line">    // Trait is not present. Ignore it.</span><br><span class="line">    return this;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  return replace(index, trait);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>实际上是把某一类relTraitsDef的trait实现更换掉，相当于变更了RelNode的物理实现，planner转化过程应该只是trait的转化过程以及相应的RelNode的物理化的过程，按照论文中的解释:</p><blockquote><p>Traits. Calcite does not use different entities to represent logical and physical operators. Instead, it describes the physical properties associated with an operator using traits. These traits help the optimizer evaluate the cost of different alternative plans. Changing a trait value does not change the logical expression being evaluated, i.e., the rows produced by the given operator will still be the same</p></blockquote><p>因此在Flink中的转化<code>StreamTableEnvironment#optimize</code>,match之后根据HepPlanner和VocanoPlanner进行convert转化成物理算子，例如将</p><p><code>LogicalJoin(RelNode) -&gt; DataStreamJoin(FlinkRelNode) -&gt; translateToPlan -&gt; NonWindowJoin (runtime)</code></p><h3>总结</h3><p>Flink SQL具体在flink中的实现分为LogicalPlan层，经过应用rule optimize之后的RelNode层，例如: DataStreamJoin, 再通过translate的时候code generator以及调用相应的runtime层的具体算子实现（这个对应的是physical plan的翻译），以上就完成了从SQL到Flink执行计划的翻译。从整个流程看calcite全程参与，使用方式非常的方便，足见整个calcite框架的扩展性做的很好。 Flink SQL中还有许多其他的细节：SQL中的回撤消息，join，distinct的具体通用算子的实现，还有sql优化，分流，ddl的实现，antlr的实现,窗口聚合等等这些实现细节后文再具体分析。</p><h3>参考</h3><p>介绍calcite与flink sql比较好的几篇文章：</p><p><a href="https://zhuanlan.zhihu.com/p/48735419" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/48735419</a> <a href="https://arxiv.org/pdf/1802.10233.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1802.10233.pdf</a> calcite的论文 <a href="https://zhuanlan.zhihu.com/p/51221350" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/51221350</a> <a href="https://zhuanlan.zhihu.com/p/58249033" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/58249033</a> <a href="https://zhuanlan.zhihu.com/p/59643962" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59643962</a></p><hr><p><a href="http://matt33.com/2019/03/17/apache-calcite-planner/" target="_blank" rel="noopener">http://matt33.com/2019/03/17/apache-calcite-planner/</a> <a href="http://matt33.com/2019/03/07/apache-calcite-process-flow/" target="_blank" rel="noopener">http://matt33.com/2019/03/07/apache-calcite-process-flow/</a> <a href="https://www.slideshare.net/julianhyde/costbased-query-optimization-in-apache-phoenix-using-apache-calcite?qid=b7a1ca0f-e7bf-49ad-bc51-0615ec8a4971&amp;v=&amp;b=&amp;from_search=4" target="_blank" rel="noopener">https://www.slideshare.net/julianhyde/costbased-query-optimization-in-apache-phoenix-using-apache-calcite?qid=b7a1ca0f-e7bf-49ad-bc51-0615ec8a4971&amp;v=&amp;b=&amp;from_search=4</a></p><hr><p><a href="https://issues.apache.org/jira/browse/FLINK-7146" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/FLINK-7146</a> Flink SQL DDL支持 <a href="https://docs.google.com/document/d/1TTP-GCC8wSsibJaSUyFZ_5NBAHYEB1FVmPpP7RgDGBA/edit#heading=h.wpsqidkaaoil" target="_blank" rel="noopener">https://docs.google.com/document/d/1TTP-GCC8wSsibJaSUyFZ_5NBAHYEB1FVmPpP7RgDGBA/edit#heading=h.wpsqidkaaoil</a> doc</p><p><a href="https://github.com/TatianaJin/calcite_playground/wiki/Query-Planning-&amp;-Optimization-II.a:-VolcanoPlanner-Basics" target="_blank" rel="noopener">https://github.com/TatianaJin/calcite_playground/wiki/Query-Planning-&amp;-Optimization-II.a:-VolcanoPlanner-Basics</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;[toc]&lt;/p&gt;&lt;p&gt;基于Flink1.4.2版本分析flink与calcite结合构建的flink sql模块。&lt;/p&gt;&lt;p&gt;&amp;lt;!--mor
    
    </summary>
    
      <category term="源码解析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
      <category term="Calcite" scheme="http://www.aitozi.com/tags/Calcite/"/>
    
  </entry>
  
  <entry>
    <title>下篇·flink基于rocksdb的timerService</title>
    <link href="http://www.aitozi.com/flink-timerservice-based-on-rocksdb-2.html"/>
    <id>http://www.aitozi.com/flink-timerservice-based-on-rocksdb-2.html</id>
    <published>2019-03-16T04:13:03.000Z</published>
    <updated>2019-03-16T04:43:21.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>[toc]</p><p>接上文分析，要将timer改成基于rocksdb，其实就是要对存储timer的set和queue提供基于rocksdb的存储方案。以下我们基于flink1.7版本源码分析</p><p>&lt;!--more--&gt;</p><p><strong>registerProcessingTimeTimer</strong></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public void registerProcessingTimeTimer(N namespace, long time) &#123;</span><br><span class="line">	InternalTimer&lt;K, N&gt; oldHead = processingTimeTimersQueue.peek();</span><br><span class="line">	if (processingTimeTimersQueue.add(new TimerHeapInternalTimer&lt;&gt;(time, (K) keyContext.getCurrentKey(), namespace))) &#123;</span><br><span class="line">		long nextTriggerTime = oldHead != null ? oldHead.getTimestamp() : Long.MAX_VALUE;</span><br><span class="line">		// check if we need to re-schedule our timer to earlier</span><br><span class="line">		// 如果新加入的timer的时间更早触发，那么就需要把先前的timer取消</span><br><span class="line">		if (time &lt; nextTriggerTime) &#123;</span><br><span class="line">			if (nextTimer != null) &#123;</span><br><span class="line">				nextTimer.cancel(false);</span><br><span class="line">			&#125;</span><br><span class="line">			nextTimer = processingTimeService.registerTimer(time, this);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>可以和看到1.4版本中的基本逻辑是一致的，只是存储方式变化了，下面我们就来分析一下新的存储方式是怎么实现的。在存储的选择上依然有Heap和RocksDB两个方式</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">switch (priorityQueueStateType) &#123;</span><br><span class="line">			case HEAP:</span><br><span class="line">				this.priorityQueueFactory = new HeapPriorityQueueSetFactory(keyGroupRange, numberOfKeyGroups, 128);</span><br><span class="line">				break;</span><br><span class="line">			case ROCKSDB:</span><br><span class="line">				this.priorityQueueFactory = new RocksDBPriorityQueueSetFactory();</span><br><span class="line">				break;</span><br><span class="line">			default:</span><br><span class="line">				throw new IllegalArgumentException(&quot;Unknown priority queue state type: &quot; + priorityQueueStateType);</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure><p></p><p>从timerService的需求来看我们可以看到这样的几个需求：</p><ol><li>能够每次poll出最近需要触发的timer，实际上是需要维护一个小顶堆</li><li>能够对每一个key的timer去重</li></ol><p>针对这两个需求，总体来看基于Heap的实现是通过基于数组实现了一个二叉堆，具体实现类为<code>HeapPriorityQueue</code>, 然后针对去重的功能又继承该<code>PQ</code>，通过一个hashmap数组，数组的每一个元素代表一个KG的一组不重复timer，同时这组timer内部也维护了timer在二叉堆中存储的下标，方便<code>deleteTimer</code>时的快速删除。</p><h4>基于Heap的实现</h4><p><code>HeapPriorityQueueElement</code>,<code>AbstractHeapPriorityQueue</code>,<code>HeapPriorityQueue</code>,<code>HeapPriorityQueueSet</code></p><ol><li>存储基于数组，通过<code>HeapPriorityQueueElement</code>记录自己所在的index，可以达到快速删除的目的</li><li>数组中存储的是一个二叉树，数组的起始位置是从1开始，为了使一些热点方法做更少的计算</li></ol><p>在实现上是采用template的设计模式，主要实现逻辑交由子类来实现:</p><ul><li>addInternal</li><li>removeInternal</li><li>getHeadElementIndex</li></ul><h5>addInternal</h5><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public boolean add(@Nonnull T toAdd) &#123;</span><br><span class="line">	addInternal(toAdd);</span><br><span class="line">	return toAdd.getInternalIndex() == getHeadElementIndex();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>添加一个timer至数组中，返回值<code>false</code>表示队首的元素没有改变，<code>true</code>则表示改变了或者不确定</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">protected void addInternal(@Nonnull T element) &#123;</span><br><span class="line">	final int newSize = increaseSizeByOne();</span><br><span class="line">	moveElementToIdx(element, newSize);</span><br><span class="line">	siftUp(newSize);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">private int increaseSizeByOne() &#123;</span><br><span class="line">	final int oldArraySize = queue.length;</span><br><span class="line">	final int minRequiredNewSize = ++size;</span><br><span class="line">	if (minRequiredNewSize &gt;= oldArraySize) &#123;</span><br><span class="line">		final int grow = (oldArraySize &lt; 64) ? oldArraySize + 2 : oldArraySize &gt;&gt; 1;</span><br><span class="line">		// 当存储元素的个数大于数组长度时，需要进行扩容，通过`Arrays.copyOf`进行数组内容的拷贝</span><br><span class="line">		resizeQueueArray(oldArraySize + grow, minRequiredNewSize);</span><br><span class="line">	&#125;</span><br><span class="line">	// TODO implement shrinking as well?</span><br><span class="line">	return minRequiredNewSize;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 将新加入的元素存储到相应的idx处，并且记录该元素在queue中的位置</span><br><span class="line">protected void moveElementToIdx(T element, int idx) &#123;</span><br><span class="line">		queue[idx] = element;</span><br><span class="line">		element.setInternalIndex(idx);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">private void siftUp(int idx) &#123;</span><br><span class="line">		final T[] heap = this.queue;</span><br><span class="line">		final T currentElement = heap[idx];</span><br><span class="line">		int parentIdx = idx &gt;&gt;&gt; 1;</span><br><span class="line">		</span><br><span class="line">		// 每次将比较的index，缩小一半，如果被比较元素的优先级高于新插入的元素就将被比较元素后移，直至比较到第一个元素。这样能够保证idx为1的元素是最早时间触发的</span><br><span class="line">		while (parentIdx &gt; 0 &amp;&amp; isElementPriorityLessThen(currentElement, heap[parentIdx])) &#123;</span><br><span class="line">			moveElementToIdx(heap[parentIdx], idx);</span><br><span class="line">			idx = parentIdx;</span><br><span class="line">			parentIdx &gt;&gt;&gt;= 1;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		moveElementToIdx(currentElement, idx);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 比较两个值的优先级</span><br><span class="line">private boolean isElementPriorityLessThen(T a, T b) &#123;</span><br><span class="line">		return elementPriorityComparator.comparePriority(a, b) &lt; 0;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><h5>removeInternal</h5><ol><li>抽取第一个timer用以触发</li><li>用户删除某个timer的行为</li></ol><p>删除的方式是通过idx下标来实现快速删除的，这也就是<code>HeapPriorityQueueElement</code>中记录idx的作用</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">protected T removeInternal(int removeIdx) &#123;</span><br><span class="line">	T[] heap = this.queue;</span><br><span class="line">	T removedValue = heap[removeIdx];</span><br><span class="line"></span><br><span class="line">	// 要删除的idx应该和内部存储value值保存的idx一致</span><br><span class="line">	assert removedValue.getInternalIndex() == removeIdx;</span><br><span class="line"></span><br><span class="line">	final int oldSize = size;</span><br><span class="line"></span><br><span class="line">	// 删除的不是数组的最后一个元素需要进行位置的调整</span><br><span class="line">	if (removeIdx != oldSize) &#123;</span><br><span class="line">		T element = heap[oldSize];</span><br><span class="line">		// 将原先的最后一个元素放置到要删除的idx处，但是这样的放置没有考虑优先级</span><br><span class="line">		moveElementToIdx(element, removeIdx);</span><br><span class="line">		adjustElementAtIndex(element, removeIdx);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	heap[oldSize] = null;</span><br><span class="line"></span><br><span class="line">	--size;</span><br><span class="line">	return removedValue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">private void adjustElementAtIndex(T element, int index) &#123;</span><br><span class="line">	siftDown(index);</span><br><span class="line">	if (queue[index] == element) &#123;</span><br><span class="line">		siftUp(index);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">private void siftDown(int idx) &#123;</span><br><span class="line">	final T[] heap = this.queue;</span><br><span class="line">	final int heapSize = this.size;</span><br><span class="line"></span><br><span class="line">	final T currentElement = heap[idx];</span><br><span class="line">	int firstChildIdx = idx &lt;&lt; 1;</span><br><span class="line">	int secondChildIdx = firstChildIdx + 1;</span><br><span class="line"></span><br><span class="line">	if (isElementIndexValid(secondChildIdx, heapSize) &amp;&amp;</span><br><span class="line">		isElementPriorityLessThen(heap[secondChildIdx], heap[firstChildIdx])) &#123;</span><br><span class="line">		firstChildIdx = secondChildIdx;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	while (isElementIndexValid(firstChildIdx, heapSize) &amp;&amp;</span><br><span class="line">		isElementPriorityLessThen(heap[firstChildIdx], currentElement)) &#123;</span><br><span class="line">		moveElementToIdx(heap[firstChildIdx], idx);</span><br><span class="line">		idx = firstChildIdx;</span><br><span class="line">		firstChildIdx = idx &lt;&lt; 1;</span><br><span class="line">		secondChildIdx = firstChildIdx + 1;</span><br><span class="line"></span><br><span class="line">		if (isElementIndexValid(secondChildIdx, heapSize) &amp;&amp;</span><br><span class="line">			isElementPriorityLessThen(heap[secondChildIdx], heap[firstChildIdx])) &#123;</span><br><span class="line">			firstChildIdx = secondChildIdx;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	moveElementToIdx(currentElement, idx);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>以上的操作大致上是一个二叉堆的增删的调整过程，涉及的具体算法可以查阅下文末的资料。</p><hr><p>以下来分析rocksdb存储的实现</p><h4>KeyGroupPartitionedPriorityQueue</h4><p>基于rocksdb的存储是通过这个<code>KeyGroupPartitionedPriorityQueue</code>类来实现的，这个类中通过一个内存优先级队列，也就是上文中提到的内部实现的<code>HeapPriorityQueue</code>，用以存储所有KG的timer，而每一个分组的timer是如何存储呢？</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for (int i = 0; i &lt; keyGroupedHeaps.length; i++) &#123;</span><br><span class="line">			final PQ keyGroupSubHeap =</span><br><span class="line">				orderedCacheFactory.create(firstKeyGroup + i, totalKeyGroups, keyExtractor, elementPriorityComparator);</span><br><span class="line">			keyGroupedHeaps[i] = keyGroupSubHeap;</span><br><span class="line">			heapOfKeyGroupedHeaps.add(keyGroupSubHeap);</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure><p></p><p>在这里的构造函数可以看到，其实是通过<code>orderedCacheFactory</code>,从字面意思看是一个有序的缓存，也就是为每一个KG创建一个有序的缓存类，并将其添加到优先级队列中，这里的<code>subHeap</code>也是一个可比较的类，相当于去取这两个<code>subHeap</code>的堆顶的元素拿出来比较下就可以知道这两个subheap的排序方式了。</p><p>比如<code>poll</code>的逻辑,首先先从HeapPQ中挑出堆顶（一个subPQ），然后再从这个PQ中取出堆顶就是要触发的timer了，而这个subPQ就是真是数据（timer）存储的地方了。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public T poll() &#123;</span><br><span class="line">	final PQ headList = heapOfKeyGroupedHeaps.peek();</span><br><span class="line">	final T head = headList.poll();</span><br><span class="line">	heapOfKeyGroupedHeaps.adjustModifiedElement(headList);</span><br><span class="line">	return head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4>RocksDBCachingPriorityQueueSet</h4><p>这个是上节中<code>subHeap</code>的实现类</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">private void checkRefillCacheFromStore() &#123;</span><br><span class="line">		// 不是所有的元素都在cache(treeset)中，并且cache为空  </span><br><span class="line">		if (!allElementsInCache &amp;&amp; orderedCache.isEmpty()) &#123;</span><br><span class="line">			try (final RocksBytesIterator iterator = orderedBytesIterator()) &#123;</span><br><span class="line">				// 捞取rocksdb中这个columnFamily的部分数据填充treeset至maxsize</span><br><span class="line">				orderedCache.bulkLoadFromOrderedIterator(iterator);</span><br><span class="line">				allElementsInCache = !iterator.hasNext();</span><br><span class="line">			&#125; catch (Exception e) &#123;</span><br><span class="line">				throw new FlinkRuntimeException(&quot;Exception while refilling store from iterator.&quot;, e);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public E peek() &#123;</span><br><span class="line"></span><br><span class="line">		checkRefillCacheFromStore();</span><br><span class="line"></span><br><span class="line">		if (peekCache != null) &#123;</span><br><span class="line">			// 这个是维护的全局变量，只有在堆顶改变后在会置为null</span><br><span class="line">			return peekCache;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		byte[] firstBytes = orderedCache.peekFirst();</span><br><span class="line">		if (firstBytes != null) &#123;</span><br><span class="line">			peekCache = deserializeElement(firstBytes);</span><br><span class="line">			return peekCache;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			return null;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public E poll() &#123;</span><br><span class="line"></span><br><span class="line">		checkRefillCacheFromStore();</span><br><span class="line"></span><br><span class="line">		final byte[] firstBytes = orderedCache.pollFirst();</span><br><span class="line"></span><br><span class="line">		if (firstBytes == null) &#123;</span><br><span class="line">			return null;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		// write-through sync</span><br><span class="line">		// 为什么不需要删除treeset中的元素呢？</span><br><span class="line">		removeFromRocksDB(firstBytes);</span><br><span class="line"></span><br><span class="line">		// 删除了这个columnFamily最后一个元素</span><br><span class="line">		if (orderedCache.isEmpty()) &#123;</span><br><span class="line">			seekHint = firstBytes;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		// 少一步反序列化的操作</span><br><span class="line">		if (peekCache != null) &#123;</span><br><span class="line">			E fromCache = peekCache;</span><br><span class="line">			peekCache = null;</span><br><span class="line">			return fromCache;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			return deserializeElement(firstBytes);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">public boolean add(@Nonnull E toAdd) &#123;</span><br><span class="line"></span><br><span class="line">		checkRefillCacheFromStore();</span><br><span class="line"></span><br><span class="line">		final byte[] toAddBytes = serializeElement(toAdd);</span><br><span class="line"></span><br><span class="line">		final boolean cacheFull = orderedCache.isFull();</span><br><span class="line"></span><br><span class="line">		// 如果cache没满并且之前所有元素都在cache中了  或者新加入的元素的优先级通过byte数组的优先级比较发现应该在堆顶</span><br><span class="line">		if ((!cacheFull &amp;&amp; allElementsInCache) ||</span><br><span class="line">			LEXICOGRAPHIC_BYTE_COMPARATOR.compare(toAddBytes, orderedCache.peekLast()) &lt; 0) &#123;</span><br><span class="line"></span><br><span class="line">			if (cacheFull) &#123;</span><br><span class="line">				// we drop the element with lowest priority from the cache</span><br><span class="line">				orderedCache.pollLast();</span><br><span class="line">				// the dropped element is now only in the store</span><br><span class="line">				allElementsInCache = false;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			// 用来判重</span><br><span class="line">			if (orderedCache.add(toAddBytes)) &#123;</span><br><span class="line">				// write-through sync</span><br><span class="line">				addToRocksDB(toAddBytes);</span><br><span class="line">				if (toAddBytes == orderedCache.peekFirst()) &#123;</span><br><span class="line">					// 说明新的写入导致了堆顶变化</span><br><span class="line">					peekCache = null;</span><br><span class="line">					return true;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			// 如果cache满了，或者不是所有的元素都在cache中，说明新来的数据一定不是堆顶的数据</span><br><span class="line">			// we only added to the store</span><br><span class="line">			addToRocksDB(toAddBytes);</span><br><span class="line">			allElementsInCache = false;</span><br><span class="line">		&#125;</span><br><span class="line">		return false;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">public boolean remove(@Nonnull E toRemove) &#123;</span><br><span class="line"></span><br><span class="line">		checkRefillCacheFromStore();</span><br><span class="line"></span><br><span class="line">		final byte[] oldHead = orderedCache.peekFirst();</span><br><span class="line"></span><br><span class="line">		if (oldHead == null) &#123;</span><br><span class="line">			return false;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		final byte[] toRemoveBytes = serializeElement(toRemove);</span><br><span class="line"></span><br><span class="line">		// write-through sync</span><br><span class="line">		removeFromRocksDB(toRemoveBytes);</span><br><span class="line">		orderedCache.remove(toRemoveBytes);</span><br><span class="line"></span><br><span class="line">		if (orderedCache.isEmpty()) &#123;</span><br><span class="line">			seekHint = toRemoveBytes;</span><br><span class="line">			peekCache = null;</span><br><span class="line">			return true;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if (oldHead != orderedCache.peekFirst()) &#123;</span><br><span class="line">			peekCache = null;</span><br><span class="line">			return true;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		return false;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p>Iterator中seekHint的作用：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">private RocksBytesIterator(@Nonnull RocksIteratorWrapper iterator) &#123;</span><br><span class="line">			this.iterator = iterator;</span><br><span class="line">			try &#123;</span><br><span class="line">				// We use our knowledge about the lower bound to issue a seek that is as close to the first element in</span><br><span class="line">				// the key-group as possible, i.e. we generate the next possible key after seekHint by appending one</span><br><span class="line">				// zero-byte.</span><br><span class="line">				iterator.seek(Arrays.copyOf(seekHint, seekHint.length + 1));</span><br><span class="line">				currentElement = nextElementIfAvailable();</span><br><span class="line">			&#125; catch (Exception ex) &#123;</span><br><span class="line">				// ensure resource cleanup also in the face of (runtime) exceptions in the constructor.</span><br><span class="line">				iterator.close();</span><br><span class="line">				throw new FlinkRuntimeException(&quot;Could not initialize ordered iterator.&quot;, ex);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure><p></p><h4>再说checkpoint</h4><p>在doc中作者也提到之所以要做这个feature除了因为timer过多会导致OOM等问题，还有一个原因是因为timer的属性虽然和keyed state很类似，但是代码管理以及checkpoint的方式都是单独的一块逻辑，并且checkpoint的持久化过程还是同步的（因为是以raw keyedstate的方式去进行的），再修改之后，每次注册的timeservice都会注册到<code>kvstatInfo</code>中，将checkpoint的逻辑统一到statebackend中并且实现了异步化。</p><p><a href="https://blog.csdn.net/u010224394/article/details/8834969" target="_blank" rel="noopener">https://blog.csdn.net/u010224394/article/details/8834969</a></p><p><a href="https://github.com/apache/flink/pull/6159" target="_blank" rel="noopener">https://github.com/apache/flink/pull/6159</a></p><p><a href="https://docs.google.com/document/d/1XbhJRbig5c5Ftd77d0mKND1bePyTC26Pz04EvxdA7Jc/edit" target="_blank" rel="noopener">https://docs.google.com/document/d/1XbhJRbig5c5Ftd77d0mKND1bePyTC26Pz04EvxdA7Jc/edit</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;[toc]&lt;/p&gt;&lt;p&gt;接上文分析，要将timer改成基于rocksdb，其实就是要对存储timer的set和queue提供基于rocksdb的存储方
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>上篇·flink基于rocksdb的timerService</title>
    <link href="http://www.aitozi.com/flink-timerservice-based-on-rocksdb.html"/>
    <id>http://www.aitozi.com/flink-timerservice-based-on-rocksdb.html</id>
    <published>2019-03-16T04:08:03.000Z</published>
    <updated>2019-03-16T04:36:28.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>[toc]</p><p>本文主要介绍flink中<code>TimerService based on Rocksdb</code>实现以及和之前版本的一个比较。</p><p>&lt;!--more--&gt;</p><p><strong>动机</strong></p><ol><li>timer和keyed state分开单独管理，keyed state是由<code>KeyedStateBackend</code>管理，而timer是由<code>InternalTimerServeice</code>管理</li><li><code>InternalTimerServeice</code>现有的实现是基于heap的<code>HeapInternalTimerService</code>，当timer数量较多时会有OOM的问题.如果能像keyed state一样基于<code>KeyedStateBackend</code>管理，就能在timer数量比较多的时候选用rocksdb作为backend来解决扩展性的问题</li><li>timer目前的checkpoint过程是通过raw keyed state的方式，在同步的过程中完成写出到外置存储，并且对于snapshot和restore timer都单独维护了一份代码。这块代码和其他的keyed state的实现有很多相同之处（分隔到keygroup来实现rescale，元数据的序列化和持久化..）</li></ol><p><strong>实现目标</strong></p><ol><li>Have an implementation of timer services that operates on RocksDB.</li><li>Support asynchronous snapshots for all timer state.</li><li>Support incremental snapshots for timer state in RocksDB.</li><li>Integrate timer state as another form of keyed state in keyed state backends in a way that leverages the existing snapshotting code to eliminate special casing code paths that do similar things. As as nice side effect, this would also free the raw keyed state for user state.</li></ol><p><strong>源码分析</strong></p><p>首先我们来看一下1.4版本中timerService是怎么实现的,timerService 实现了以下两个接口：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">InternalTimerService</span><br><span class="line">    long currentProcessingTime();</span><br><span class="line">    long currentWatermark();</span><br><span class="line">    void registerProcessingTimeTimer(N namespace, long time);</span><br><span class="line">    void deleteProcessingTimeTimer(N namespace, long time);</span><br><span class="line">    void registerEventTimeTimer(N namespace, long time);</span><br><span class="line">    void deleteEventTimeTimer(N namespace, long time);</span><br><span class="line">提供的是当前时间获取和注册timer的方法</span><br><span class="line"></span><br><span class="line">ProcessingTimeCallback</span><br><span class="line">    void onProcessingTime(long timestamp) throws Exception;</span><br></pre></td></tr></table></figure><p></p><p>在<code>HeapInternalTimerService</code>中分别维护了processing time和event time的timer集合</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">private final Set&lt;InternalTimer&lt;K, N&gt;&gt;[] processingTimeTimersByKeyGroup;</span><br><span class="line">private final PriorityQueue&lt;InternalTimer&lt;K, N&gt;&gt; processingTimeTimersQueue;</span><br><span class="line"></span><br><span class="line">private final Set&lt;InternalTimer&lt;K, N&gt;&gt;[] eventTimeTimersByKeyGroup;</span><br><span class="line">private final PriorityQueue&lt;InternalTimer&lt;K, N&gt;&gt; eventTimeTimersQueue;</span><br></pre></td></tr></table></figure><p></p><p><code>InternalTimer</code>是一个<code>Comparable</code>, 按照timer触发的时间进行比较，timer是由<code>key</code>,<code>namespace</code>,<code>timestamp</code>唯一确定，其实也可以理解成如果同一个key，namespace下只可以有一个时间事件被触发。<code>regsterTimer</code>实际上就是在executor中注册一个任务</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">return timerService.schedule(</span><br><span class="line">                    new TriggerTask(status, task, checkpointLock, target, timestamp), delay, TimeUnit.MILLISECONDS);</span><br></pre></td></tr></table></figure><p></p><p>同时每个timerService中还维护了一个<code>ProcessingTimeService</code>用以处理和processing time相关的时间操作，是对<code>ScheduledThreadPoolExecutor</code>的包装，提供一些周期性执行和将来某时执行一次的操作.</p><p>我们在回头看<code>HeapInternalTimerService</code>的实现：</p><h4>registerProcessingTimeTimer</h4><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">public void registerProcessingTimeTimer(N namespace, long time) &#123;</span><br><span class="line">    InternalTimer&lt;K, N&gt; timer = new InternalTimer&lt;&gt;(time, (K) keyContext.getCurrentKey(), namespace);</span><br><span class="line"></span><br><span class="line">    // make sure we only put one timer per key into the queue</span><br><span class="line">    // 在存储timer的时候会存储两份，一份是通过Set的形式将各个Keygroup的区分开，另一份是按照时间顺序排除存储在一个`PriorityQueue`中</span><br><span class="line">    Set&lt;InternalTimer&lt;K, N&gt;&gt; timerSet = getProcessingTimeTimerSetForTimer(timer);</span><br><span class="line">    if (timerSet.add(timer)) &#123;</span><br><span class="line"></span><br><span class="line">        InternalTimer&lt;K, N&gt; oldHead = processingTimeTimersQueue.peek();</span><br><span class="line">        long nextTriggerTime = oldHead != null ? oldHead.getTimestamp() : Long.MAX_VALUE;</span><br><span class="line"></span><br><span class="line">        processingTimeTimersQueue.add(timer);</span><br><span class="line"></span><br><span class="line">        // check if we need to re-schedule our timer to earlier</span><br><span class="line">        // 如果新注册的timer比最近要触发的timer时间早，那么就会终止最近要触发的timer（如果已经跑起来了就不中断了）</span><br><span class="line">        if (time &lt; nextTriggerTime) &#123;</span><br><span class="line">            if (nextTimer != null) &#123;</span><br><span class="line">                nextTimer.cancel(false);</span><br><span class="line">            &#125;</span><br><span class="line">            // 通过ScheduledThreadPoolExecutor注册一个task</span><br><span class="line">            nextTimer = processingTimeService.registerTimer(time, this);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 获取这个timer的key所属的已经注册的timer列表，从上面的注释我们可以看出是为了保证不注册重复timer</span><br><span class="line">private Set&lt;InternalTimer&lt;K, N&gt;&gt; getProcessingTimeTimerSetForTimer(InternalTimer&lt;K, N&gt; timer) &#123;</span><br><span class="line">    checkArgument(localKeyGroupRange != null, &quot;The operator has not been initialized.&quot;);</span><br><span class="line">    int keyGroupIdx = KeyGroupRangeAssignment.assignToKeyGroup(timer.getKey(), this.totalKeyGroups);</span><br><span class="line">    return getProcessingTimeTimerSetForKeyGroup(keyGroupIdx);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private Set&lt;InternalTimer&lt;K, N&gt;&gt; getProcessingTimeTimerSetForKeyGroup(int keyGroupIdx) &#123;</span><br><span class="line">    int localIdx = getIndexForKeyGroup(keyGroupIdx);</span><br><span class="line">    Set&lt;InternalTimer&lt;K, N&gt;&gt; timers = processingTimeTimersByKeyGroup[localIdx];</span><br><span class="line">    // 如过这个set没有出现过，就构建一个新的set存放这个key的timer</span><br><span class="line">    if (timers == null) &#123;</span><br><span class="line">        timers = new HashSet&lt;&gt;();</span><br><span class="line">        processingTimeTimersByKeyGroup[localIdx] = timers;</span><br><span class="line">    &#125;</span><br><span class="line">    return timers;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private int getIndexForKeyGroup(int keyGroupIdx) &#123;</span><br><span class="line">    checkArgument(localKeyGroupRange.contains(keyGroupIdx),</span><br><span class="line">        &quot;Key Group &quot; + keyGroupIdx + &quot; does not belong to the local range.&quot;);</span><br><span class="line">    return keyGroupIdx - this.localKeyGroupRangeStartIdx;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4>registerEventTimeTimer</h4><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public void registerEventTimeTimer(N namespace, long time) &#123;</span><br><span class="line">    InternalTimer&lt;K, N&gt; timer = new InternalTimer&lt;&gt;(time, (K) keyContext.getCurrentKey(), namespace);</span><br><span class="line">    Set&lt;InternalTimer&lt;K, N&gt;&gt; timerSet = getEventTimeTimerSetForTimer(timer);</span><br><span class="line">    if (timerSet.add(timer)) &#123;</span><br><span class="line">        eventTimeTimersQueue.add(timer);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>可以看到eventtimer注册就不需要校验是否有将要执行的任务，因为eventtimer的实现不依赖于schedulerExxecutor。</p><h4>onProcessingTime</h4><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">// 这个方法是配合registerProcessingTimer，通过SystemProcessingTimeService来实现timer语义，在timer中注册的任务会回调这个onProcessing方法</span><br><span class="line">public void onProcessingTime(long time) throws Exception &#123;</span><br><span class="line">    // null out the timer in case the Triggerable calls registerProcessingTimeTimer()</span><br><span class="line">    // inside the callback.</span><br><span class="line">    // 如果不置为null，在执行triggerTarget.onProcessingTime(timer);里面执行了registerProcessingTimeTimer会调用本任务的cancel</span><br><span class="line">    nextTimer = null;</span><br><span class="line"></span><br><span class="line">    InternalTimer&lt;K, N&gt; timer;</span><br><span class="line"></span><br><span class="line">    // 将processingTimeTimersQueue中所有小于当前时间的任务都取出进行出发</span><br><span class="line">    while ((timer = processingTimeTimersQueue.peek()) != null &amp;&amp; timer.getTimestamp() &lt;= time) &#123;</span><br><span class="line"></span><br><span class="line">        // 删除set中存储的timer</span><br><span class="line">        Set&lt;InternalTimer&lt;K, N&gt;&gt; timerSet = getProcessingTimeTimerSetForTimer(timer);</span><br><span class="line"></span><br><span class="line">        timerSet.remove(timer);</span><br><span class="line">        processingTimeTimersQueue.remove();</span><br><span class="line"></span><br><span class="line">        // 每次触发之前需要设置当前的key</span><br><span class="line">        keyContext.setCurrentKey(timer.getKey());</span><br><span class="line">        triggerTarget.onProcessingTime(timer);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 说明队列中还存在还没到时间需要触发的timer，需要注册新的FutureTask</span><br><span class="line">    if (timer != null) &#123;</span><br><span class="line">        if (nextTimer == null) &#123;</span><br><span class="line">            nextTimer = processingTimeService.registerTimer(timer.getTimestamp(), this);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4>advanceWatermark</h4><p>processing timer是基于executor来实现的，eventtime 的timer触发就依赖于watermark来触发，每次收到上游的watermark会触发调用<code>advanceWatermark</code>来将eventtime queue中的timer取出进行触发</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public void processWatermark(Watermark mark) throws Exception &#123;</span><br><span class="line">    if (timeServiceManager != null) &#123;</span><br><span class="line">        timeServiceManager.advanceWatermark(mark);</span><br><span class="line">    &#125;</span><br><span class="line">    output.emitWatermark(mark);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// 这里的time就是最近的这次watermark的时间</span><br><span class="line">public void advanceWatermark(long time) throws Exception &#123;</span><br><span class="line">    currentWatermark = time;</span><br><span class="line"></span><br><span class="line">    InternalTimer&lt;K, N&gt; timer;</span><br><span class="line"></span><br><span class="line">    // 同样是取出所有的小于watermark的timer进行触发</span><br><span class="line">    while ((timer = eventTimeTimersQueue.peek()) != null &amp;&amp; timer.getTimestamp() &lt;= time) &#123;</span><br><span class="line"></span><br><span class="line">        Set&lt;InternalTimer&lt;K, N&gt;&gt; timerSet = getEventTimeTimerSetForTimer(timer);</span><br><span class="line">        timerSet.remove(timer);</span><br><span class="line">        eventTimeTimersQueue.remove();</span><br><span class="line"></span><br><span class="line">        keyContext.setCurrentKey(timer.getKey());</span><br><span class="line">        triggerTarget.onEventTime(timer);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4>snapshot</h4><p>之前在分析state实现的时候也分析过，在对operator进行snapshot的时候有一步就是对timerservice的数据进行snapshot</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">KeyGroupsList allKeyGroups = out.getKeyGroupList();</span><br><span class="line">for (int keyGroupIdx : allKeyGroups) &#123;</span><br><span class="line">    out.startNewKeyGroup(keyGroupIdx);</span><br><span class="line"></span><br><span class="line">    timeServiceManager.snapshotStateForKeyGroup(</span><br><span class="line">        new DataOutputViewStreamWrapper(out), keyGroupIdx);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">public void snapshotStateForKeyGroup(DataOutputView stream, int keyGroupIdx) throws IOException &#123;</span><br><span class="line">    InternalTimerServiceSerializationProxy&lt;K, N&gt; serializationProxy =</span><br><span class="line">        new InternalTimerServiceSerializationProxy&lt;&gt;(timerServices, keyGroupIdx);</span><br><span class="line"></span><br><span class="line">    serializationProxy.write(stream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>proxy这块主要是为兼容做了很多的工作</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public void write(DataOutputView out) throws IOException &#123;</span><br><span class="line">    super.write(out);</span><br><span class="line"></span><br><span class="line">    out.writeInt(timerServices.size());</span><br><span class="line">    for (Map.Entry&lt;String, HeapInternalTimerService&lt;K, N&gt;&gt; entry : timerServices.entrySet()) &#123;</span><br><span class="line">        String serviceName = entry.getKey();</span><br><span class="line">        HeapInternalTimerService&lt;K, N&gt; timerService = entry.getValue();</span><br><span class="line"></span><br><span class="line">        out.writeUTF(serviceName);</span><br><span class="line">        InternalTimersSnapshotReaderWriters</span><br><span class="line">            .getWriterForVersion(VERSION, timerService.snapshotTimersForKeyGroup(keyGroupIdx))</span><br><span class="line">            .writeTimersSnapshot(out);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4>restore</h4><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">protected void read(DataInputView in, boolean wasVersioned) throws IOException &#123;</span><br><span class="line">    int noOfTimerServices = in.readInt();</span><br><span class="line"></span><br><span class="line">    for (int i = 0; i &lt; noOfTimerServices; i++) &#123;</span><br><span class="line">        String serviceName = in.readUTF();</span><br><span class="line"></span><br><span class="line">        HeapInternalTimerService&lt;K, N&gt; timerService = timerServices.get(serviceName);</span><br><span class="line">        if (timerService == null) &#123;</span><br><span class="line">            timerService = new HeapInternalTimerService&lt;&gt;(</span><br><span class="line">                totalKeyGroups,</span><br><span class="line">                localKeyGroupRange,</span><br><span class="line">                keyContext,</span><br><span class="line">                processingTimeService);</span><br><span class="line">            timerServices.put(serviceName, timerService);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        int readerVersion = wasVersioned ? getReadVersion() : InternalTimersSnapshotReaderWriters.NO_VERSION;</span><br><span class="line">        InternalTimersSnapshot&lt;?, ?&gt; restoredTimersSnapshot = InternalTimersSnapshotReaderWriters</span><br><span class="line">            .getReaderForVersion(readerVersion, userCodeClassLoader)</span><br><span class="line">            .readTimersSnapshot(in);</span><br><span class="line"></span><br><span class="line">        timerService.restoreTimersForKeyGroup(restoredTimersSnapshot, keyGroupIdx);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p></p><p>本文主要是对1.4版本的分析，下一篇文章基于1.7版本再分析<code>timerservice on rocksdb</code>的实现</p><p>参考:</p><p><a href="https://docs.google.com/document/d/1XbhJRbig5c5Ftd77d0mKND1bePyTC26Pz04EvxdA7Jc/edit#heading=h.17v0k3363r6q" target="_blank" rel="noopener">https://docs.google.com/document/d/1XbhJRbig5c5Ftd77d0mKND1bePyTC26Pz04EvxdA7Jc/edit#heading=h.17v0k3363r6q</a></p><p><a href="https://issues.apache.org/jira/browse/FLINK-9485" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/FLINK-9485</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;[toc]&lt;/p&gt;&lt;p&gt;本文主要介绍flink中&lt;code&gt;TimerService based on Rocksdb&lt;/code&gt;实现以及和之前版本
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink网络传输的前世今生</title>
    <link href="http://www.aitozi.com/flink-network-feature.html"/>
    <id>http://www.aitozi.com/flink-network-feature.html</id>
    <published>2019-03-16T03:57:03.000Z</published>
    <updated>2019-03-16T04:06:26.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>flink的网络传输在1.5版本进行了重构，本文就这个feature来对flink网络传输进行系统的源码分析</p><p>&lt;!--more--&gt;</p><h3>发送端</h3><p>首先我们先来看数据发送端的主要流程如下：</p><p><strong>数据发送链路</strong></p><blockquote><p><code>RecordWriter =&gt; ResultPartition =&gt; ResultSubPartition =&gt; ResultSubpartitionView =&gt; BufferAvailabilityListener =&gt; PartitionRequestQueue</code> 解释一下： 用户程序中调用<code>output.collect()</code>,首先会通过<code>RecordWriter</code>进行数据或者event的序列化。并且将其从堆内内存拷贝至堆外内存，然后添加至相应的<code>ResultPartition</code>中。<code>ResultPartition</code>根据数据<code>selectChannel</code>发送给下游的哪个<code>subIndex</code>,<code>BufferConsumer</code>就会被添加到相应的subpartition所维护的一个双端队列中。在某些条件下需要通过,在服务启动最开始注册上来的<code>ResultSubpartitionView</code>去通知消费端来进行消费buffer，view做的事情就是通过调用<code>BufferAvailableListener</code>的具体实现来进行通知事件通知。最终在netty端，通过<code>PartitionRequestQueue</code>进行最终的buffer发送。</p></blockquote><p>上面讲述了大体的流程，下面我们来结合代码来进行细节分析，下面代码可能会结合1.4和1.7两个版本来进行讲解</p><h4>RecordWriter</h4><h4>RecordSerializer</h4><p>在1.4版本中序列化器是和下游的并发度一一绑定的，这样会导致一个问题，比如发送下游是hash的分区模式的话，在上游的每一个并发度就会存储5MB的序列化后的缓存数据，当下游的并发较大的时候就会占据比较大的内存，带来一定的gc问题。序列化器会负责做这样几件事情：</p><ul><li>数据的序列化<ul><li>在写数据的时候并不会校验缓存块的大小</li><li>写的时候同时用一个4字节的bytebuffer记录数据的大小，有多少个字节</li></ul></li><li>数据序列化结果的拷贝，对拷贝结果的判断<ul><li>数据拷贝了一部分，memorysegment已经满了</li><li>拷贝了完整记录</li><li>拷贝了完整记录，并且segment满了</li></ul></li><li>缓存清理</li><li>...</li></ul><p>重点拷贝过程</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">private boolean copyFromSerializerToTargetChannel(int targetChannel) throws IOException, InterruptedException &#123;</span><br><span class="line">	// We should reset the initial position of the intermediate serialization buffer before</span><br><span class="line">	// copying, so the serialization results can be copied to multiple target buffers.</span><br><span class="line">	// 这一步reset是为了在数据发送如果是broadcast这种一份数据需要发送多个下游通道的时候，就可以只序列化一次，后续数据发送的时候只需要将bytebuffer</span><br><span class="line">	// 的position值值置到0就可以了。</span><br><span class="line">	serializer.reset();</span><br><span class="line"></span><br><span class="line">	boolean pruneTriggered = false;</span><br><span class="line">	BufferBuilder bufferBuilder = getBufferBuilder(targetChannel);</span><br><span class="line">	SerializationResult result = serializer.copyToBufferBuilder(bufferBuilder);</span><br><span class="line">	// buffer没写满说明数据肯定已经写完了，直接进行下面的逻辑</span><br><span class="line">	while (result.isFullBuffer()) &#123;</span><br><span class="line">		// buffer写满了，首先将bufferBuilder标记为写完了，就是将positionMarker置为相反数</span><br><span class="line">		numBytesOut.inc(bufferBuilder.finish());</span><br><span class="line">		numBuffersOut.inc();</span><br><span class="line"></span><br><span class="line">		// If this was a full record, we are done. Not breaking out of the loop at this point</span><br><span class="line">		// will lead to another buffer request before breaking out (that would not be a</span><br><span class="line">		// problem per se, but it can lead to stalls in the pipeline).</span><br><span class="line">		// buffer写满，并且记录也写满了，那么发送到这个channel就完成了，否则就需要继续申请bufferBuilder继续拷贝</span><br><span class="line">		if (result.isFullRecord()) &#123;</span><br><span class="line">			pruneTriggered = true;</span><br><span class="line">			bufferBuilders[targetChannel] = Optional.empty();</span><br><span class="line">			break;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		bufferBuilder = requestNewBufferBuilder(targetChannel);</span><br><span class="line">		result = serializer.copyToBufferBuilder(bufferBuilder);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public SerializationResult copyToBufferBuilder(BufferBuilder targetBuffer) &#123;</span><br><span class="line">	targetBuffer.append(lengthBuffer);</span><br><span class="line">	targetBuffer.append(dataBuffer);</span><br><span class="line">	targetBuffer.commit();</span><br><span class="line"></span><br><span class="line">	return getSerializationResult(targetBuffer);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public int append(ByteBuffer source) &#123;</span><br><span class="line">	checkState(!isFinished());</span><br><span class="line"></span><br><span class="line">	int needed = source.remaining();</span><br><span class="line">	int available = getMaxCapacity() - positionMarker.getCached();</span><br><span class="line">	// segment不一定足够大，可能存不下这批buffer, 堆外内存拷贝的时候需要提前计算好可以拷贝的量，否则会有异常</span><br><span class="line">	int toCopy = Math.min(needed, available);</span><br><span class="line"></span><br><span class="line">	// 将source buffer中的数据/堆内存，put至memorySegment中，利用Unsafe进行数据拷贝</span><br><span class="line">	memorySegment.put(positionMarker.getCached(), source, toCopy);</span><br><span class="line">	// 设置新的position</span><br><span class="line">	positionMarker.move(toCopy);</span><br><span class="line">	return toCopy;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4>BufferBuilder，BufferConsumer，PositionMarker</h4><p>在上面copy代码中看到其实拷贝的时候是依赖buffer的,如果没有申请到<code>BufferBuiler</code>,是会一直blocking的，那么这个bufferbuilder是什么呢？</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">private BufferBuilder requestNewBufferBuilder(int targetChannel) throws IOException, InterruptedException &#123;</span><br><span class="line">	checkState(!bufferBuilders[targetChannel].isPresent() || bufferBuilders[targetChannel].get().isFinished());</span><br><span class="line"></span><br><span class="line">	BufferBuilder bufferBuilder = targetPartition.getBufferProvider().requestBufferBuilderBlocking();</span><br><span class="line">	bufferBuilders[targetChannel] = Optional.of(bufferBuilder);</span><br><span class="line">	// 一个bufferbuilder对应一个bufferconsumer</span><br><span class="line">	targetPartition.addBufferConsumer(bufferBuilder.createBufferConsumer(), targetChannel);</span><br><span class="line">	return bufferBuilder;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>在向<code>BufferProvider</code>，一般是localBufferPool申请完得到一个memorysegment后，将其封装成一个bufferbuilder，每一个bufferbuilder会对应 一个bufferconsumer和positionMarker，positionMarker会标记生产端的数据写到多少个字节了，这个在消费端的时候也会用到这个position， 由于是多线程使用所以position的值需要被标记成<code>volatile</code>来保证数据的可见性，每次消费端拉取数据的时候，对于没有写完的buffer同样可以进行消费， 消费前更新一个buffer的position真实位置，这里用到了一个小技巧，由于数据在生产的时候需要频繁的更新position，如果是<code>volatile</code>的， 虽然比较轻量，频繁更新也是比较大的开销，因此加入了一个<code>cachedPosition</code>，在写数据的时候只需要更新builder中的<code>cachedPosition</code>，生产端每次 完成一批的书写才会commit给<code>volatile position</code>，以此来减少缓存刷新。</p><p>从一个正在写的bufferbuiler中构建一个可消费的slice</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public Buffer build() &#123;</span><br><span class="line">	// 获取最近builder，commit到的position</span><br><span class="line">	writerPosition.update();</span><br><span class="line">	int cachedWriterPosition = writerPosition.getCached();</span><br><span class="line">	// slice 切分只读区块</span><br><span class="line">	Buffer slice = buffer.readOnlySlice(currentReaderPosition, cachedWriterPosition - currentReaderPosition);</span><br><span class="line">	currentReaderPosition = cachedWriterPosition;</span><br><span class="line">	// 增加引用计数</span><br><span class="line">	return slice.retainBuffer();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h4>PartitionRequestQueue</h4><p>在将bufferConsumer添加到subpartition的队列之后，同时会在partitionRequestQueue中维护一个availableReader的队列，这个队列表示可以往下 下游发送的buffer数据，这样通过一个<code>while true</code>循环持续的将队列中的数据往下游发送，当然这个<code>availableReader</code>队列的维护既考量了上游subpartition 有没有buffer的因素，也考量了下游要发送的receiver端的credit的情况，如果没有credit也是无法进入这个待发送队列的。</p><h3>消费端</h3><p><strong>数据接收链路</strong></p><blockquote><p><code>CreditBasedPartitionRequestClientHandler =&gt; RemoteInputChannel =&gt; SingleInputGate =&gt; BarrierHandler =&gt; StreamInputProcessor =&gt; StreamOperator</code> 首先会通过netty client进行数据的接收，然后从localbufferpool申请内存接收数据，然后根据backlog的信息去决定是不是要给上游分发credit，以及数据处理的流程</p></blockquote><p>这里主要分析下credit的判断逻辑</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Receives the backlog from the producer&apos;s buffer response. If the number of available</span><br><span class="line"> * buffers is less than backlog + initialCredit, it will request floating buffers from the buffer</span><br><span class="line"> * pool, and then notify unannounced credits to the producer.</span><br><span class="line"> *</span><br><span class="line"> * @param backlog The number of unsent buffers in the producer&apos;s sub partition.</span><br><span class="line"> */</span><br><span class="line">void onSenderBacklog(int backlog) throws IOException &#123;</span><br><span class="line">	int numRequestedBuffers = 0;</span><br><span class="line"></span><br><span class="line">	synchronized (bufferQueue) &#123;</span><br><span class="line">		// Similar to notifyBufferAvailable(), make sure that we never add a buffer</span><br><span class="line">		// after releaseAllResources() released all buffers (see above for details).</span><br><span class="line">		if (isReleased.get()) &#123;</span><br><span class="line">			return;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		numRequiredBuffers = backlog + initialCredit;</span><br><span class="line">		// 检查当前input通道的buffer是否做够上游produce所需要的buffer，如果不够就去bufferpool申请</span><br><span class="line">		while (bufferQueue.getAvailableBufferSize() &lt; numRequiredBuffers &amp;&amp; !isWaitingForFloatingBuffers) &#123;</span><br><span class="line">			Buffer buffer = inputGate.getBufferPool().requestBuffer();</span><br><span class="line">			if (buffer != null) &#123;</span><br><span class="line">				// 申请到buffer之后先占据住</span><br><span class="line">				bufferQueue.addFloatingBuffer(buffer);</span><br><span class="line">				numRequestedBuffers++;</span><br><span class="line">				//  没有足够的buffer，那么注册回调等buffer回收</span><br><span class="line">			&#125; else if (inputGate.getBufferProvider().addBufferListener(this)) &#123;</span><br><span class="line">				// If the channel has not got enough buffers, register it as listener to wait for more floating buffers.</span><br><span class="line">				isWaitingForFloatingBuffers = true;</span><br><span class="line">				break;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	// 如果生产端有buffer需求，并且之前的unannouncedCredit为0那么就需要通知上游有buffer了</span><br><span class="line">	if (numRequestedBuffers &gt; 0 &amp;&amp; unannouncedCredit.getAndAdd(numRequestedBuffers) == 0) &#123;</span><br><span class="line">		notifyCreditAvailable();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>整理流程图</h3><p><img src="https://raw.githubusercontent.com/Aitozi/images/master/flink/flink%E7%BD%91%E7%BB%9C%E6%A0%88%E5%9B%BE%E8%A7%A3.png" alt="flink-network" title="flink-network"></p><h3>netty内存的优化</h3><p>以下是message encode的时候的一段代码</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// only allocate header buffer - we will combine it with the data buffer below</span><br><span class="line">headerBuf = allocateBuffer(allocator, ID, messageHeaderLength, buffer.readableBytes(), false);</span><br><span class="line"></span><br><span class="line">receiverId.writeTo(headerBuf);</span><br><span class="line">headerBuf.writeInt(sequenceNumber);</span><br><span class="line">headerBuf.writeInt(backlog);</span><br><span class="line">headerBuf.writeBoolean(isBuffer);</span><br><span class="line">headerBuf.writeInt(buffer.readableBytes());</span><br><span class="line"></span><br><span class="line">CompositeByteBuf composityBuf = allocator.compositeDirectBuffer();</span><br><span class="line">composityBuf.addComponent(headerBuf);</span><br><span class="line">composityBuf.addComponent(buffer);</span><br><span class="line">// update writer index since we have data written to the components:</span><br><span class="line">composityBuf.writerIndex(headerBuf.writerIndex() + buffer.writerIndex());</span><br><span class="line">return composityBuf;</span><br></pre></td></tr></table></figure><p></p><p>可以看到这里和以前版本不一样的地方就是不需要再去申请一块netty内存做一次拷贝，因为这里将buffer对象的实现直接改成了继承netty的ByteBuf类， 所以减少了一次netty申请directBuffer以及从堆外拷贝到netty directBuffer的开销。在buffer处理完由netty回收时会放回<code>localBufferPool</code>中</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">protected void deallocate() &#123;</span><br><span class="line">	recycler.recycle(memorySegment); // 在网络传输完内存释放的时候直接将segment回收到localbufferpool中</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>和flink1.4相比有了哪些改进</h3><p>https://docs.google.com/document/d/1chTOuOqe0sBsjldA_r-wXYeSIhU2zRGpUaTaik7QZ84</p><p>https://issues.apache.org/jira/browse/FLINK-7282?subTaskView=all</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;flink的网络传输在1.5版本进行了重构，本文就这个feature来对flink网络传输进行系统的源码分析&lt;/p&gt;&lt;p&gt;&amp;lt;!--more--&amp;
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
      <category term="Netty" scheme="http://www.aitozi.com/tags/Netty/"/>
    
  </entry>
  
  <entry>
    <title>git常用命令大全</title>
    <link href="http://www.aitozi.com/git-advance-tips-keeping-on-updating.html"/>
    <id>http://www.aitozi.com/git-advance-tips-keeping-on-updating.html</id>
    <published>2019-03-16T03:40:03.000Z</published>
    <updated>2019-03-16T03:52:16.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>主要是工作中常用的一些git命令和一些场景的使用方式</p><p>&lt;!--more--&gt;</p><h2>常见命令</h2><h3>checkout</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b release-1.7.2 origin/release-1.7.2  # 从远端仓库checkout出release-1.7.2分支</span><br><span class="line">git checkout -- filename # 回退某文件至修改前的状态，也可用于误删文件恢复</span><br><span class="line">git checkout 0c6ded6af7068ff9fa4505d81855a38fc9861871 filename # 将某文件回退至某个版本</span><br></pre></td></tr></table></figure><p></p><h3>commit</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit --amend   # 修改commit信息</span><br></pre></td></tr></table></figure><p></p><h3>stash</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git stash # 保存工作现场  没有commit的内容</span><br><span class="line">git stash list # 查看stash队列</span><br><span class="line">git stash apply stash@&#123;num&#125;  # 恢复对应的stash</span><br><span class="line">git stash pop # 应用并删除最上面的stash</span><br></pre></td></tr></table></figure><p></p><h3>push</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git push &lt;远程主机名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt;</span><br><span class="line">git push origin yarn_hdfs:yarn_and_hdfs_tools</span><br></pre></td></tr></table></figure><p></p><h3>tag</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git ls-remote --tags upstream # 查看远端的tag列表</span><br><span class="line">git fetch --all --tags --prune # 获取远程所有的tag，如果有origin和upstream两个，那么都会拉下来 https://stackoverflow.com/questions/35979642/what-is-git-tag-how-to-create-tags-how-to-checkout-git-remote-tags  有时远端仓库更新了tag就需要拉一次</span><br><span class="line">git tag --list # 列出所有的tag</span><br><span class="line">git checkout -b tset v0.1.0  # checkout到某tag</span><br></pre></td></tr></table></figure><p></p><h3>rebase</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git rebase -i commid # [当前commit,指定commitId) 左开右闭 # 修改commit信息，合并commit, 调整commit顺序，将一个类型的commit合并在一起</span><br><span class="line">git rebase -i HEAD~2</span><br></pre></td></tr></table></figure><p></p><h3>cherry-pick</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git cherry-pick A..B # 合并单个和多个</span><br><span class="line">git cherry-pick A</span><br></pre></td></tr></table></figure><p></p><h3>reset</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard &lt;sha1-commit-id&gt; # 直接删除到这个commitId</span><br><span class="line">git reset --soft</span><br></pre></td></tr></table></figure><p></p><h3>branch</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git branch -m &lt;old_name&gt; &lt;new_name&gt; # 重命名 branch 名称</span><br><span class="line">git branch -m &lt;new_name&gt; # 重命名 branch 名称</span><br></pre></td></tr></table></figure><p></p><h3>log</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git log --graph --oneline --decorate # 查看提交历史,https://segmentfault.com/a/1190000008039809</span><br><span class="line">git log --merges # merge 历史</span><br></pre></td></tr></table></figure><p></p><h2>常见操作方式</h2><h3>为仓库添加一个源</h3><p>例如在内部flink仓库添加一个社区的源用以合并代码</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git remote add upstream https://github.com/apache/flink.git</span><br><span class="line">git pull upstream master # 指定上游和分支拉取代码</span><br></pre></td></tr></table></figure><p></p><h3>设置新的分支与远程分支的对应关系</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git branch --set-upstream-to=origin/dev_1.3.2_minwenjun</span><br><span class="line">git branch --set-upstream release-1.2.0-100 origin/release-1.2.0-100</span><br></pre></td></tr></table></figure><p></p><h3>克隆单个分支的代码</h3><p>常用于review大的MR，单独拉取用户提交的一个分支</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone [url] -b [branch-name] --single-branch</span><br><span class="line">git clone https://github.com/sihuazhou/flink.git -b FLINK-9804 --single-branch</span><br></pre></td></tr></table></figure><p></p><h3>cherry-pick merge commit</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://stackoverflow.com/questions/9229301/git-cherry-pick-says-38c74d-is-a-merge-but-no-m-option-was-given</span><br><span class="line">git cherry-pick -m 1 fd9f578</span><br><span class="line">git show --pretty=raw fd48e1ab722c20c196adb3e68583ba0d046b9cad (merge commit)</span><br></pre></td></tr></table></figure><p></p><h3>将当前代码提交到另一个仓库</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin_repo_b git@server_ip:/path/repo_b.git</span><br><span class="line">git push origin_repo_b branch_a(要推的那个本地分支的名字)</span><br></pre></td></tr></table></figure><p></p><h3>git修改传输协议</h3><p>修改你本地的ssh remote url. 不用https协议，改用git协议</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git remote -v</span><br><span class="line">git remote set-url origin</span><br><span class="line"></span><br><span class="line">[minwenjun@bigdata-test04 flink-metric-analyse]$ git remote set-url origin git@github.com:minwenjun/flink-metric-analyse.git</span><br><span class="line">[minwenjun@bigdata-test04 flink-metric-analyse]$ git remote -v</span><br><span class="line">origin	git@ github.com:minwenjun/flink-metric-analyse.git (fetch)</span><br><span class="line">origin	git@github.com:minwenjun/flink-metric-analyse.git (push)</span><br></pre></td></tr></table></figure><p></p><p>参考资料:</p><p><a href="https://yuzhouwan.com/posts/30041/" target="_blank" rel="noopener">https://yuzhouwan.com/posts/30041/</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;主要是工作中常用的一些git命令和一些场景的使用方式&lt;/p&gt;&lt;p&gt;&amp;lt;!--more--&amp;gt;&lt;/p&gt;&lt;h2&gt;常见命令&lt;/h2&gt;&lt;h3&gt;chec
    
    </summary>
    
      <category term="编程工具" scheme="http://www.aitozi.com/categories/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Git" scheme="http://www.aitozi.com/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>maven java.lang.NoClassDefFoundError with provided scope</title>
    <link href="http://www.aitozi.com/maven-noclassdeffounderror.html"/>
    <id>http://www.aitozi.com/maven-noclassdeffounderror.html</id>
    <published>2019-03-15T15:12:03.000Z</published>
    <updated>2019-03-16T03:53:02.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>关于maven中执行类遇到的<code>java.lang.NoClassDefFoundError</code>的问题</p><p>&lt;!-- more --&gt;</p><p>昨天有个同事问我，在Flink某个包中加了一个类用来进行测试，结果运行就会报如下错误</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/flink/api/common/serialization/DeserializationSchema</span><br><span class="line">	at com.didi.flink.app.FlinkTableSinkTest.main(FlinkTableSinkTest.scala)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.flink.api.common.serialization.DeserializationSchema</span><br><span class="line">	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">	... 1 more</span><br></pre></td></tr></table></figure><p></p><p>排查后怀疑是pom中该jar的依赖scope是provided导致的,测试删除后就解决了。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">	&lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">	&lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt;</span><br><span class="line">	&lt;version&gt;$&#123;project.version&#125;&lt;/version&gt;</span><br><span class="line">	&lt;scope&gt;provided&lt;/scope&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p></p><p>但是觉得这样会很麻烦，所有其他用到的类都需要去除provided了？顺手Google了一下，Stack Overflow上有人问过同样的问题（这个人竟然是之前blink meetup上的flink+zeppelin的演讲者<em>章剑锋（简锋）</em>）</p><p><a href="https://stackoverflow.com/questions/30453269/maven-provided-dependency-will-cause-noclassdeffounderror-in-intellij" target="_blank" rel="noopener">https://stackoverflow.com/questions/30453269/maven-provided-dependency-will-cause-noclassdeffounderror-in-intellij</a></p><p>简单的说是因为provided的scope只在编译期和test期间有效，所以正确的姿势应该是测试类就放在测试包下面测试，这样provided的包依然是有效的</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;关于maven中执行类遇到的&lt;code&gt;java.lang.NoClassDefFoundError&lt;/code&gt;的问题&lt;/p&gt;&lt;p&gt;&amp;lt;!-- 
    
    </summary>
    
      <category term="问题排查" scheme="http://www.aitozi.com/categories/%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"/>
    
    
      <category term="Maven" scheme="http://www.aitozi.com/tags/Maven/"/>
    
  </entry>
  
  <entry>
    <title>to be or not to be in 2019</title>
    <link href="http://www.aitozi.com/2019-flag.html"/>
    <id>http://www.aitozi.com/2019-flag.html</id>
    <published>2019-03-14T17:25:03.000Z</published>
    <updated>2019-03-14T17:25:37.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>2019年的flag</p><p>&lt;!-- more --&gt;</p><p>2019年的Q1快接近尾声了，是时候给今年来一个flag了，今年有以下3个目标：</p><ol><li>跑步500公里</li><li>读书10本+ <em>技术书籍不低于3本</em></li><li>flink，netty，hbase系列的源码分析博客及仓库更新</li><li>机器学习简单入门</li><li>简单学会尤克里里的弹奏</li><li>努力工作，攒钱</li></ol><p>截止时间:</p><p>-----------------------------------2020.01.01-----------------------------------------------</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;2019年的flag&lt;/p&gt;&lt;p&gt;&amp;lt;!-- more --&amp;gt;&lt;/p&gt;&lt;p&gt;2019年的Q1快接近尾声了，是时候给今年来一个flag了，今年
    
    </summary>
    
      <category term="杂七杂八" scheme="http://www.aitozi.com/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"/>
    
    
      <category term="flag" scheme="http://www.aitozi.com/tags/flag/"/>
    
  </entry>
  
  <entry>
    <title>String,StringBuffer,StringBuilder的区别</title>
    <link href="http://www.aitozi.com/string-stringbuilder-stringbuffer.html"/>
    <id>http://www.aitozi.com/string-stringbuilder-stringbuffer.html</id>
    <published>2018-08-27T16:13:48.000Z</published>
    <updated>2019-03-14T17:09:57.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>常用的jdk的组件的源码分析之：<code>String</code>,<code>StringBuffer</code>,<code>StringBuilder</code></p><p>&lt;!-- more --&gt;</p><p>String 字符串常量 StringBuffer 字符串变量（线程安全） StringBuilder 字符串变量（非线程安全）</p><p>String和StringBuffer的主要性能区别其实在于 String是不可变的对象, 因此在每次对String类型进行改变的时候其实都等同于生成了一个新的 String对象，然后将指针指向新的String对象，所以经常改变内容的字符串最好不要用String，因为每次生成对象都会对系统性能产生影响，特别当内存中无引用对象多了以后，JVM的GC就会开始工作，那速度是一定会相当慢的。</p><p>那么String为什么要是不可变的呢？</p><h3>String类不可变的好处</h3><ol><li>只有当字符串是不可变的，字符串池才有可能实现。字符串池的实现可以在运行时节约很多heap空间，因为不同的字符串变量都指向池中的同一个字符串。但如果字符串是可变的，那么String interning将不能实现，String interning是指对不同的字符串仅仅只保存一个，即不会保存多个相同的字符串。因为这样的话，如果变量改变了它的值，那么其它指向这个值的变量的值也会一起改变。</li><li>如果字符串是可变的，那么会引起很严重的安全问题。譬如，数据库的用户名、密码都是以字符串的形式传入来获得数据库的连接，或者在socket编程中，主机名和端口都是以字符串的形式传入。因为字符串是不可变的，所以它的值是不可改变的，否则黑客们可以钻到空子，改变字符串指向的对象的值，造成安全漏洞。</li><li>因为字符串是不可变的，所以是多线程安全的，同一个字符串实例可以被多个线程共享。这样便不用因为线程安全问题而使用同步。字符串自己便是线程安全的。</li><li>类加载器要用到字符串，不可变性提供了安全性，以便正确的类被加载。譬如你想加载java.sql.Connection类，而这个值被改成了myhacked.Connection，那么会对你的数据库造成不可知的破坏。</li><li>因为字符串是不可变的，所以在它创建的时候hashcode就被缓存了，不需要重新计算。这就使得字符串很适合作为Map中的键，字符串的处理速度要快过其它的键对象。这就是HashMap中的键往往都使用字符串。也同时指出一个理念，千万不要把可变类型作为HashMap和HashSet的键值</li></ol><h3>在java中如何设计不可变</h3><ol><li>对于属性不提供设值的方法</li><li>所有的属性定义为private final</li><li>类声明为final不允许继承</li><li>return deep cloned objects with copied content for all mutable fields in class</li></ol><p>翻看string的源码，可以看到string的本质是个char数组，并且使用final关键字修饰。但是char数组用final修饰只能让数组的引用地址不变，array数组还是可变的，主要是SUN的工程师没有暴露内部成员字段，所以String不可变主要在底层实现，而不是在final。</p><h3>String的内存存储</h3><p>一般而言，Java 对象在虚拟机的结构如下：</p><ul><li>对象头（object header）：8 个字节</li><li>Java 原始类型数据：如 int, float, char 等类型的数据，各类型数据占内存。<ul><li>boolean 1</li><li>byte</li><li>char 2</li><li>short</li><li>int 4</li><li>long 8</li></ul></li><li>引用（reference）：4 个字节</li><li>填充符（padding）</li></ul><p>然而，一个 Java 对象实际还会占用些额外的空间，如：对象的 class 信息、ID、在虚拟机中的状态。在 Oracle JDK 的 Hotspot 虚拟机中，一个普通的对象需要额外 8 个字节。</p><p>String对象的声明</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">private final char value[]; </span><br><span class="line">private final int offset; </span><br><span class="line">private final int count; </span><br><span class="line">private int hash;</span><br></pre></td></tr></table></figure><p></p><p>那么因该如何计算该 String 所占的空间？</p><p>首先计算一个空的 char 数组所占空间，在 Java 里数组也是对象，因而数组也有对象头，故一个数组所占的空间为对象头所占的空间加上数组长度，即 8 + 4 = 12 字节 , 经过填充后为 16 字节。</p><p>那么一个空 String 所占空间为：</p><p>对象头（8 字节）+ char 数组（16 字节）+ 3 个 int（3 × 4 = 12 字节）+1 个 char 数组的引用 (4 字节 ) = 40 字节。</p><p>因此一个实际的 String 所占空间的计算公式如下：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">8*( ( 8+2*n+4+12)+7 ) / 8 = 8*(int) ( ( ( (n) *2 )+43) /8 )</span><br></pre></td></tr></table></figure><p></p><p>在java中对String对象特殊对待，所以在heap上分为两块，一块是String constant pool存储java字符串常量，另一块存储普通对象和字符串对象，主要区别：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">String a = &quot;abc&quot;;</span><br><span class="line">String b = new String(&quot;acb&quot;)</span><br></pre></td></tr></table></figure><p></p><p>第一种jvm会先去查找constant pool是否存在此常量，不存在就在constant pool上进行创建，第二种是在堆上创建对象，并且不会加入到constant pool上，因此可能会带来字符串重复占用内存的问题。可以调用String.intern()加入到String constant pool中，其实是JVM heap 中 PermGen 相应的区域。</p><p>jdk1.6和1.7还有所不同，jdk1.7的常量池是在堆中的</p><h3>StringBuffer</h3><p>StringBuffer和String不同，每次修改都会对 StringBuffer 对象本身进行操作，而不是生成新的对象，再改变对象引用。所以在一般情况下我们推荐使用 StringBuffer ，特别是字符串对象经常改变的情况下。而在某些特别情况下， String 对象的字符串拼接其实是被JVM解释成了 StringBuffer 对象的拼接，所以这些时候 String 对象的速度并不会比 StringBuffer 对象慢，而特别是以下的字符串对象生成中， String效率是远要比 StringBuffer 快的：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">String S1 = “This is only a” + “ simple” + “ test”;</span><br><span class="line">StringBuffer Sb = new StringBuilder(“This is only a”).append(“ simple”).append(“ test”);</span><br></pre></td></tr></table></figure><p></p><p>你会很惊讶的发现，生成 String S1 对象的速度简直太快了，而这个时候 StringBuffer 居然速度上根本一点都不占优势。其实这是 JVM 的一个把戏，在 JVM 眼里，这个</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">String S1 = “This is only a” + “ simple” + “test”;</span><br><span class="line">其实就是：</span><br><span class="line">String S1 = “This is only a simple test”;</span><br></pre></td></tr></table></figure><p></p><p>当然不需要太多的时间了。但大家这里要注意的是，如果你的字符串是来自另外的 String 对象的话，速度就没那么快了，譬如：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">String S2 = “This is only a”;</span><br><span class="line">String S3 = “ simple”;</span><br><span class="line">String S4 = “ test”;</span><br><span class="line">String S1 = S2 +S3 + S4;</span><br></pre></td></tr></table></figure><p></p><p>这时候 JVM 会规规矩矩的按照原来的方式去做</p><p><a href="https://www.ibm.com/developerworks/cn/java/j-lo-optmizestring/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/java/j-lo-optmizestring/index.html</a></p><p><a href="https://blog.csdn.net/qq_36357995/article/details/79985538" target="_blank" rel="noopener">https://blog.csdn.net/qq_36357995/article/details/79985538</a></p><p><a href="https://segmentfault.com/a/1190000004261063" target="_blank" rel="noopener">https://segmentfault.com/a/1190000004261063</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;常用的jdk的组件的源码分析之：&lt;code&gt;String&lt;/code&gt;,&lt;code&gt;StringBuffer&lt;/code&gt;,&lt;code&gt;StringB
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="jdk" scheme="http://www.aitozi.com/tags/jdk/"/>
    
  </entry>
  
  <entry>
    <title>rocksdb概念简介</title>
    <link href="http://www.aitozi.com/rocksdb-wiki.html"/>
    <id>http://www.aitozi.com/rocksdb-wiki.html</id>
    <published>2018-08-23T15:20:44.000Z</published>
    <updated>2019-03-14T17:07:11.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>本文翻译自：https://github.com/facebook/rocksdb/wiki/rocksdb-basics</p><p>主要是rocksdb的一些概念理解和介绍</p><p>&lt;!-- more --&gt;</p><p>rocksdb主要组成部分memtable,sstfile,logfile，rocksdb 支持将database切分成多个columnFamily，所有的数据库创建如果没有指定的话会是一个default column 他支持批量原子写入，key和value都是纯byte流，key和value的大小都没有做限制</p><p>所有database中的数据都是以一个有序的形式被放置（怎么做的呢？后append的数据怎么有序），应用可以指定key的comparison方法，来指定key的排序方式，Iterator API可以在database做一个RangeScan操作，他会指向一个特定的key，然后进行一个一个遍历。在调用iterator的时候会创建database的即时视图，因此所有查询的key都是一致的。</p><h3>Snapshot</h3><p>Snapshot Api也支持创建database某一时间点的视图，Get和Iterator Api可以用以读取指定snapshot的数据，从某种意义上说，snapshot和iterator都会提供database的当前视图，但是他们的实现不同。iterator是短期的/前台线程的scan，而长期/后台的scan最好是通过snapshot。iterator会对所有底层与pint-in-time database视图相关的的文件保留一个引用计数，这些文件知道iterator结束之后才会被删除。然而snapshot不会阻碍文件的删除，取而代之的是在compaction的过程会意识到snapshots的存在，直接不会删除在已经存在于snapshot中的key。snapshot在database重启的会丢失，reload rocksdb library会释放所有的snapshot</p><h3>Prefix Iterators</h3><p>大多数基于LSM设计的存储引擎都不太能支持高效的RangeScan API，因为他需要查阅每个文件，但是真正的应用并不是纯粹的随机读取key，一般会以一个key-prefix去查询，rocksdb利用了这个特点做了一些优化。应用可以配置prefix_extractor来指定key-prefix，rocksdb决定，存储的blooms，iterator可以通过ReadOptios指定prefix，然后rocksDB将会使用这些bloom bits来避免查询那些不包含那些key-perfix开始的key的文件。</p><h3>Persistence</h3><p>rocksdb有一个事务日志，所有的puts操作会被存储在memtables中，同时也会可选的写入事务日志中，在重启的时候，会重新执行事务日志中的记录。事务日志可以配置和sst文件放在不同的目录。这是因为有时你并不想持久化数据文件，同时又可以将事务日志持久化到一个相对较慢的持久化存储中，来确保数据不会丢失。 每一个Put都有一个标志，通过WriteOptios来标志是否需要写入事务日志中，同时也可以配置是否需要同步等到数据已经被写入到事务日志完成之后才将Put操作标记为commit完成。</p><p>在内部实现中，RocksDB会使用batch-commit的机制去批量的将事务操作提交到事务日志中，所以在一次同步调用中会提交多个transactions</p><h3>Fault Tolerance</h3><p>Rocksdb使用checksum去检测存储是否有损坏</p><h3>Multi-Threaded Compactions</h3><p>compaction的存在是为了删除同一个key的多个副本，这种情况发生在用户更新了某个key的值，compaction也负责将要删除的key进行删除。整个database是存储在sstable中的，当memtable满了的时候会写入到Level-0（L0）的文件中，RocksDB在将数据从memtable flush到文件的时候会先将重复的key进行删除。然后一些文件会周期性的读入并形成更大的文件，这就是compaction的过程。</p><p>对于一个LSM的database的写入的吞吐量取决于compaction所能达到的速度，特别是数据存储在ssd或者RAM中。RocksDB可以配置为启用多个并发compaction线程。据观察，与单线程compaction相比，基于ssd的数据库的持续写入速率可能在多线程compaction的情况下增加10倍之多</p><h3>compaction Styles</h3><p>通常的style的compaction是完全基于排序的，运行与L0文件或者L1+. Compaction会挑选一些按时间顺序相邻文件，然后将其合并成一个新的sstable</p><p>level style compaction在数据库存储会分为多个等级，最近的数据存储在L0层，最老的数据存储在Lmax层，只有L0层会存在重叠的key，一次compaction会将Ln的file和Ln+1的file做compaction然后形成新的文件替换Ln+1的文件，Universal Style 和Level style相比通常会有较低的写入放大但是较高的磁盘占用和读放大</p><p>（写入放大）： https://www.zhihu.com/question/31024021 https://www.wikiwand.com/zh-hans/%E5%86%99%E5%85%A5%E6%94%BE%E5%A4%A7</p><p>同时RocksDB也支持用户自定义compaction方式，可以通过<code>Options.disable_auto_compaction</code>关闭原生的compaction算法，同时<code>GetLiveFilesMetaData</code>接口可以让外置组件查看每一个database中的数据文件从而决定哪些数据需要merge和合并。通过调用<code>CompactFiles</code>来进行文件的合并，<code>DeleteFile</code>来进行文件的删除</p><h3>metadata storage</h3><p>数据库中的MANIFEST文件记录了数据库的状态，compaction线程会新增新文件，删除旧文件，这些操作会通过 MANIFEST记录来持久化，同样记录操作也是用了batch-commit的算法来缓冲重复的对MANIFEST文件的同步写入</p><h3>Avoiding Stalls</h3><p>后台的compaction线程会将memtable中的内容flush到文件中，如果所有的后台线程都忙着做长时间的compaction，那么突然一个大流量的写入可能就会把memtable写满，这就会导致新的写入被hung住，这个问题可以通过配置rocksdb保持特定的几个线程专门保留来进行flush操作</p><h3>Compaction Filter</h3><p>一些应用可能在compaction的时候可能期望对key做出一些处理，比如数据库内部实现可能需要支持TTL，来删除过期的key，这可以通过自定义实现compaction filter来实现。他提供了用户在compaction的过程中修改key value以及丢弃这个key的数据的能力</p><h3>Read only mode</h3><p>database可以以read only的模式打开这样数据库保证所有的数据都不可修改，同时也会极大的提升读性能，因为完全了避免了锁</p><h3>Full Backups, Incremental Backups and Replication</h3><p>RocksDB支持增量的备份，BackupableDB使得Rocksdb的备份很简单，后续会深入介绍</p><p>增量的复制需要能够找到数据库最近的改变，GetUpdatesSince API支持应用tail最近的事务日志，因此他能够连续的获取事务日志，然后将其应用于远程的复制或者备份</p><p>未完待续</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;本文翻译自：https://github.com/facebook/rocksdb/wiki/rocksdb-basics&lt;/p&gt;&lt;p&gt;主要是rock
    
    </summary>
    
      <category term="刨根问底" scheme="http://www.aitozi.com/categories/%E5%88%A8%E6%A0%B9%E9%97%AE%E5%BA%95/"/>
    
    
      <category term="RocksDB" scheme="http://www.aitozi.com/tags/RocksDB/"/>
    
  </entry>
  
  <entry>
    <title>flink中状态实现的深入理解</title>
    <link href="http://www.aitozi.com/flink-state.html"/>
    <id>http://www.aitozi.com/flink-state.html</id>
    <published>2018-08-04T08:35:29.000Z</published>
    <updated>2018-08-05T04:42:13.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>本文是源于要在内部分享，所以提前整理了一些flink中的状态的一些知识，flink状态所包含的东西很多，在下面列举了一些，还有一 些在本文没有体现，后续会单独的挑出来再进行讲解</p><p>&lt;!-- more --&gt;</p><ul><li>state的层次结构</li><li>keyedState =&gt; windowState</li><li>OperatorState =&gt; kafkaOffset</li><li>stateBackend</li><li>snapshot/restore</li><li><em>internalTimerService</em></li><li><strong>RocksDB操作的初探</strong></li><li><em>state ttL</em></li><li><em>state local recovery</em></li><li><strong>QueryableState</strong></li><li><strong>increamental checkpoint</strong></li><li>state redistribution</li><li><em>broadcasting state</em></li><li><strong>CheckpointStreamFactory</strong></li></ul><hr><h3>内部和外部状态</h3><p>flink状态分为了内部和外部使用接口，但是两个层级都是一一对应，内部接口都实现了外部接口，主要是有两个目的</p><ul><li>内部接口提供了更多的方法，包括获取state中的serialize之后的byte，以及Namespace的操作方法。内部状态主要用于内部runtime实现时所需要用到的一些状态比如window中的windowState，CEP中的sharedBuffer,kafkaConsumer中offset管理的ListState,而外部State接口主要是用户自定义使用的一些状态</li><li>考虑到各个版本的兼容性，外部接口要保障跨版本之间的兼容问题，而内部接口就很少受到这个限制，因此也就比较灵活</li></ul><p>层次结构图：</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-8-2/82936981.jpg" alt></p><h3>状态的使用</h3><p>了解了flink 状态的层次结构，那么编程中和flink内部是如何使用这些状态呢？</p><p>flink中使用状态主要是两部分，一部分是函数中使用状态，另一部分是在operator中使用状态</p><p>方式：</p><ul><li>CheckpointedFunction</li><li>ListCheckpointed</li><li>RuntimeContext （DefaultKeyedStateStore）</li><li>StateContext</li></ul><p>StateContext</p><p>StateInitializationContext</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Iterable&lt;StatePartitionStreamProvider&gt; getRawOperatorStateInputs();</span><br><span class="line"></span><br><span class="line">Iterable&lt;KeyGroupStatePartitionStreamProvider&gt; getRawKeyedStateInputs();</span><br></pre></td></tr></table></figure><p></p><p>ManagedInitializationContext</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">OperatorStateStore getOperatorStateStore();</span><br><span class="line">KeyedStateStore getKeyedStateStore();</span><br></pre></td></tr></table></figure><p></p><p>举例：</p><ol><li><p>AbstractStreamOperator封装了这个方法<code>initializeState(StateInitializationContext context)</code>用以在operator中进行raw和managed的状态管理</p></li><li><p>CheckpointedFunction的用法其实也是借助于StateContext进行相关实现</p></li></ol><p><code>CheckpointedFunction#initializeState</code>方法在transformation function的各个并发实例初始化的时候被调用这个方法提供了<code>FunctionInitializationContext</code>的对象，可以通过这个<code>context</code>来获取<code>OperatorStateStore</code>或者<code>KeyedStateStore</code>，也就是说通过这个接口可以注册这两种类型的State，这也是和ListCheckpointed接口不一样的地方，只是说<code>KeyedStateStore</code>只能在keyedstream上才能注册，否则就会报错而已,以下是一个使用这两种类型状态的样例。 可以参见<code>FlinkKafkaConsumerBase</code>通过这个接口来实现offset的管理。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">public class MyFunction&lt;T&gt; implements MapFunction&lt;T, T&gt;, CheckpointedFunction &#123;</span><br><span class="line"></span><br><span class="line">     private ReducingState&lt;Long&gt; countPerKey;</span><br><span class="line">     private ListState&lt;Long&gt; countPerPartition;</span><br><span class="line"></span><br><span class="line">     private long localCount;</span><br><span class="line"></span><br><span class="line">     public void initializeState(FunctionInitializationContext context) throws Exception &#123;</span><br><span class="line">         // get the state data structure for the per-key state</span><br><span class="line">         countPerKey = context.getKeyedStateStore().getReducingState(</span><br><span class="line">                 new ReducingStateDescriptor&lt;&gt;(&quot;perKeyCount&quot;, new AddFunction&lt;&gt;(), Long.class));</span><br><span class="line"></span><br><span class="line">         // get the state data structure for the per-partition state</span><br><span class="line">         countPerPartition = context.getOperatorStateStore().getOperatorState(</span><br><span class="line">                 new ListStateDescriptor&lt;&gt;(&quot;perPartitionCount&quot;, Long.class));</span><br><span class="line"></span><br><span class="line">         // initialize the &quot;local count variable&quot; based on the operator state</span><br><span class="line">         for (Long l : countPerPartition.get()) &#123;</span><br><span class="line">             localCount += l;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     public void snapshotState(FunctionSnapshotContext context) throws Exception &#123;</span><br><span class="line">         // the keyed state is always up to date anyways</span><br><span class="line">         // just bring the per-partition state in shape</span><br><span class="line">         countPerPartition.clear();</span><br><span class="line">         countPerPartition.add(localCount);</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     public T map(T value) throws Exception &#123;</span><br><span class="line">         // update the states</span><br><span class="line">         countPerKey.add(1L);</span><br><span class="line">         localCount++;</span><br><span class="line"></span><br><span class="line">         return value;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p></p><p>这个Context的继承接口StateSnapshotContext的方法则提供了raw state的存储方法，但是其实没有对用户函数提供相应的接口，只是在引擎中有相关的使用，相比较而言这个接口提供的方法，context比较多，也有一些简单的方法去注册使用operatorstate 和 keyedState。如通过<code>RuntimeContext</code>注册keyedState:</p><p>因此使用简易化程度为:</p><blockquote><p>RuntimeContext &gt; FunctionInitializationContext &gt; StateSnapshotContext</p></blockquote><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">keyedStream.map(new RichFlatMapFunction&lt;MyType, List&lt;MyType&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">     private ListState&lt;MyType&gt; state;</span><br><span class="line"></span><br><span class="line">     public void open(Configuration cfg) &#123;</span><br><span class="line">         state = getRuntimeContext().getListState(</span><br><span class="line">                 new ListStateDescriptor&lt;&gt;(&quot;myState&quot;, MyType.class));</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     public void flatMap(MyType value, Collector&lt;MyType&gt; out) &#123;</span><br><span class="line">         if (value.isDivider()) &#123;</span><br><span class="line">             for (MyType t : state.get()) &#123;</span><br><span class="line">                 out.collect(t);</span><br><span class="line">             &#125;</span><br><span class="line">         &#125; else &#123;</span><br><span class="line">             state.add(value);</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;);</span><br></pre></td></tr></table></figure><p></p><p>通过实现<code>ListCheckpointed</code>来注册OperatorState，但是这个有限制： 一个function只能注册一个state，因为并不能像其他接口一样指定state的名字.</p><p>example：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">public class CountingFunction&lt;T&gt; implements MapFunction&lt;T, Tuple2&lt;T, Long&gt;&gt;, ListCheckpointed&lt;Long&gt; &#123;</span><br><span class="line"></span><br><span class="line">     // this count is the number of elements in the parallel subtask</span><br><span class="line">     private long count;</span><br><span class="line"></span><br><span class="line">     &#123;@literal @&#125;Override</span><br><span class="line">     public List&lt;Long&gt; snapshotState(long checkpointId, long timestamp) &#123;</span><br><span class="line">         // return a single element - our count</span><br><span class="line">         return Collections.singletonList(count);</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     &#123;@literal @&#125;Override</span><br><span class="line">     public void restoreState(List&lt;Long&gt; state) throws Exception &#123;</span><br><span class="line">         // in case of scale in, this adds up counters from different original subtasks</span><br><span class="line">         // in case of scale out, list this may be empty</span><br><span class="line">         for (Long l : state) &#123;</span><br><span class="line">             count += l;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     &#123;@literal @&#125;Override</span><br><span class="line">     public Tuple2&lt;T, Long&gt; map(T value) &#123;</span><br><span class="line">         count++;</span><br><span class="line">         return new Tuple2&lt;&gt;(value, count);</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p></p><p>下面比较一下里面的两种stateStore</p><ul><li>KeyedStateStore</li><li>OperatorStateStore</li></ul><p>查看OperatorStateStore接口可以看到OperatorState只提供了ListState一种形式的状态接口,OperatorState和KeyedState主要有以下几个区别：</p><ul><li>keyedState只能应用于KeyedStream，而operatorState都可以</li><li>keyedState可以理解成一个算子为每个subtask的每个key维护了一个状态namespace，而OperatorState是每个subtask共享一个状态</li><li>operatorState只提供了ListState，而keyedState提供了<code>ValueState</code>,<code>ListState</code>,<code>ReducingState</code>,<code>MapState</code></li><li>operatorStateStore的默认实现只有<code>DefaultOperatorStateBackend</code>可以看到他的状态都是存储在堆内存之中，而keyedState根据backend配置的不同，线上都是存储在rocksdb之中</li></ul><h3>snapshot</h3><p>这个让我们着眼于两个Operator的snapshot，<code>AbstractStreamOperator</code> 和 <code>AbstractUdfStreamOperator</code>,这两个基类几乎涵盖了所有相关operator和function在做snapshot的时候会做的处理。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">if (null != operatorStateBackend) &#123;</span><br><span class="line">				snapshotInProgress.setOperatorStateManagedFuture(</span><br><span class="line">					operatorStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions));</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			if (null != keyedStateBackend) &#123;</span><br><span class="line">				snapshotInProgress.setKeyedStateManagedFuture(</span><br><span class="line">					keyedStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><ol><li>按keyGroup去snapshot各个timerService的状态，包括processingTimer和eventTimer（RawKeyedOperatorState）</li><li>将operatorStateBackend和keyedStateBackend中的状态做snapshot</li><li>如果Operator还包含了userFunction，即是一个<code>UdfStreamOperator</code>,那么可以注意到udfStreamOperator覆写了父类的<code>snapshotState(StateSnapshotContext context)</code>方法，其主要目的就是为了将Function中的状态及时的register到相应的backend中，在第二步的时候统一由<code>CheckpointStreamFactory</code>去做快照</li></ol><h4>StreamingFunctionUtils#snapshotFunctionState</h4><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">if (userFunction instanceof CheckpointedFunction) &#123;</span><br><span class="line">			((CheckpointedFunction) userFunction).snapshotState(context);</span><br><span class="line"></span><br><span class="line">			return true;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if (userFunction instanceof ListCheckpointed) &#123;</span><br><span class="line">			@SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">			List&lt;Serializable&gt; partitionableState = ((ListCheckpointed&lt;Serializable&gt;) userFunction).</span><br><span class="line">				snapshotState(context.getCheckpointId(), context.getCheckpointTimestamp());</span><br><span class="line"></span><br><span class="line">			ListState&lt;Serializable&gt; listState = backend.</span><br><span class="line">				getSerializableListState(DefaultOperatorStateBackend.DEFAULT_OPERATOR_STATE_NAME);</span><br><span class="line"></span><br><span class="line">			listState.clear();</span><br><span class="line"></span><br><span class="line">			if (null != partitionableState) &#123;</span><br><span class="line">				try &#123;</span><br><span class="line">					for (Serializable statePartition : partitionableState) &#123;</span><br><span class="line">						listState.add(statePartition);</span><br><span class="line">					&#125;</span><br><span class="line">				&#125; catch (Exception e) &#123;</span><br><span class="line">					listState.clear();</span><br><span class="line"></span><br><span class="line">					throw new Exception(&quot;Could not write partitionable state to operator &quot; +</span><br><span class="line">						&quot;state backend.&quot;, e);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br></pre></td></tr></table></figure><p></p><p>可以看到这里就只有以上分析的两种类型的checkpoined接口，<code>CheckpointedFunction</code>，只需要执行相应的snapshot方法，相应的函数就已经将要做snapshot的数据打入了相应的state中，而<code>ListCheckpointed</code>接口由于返回的是个List，所以需要手动的通过<code>getSerializableListState</code>注册一个<code>ListState</code>(<em>这也是ListCheckpointed只能注册一个state的原因</em>),然后将List数据挨个存入ListState中。</p><h4>operatorStateBackend#snapshot</h4><ol><li>针对所有注册的state作deepCopy,为了防止在checkpoint的时候数据结构又被修改，deepcopy其实是通过序列化和反序列化的过程（参见<a href="http://aitozi.com/java-serialization.html" target="_blank" rel="noopener">http://aitozi.com/java-serialization.html</a>）</li><li>异步将state以及metainfo的数据写入到hdfs中，使用的是flink的asyncIO（这个也可以后续深入了解下），并返回相应的statehandle用作restore的过程</li><li>在StreamTask触发checkpoint的时候会将一个Task中所有的operator触发一次snapshot，触发部分就是上面1，2两个步骤，其中第二步是会返回一个RunnableFuture，在触发之后会提交一个<code>AsyncCheckpointRunnable</code>异步任务，会阻塞一直等到checkpoint的<code>Future</code>，其实就是去调用这个方法<code>AbstractAsyncIOCallable</code>, 直到完成之后OperatorState会返回一个<code>OperatorStateHandle</code>,这个地方和后文的keyedState返回的handle不一样。</li></ol><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">	public V call() throws Exception &#123;</span><br><span class="line"></span><br><span class="line">		synchronized (this) &#123;</span><br><span class="line">			if (isStopped()) &#123;</span><br><span class="line">				throw new IOException(&quot;Task was already stopped. No I/O handle opened.&quot;);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			ioHandle = openIOHandle();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		try &#123;</span><br><span class="line"></span><br><span class="line">			return performOperation();</span><br><span class="line"></span><br><span class="line">		&#125; finally &#123;</span><br><span class="line">			closeIOHandle();</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure><p></p><p>在managed keyedState、managed operatorState、raw keyedState、和raw operatorState都完成返回相应的Handle之后，会生成一个SubTaskState来ack jobmanager,这个主要是用在restore的过程中</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">SubtaskState subtaskState = createSubtaskStateFromSnapshotStateHandles(</span><br><span class="line">					chainedNonPartitionedOperatorsState,</span><br><span class="line">					chainedOperatorStateBackend,</span><br><span class="line">					chainedOperatorStateStream,</span><br><span class="line">					keyedStateHandleBackend,</span><br><span class="line">					keyedStateHandleStream);</span><br><span class="line">					</span><br><span class="line">owner.getEnvironment().acknowledgeCheckpoint(</span><br><span class="line">	checkpointMetaData.getCheckpointId(),</span><br><span class="line">	checkpointMetrics,</span><br><span class="line">	subtaskState);</span><br></pre></td></tr></table></figure><p></p><p>在jm端，ack的时候又将各个handle封装在<code>pendingCheckpoint =&gt; operatorStates =&gt; operatorState =&gt; operatorSubtaskState</code>中,最后无论是savepoint或者是externalCheckpoint都会将相应的handle序列化存储到hdfs，这也就是所谓的checkpoint元数据。这个可以起个任务观察下zk和hdfs上的文件，补充一下相关的验证。</p><p>至此完成operator state的snapshot/checkpoint阶段</p><h4>KeyedStateBackend#snapshot</h4><p>和operatorStateBackend一样，snapshot也分为了同步和异步两个部分。</p><ol><li>rocksDB的keyedStateBackend的snapshot提供了增量和全量两种方式</li><li>利用rocksdb自身的snapshot进行<code>this.snapshot = stateBackend.db.getSnapshot();</code> 这个过程是同步的，rocksdb这块是怎么snapshot还不是很了解，待后续学习</li><li>之后也是一样异步将数据写入hdfs，返回相应的keyGroupsStateHandle <code>snapshotOperation.closeCheckpointStream();</code></li></ol><p>不同的地方在于增量返回的是<code>IncrementalKeyedStateHandle</code>,而全量返回的是<code>KeyGroupsStateHandle</code>，</p><h3>restore / redistribution</h3><h4>OperatorState的rescale</h4><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">void setInitialState(TaskStateHandles taskStateHandles) throws Exception;</span><br></pre></td></tr></table></figure><p></p><p>一个task在真正的执行任务之前所需要做的事情是把状态inject到task中，如果一个任务是失败之后从上次的checkpoint点恢复的话，他的状态就是非空的。streamTask也就靠是否有这样的一个恢复状态来确认算子是不是在restore来branch他的启动逻辑</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">if (null != taskStateHandles) &#123;</span><br><span class="line">		if (invokable instanceof StatefulTask) &#123;</span><br><span class="line">			StatefulTask op = (StatefulTask) invokable;</span><br><span class="line">			op.setInitialState(taskStateHandles);</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			throw new IllegalStateException(&quot;Found operator state for a non-stateful task invokable&quot;);</span><br><span class="line">		&#125;</span><br><span class="line">		// be memory and GC friendly - since the code stays in invoke() for a potentially long time,</span><br><span class="line">		// we clear the reference to the state handle</span><br><span class="line">		//noinspection UnusedAssignment</span><br><span class="line">		taskStateHandles = null;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>那么追根究底一下这个Handle是怎么带入的呢？</p><p><code>FixedDelayRestartStrategy =&gt; triggerFullRecovery =&gt; Execution#restart =&gt; Execution#scheduleForExecution =&gt; Execution#deployToSlot =&gt; ExecutionVertex =&gt; TaskDeploymentDescriptor =&gt; taskmanger =&gt; task</code></p><p>当然还有另一个途径就是通过向jobmanager submitJob的时候带入restore的checkpoint path， 这两种方式最终都会通过<code>checkpointCoordinator#restoreLatestCheckpointedState</code>来恢复hdfs中的状态来获取到snapshot时候存入的StateHandle。</p><p>恢复的过程如何进行redistribution呢？ 也就是大家关心的并发度变了我的状态的行为是怎么样的。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// re-assign the task states</span><br><span class="line">final Map&lt;OperatorID, OperatorState&gt; operatorStates = latest.getOperatorStates();</span><br><span class="line"></span><br><span class="line">StateAssignmentOperation stateAssignmentOperation =</span><br><span class="line">		new StateAssignmentOperation(tasks, operatorStates, allowNonRestoredState);</span><br><span class="line"></span><br><span class="line">stateAssignmentOperation.assignStates();</span><br></pre></td></tr></table></figure><p></p><ol><li>如果并发度没变那么不做重新的assign，除非state的模式是broadcast，会将一个task的state广播给所有的task</li><li>对于operator state会针对每一个name的state计算出每个subtask中的element个数之和（这就要求每个element之间相互独立）进行roundrobin分配</li><li>keyedState的重新分配相对简单，就是根据新的并发度和最大并发度计算新的keygroupRange，然后根据subtaskIndex获取keyGroupRange，然后获取到相应的keyStateHandle完成状态的切分。</li></ol><p>这里补充关于raw state和managed state在rescale上的差别，由于operator state在reassign的时候是根据metaInfo来计算出所有的List&lt;element&gt;来重新分配，operatorbackend中注册的状态是会保存相应的metainfo，最终也会在snapshot的时候存入OperatorHandle，那raw state的metainfo是在哪里呢？</p><p>其实会在写入hdfs返回相应的handle的时候构建一个默认的，<code>OperatorStateCheckpointOutputStream#closeAndGetHandle</code>,其中状态各个partition的构建来自<code>startNewPartition</code>方法，引擎中我所看到的rawstate仅有timerservice的raw keyedState</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">OperatorStateHandle closeAndGetHandle() throws IOException &#123;</span><br><span class="line">		StreamStateHandle streamStateHandle = delegate.closeAndGetHandle();</span><br><span class="line"></span><br><span class="line">		if (null == streamStateHandle) &#123;</span><br><span class="line">			return null;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		if (partitionOffsets.isEmpty() &amp;&amp; delegate.getPos() &gt; initialPosition) &#123;</span><br><span class="line">			startNewPartition();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		Map&lt;String, OperatorStateHandle.StateMetaInfo&gt; offsetsMap = new HashMap&lt;&gt;(1);</span><br><span class="line"></span><br><span class="line">		OperatorStateHandle.StateMetaInfo metaInfo =</span><br><span class="line">				new OperatorStateHandle.StateMetaInfo(</span><br><span class="line">						partitionOffsets.toArray(),</span><br><span class="line">						OperatorStateHandle.Mode.SPLIT_DISTRIBUTE);</span><br><span class="line"></span><br><span class="line">		offsetsMap.put(DefaultOperatorStateBackend.DEFAULT_OPERATOR_STATE_NAME, metaInfo);</span><br><span class="line"></span><br><span class="line">		return new OperatorStateHandle(offsetsMap, streamStateHandle);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><h4>KeyedState的keyGroup</h4><p>keyedState重新分配里引入了一个keyGroup的概念，那么这里为什么要引入keygroup这个概念呢？</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-8-3/53377760.jpg" alt></p><p><img src="http://or0igopk2.bkt.clouddn.com/18-8-3/18711916.jpg" alt></p><ol><li>hash(key) = key(identity)</li><li>key_group(key) = hash(key) % number_of_key_groups (等于最大并发)，默认flink任务会设置一个max parallel</li><li>subtask(key) = key_greoup(key) * parallel / number_of_key_groups</li></ol><ul><li>避免在恢复的时候带来随机IO</li><li>避免每个subtask需要将所有的状态数据读取出来pick和自己subtask相关的浪费了很多io资源</li><li>减少元数据的量，不再需要保存每次的key，每一个keygroup组只需保留一个range</li></ul><p>实际实现上的keyGroup range和上图有区别，是连续的:</p><p>比如：subtask1: [0-10], subtask2: [11-12] ...</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int start = operatorIndex == 0 ? 0 : ((operatorIndex * maxParallelism - 1) / parallelism) + 1;</span><br><span class="line">int end = ((operatorIndex + 1) * maxParallelism - 1) / parallelism;</span><br><span class="line">return new KeyGroupRange(start, end);</span><br></pre></td></tr></table></figure><p></p><ul><li>每一个backend（subtask）上只有一个keygroup range</li><li>每一个subtask在restore的时候就接收到了已经分配好的和重启后当前这个并发相绑定的keyStateHandle</li></ul><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">subManagedKeyedState = getManagedKeyedStateHandles(operatorState, keyGroupPartitions.get(subTaskIndex));</span><br><span class="line">subRawKeyedState = getRawKeyedStateHandles(operatorState, keyGroupPartitions.get(subTaskIndex));</span><br></pre></td></tr></table></figure><p></p><p>这里面关键的一步在于，根据新的subtask上的keyGroupRange，从原来的operator的keyGroupsStateHandle中求取本subtask所关心的一部分Handle，可以看到每个KeyGroupsStateHandle都维护了<code>KeyGroupRangeOffsets</code>这样一个变量，来标记这个handle所覆盖的keygrouprange，以及keygrouprange在stream中offset的位置，可以看下再snapshot的时候会记录offset到这个对象中来</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keyGroupRangeOffsets.setKeyGroupOffset(mergeIterator.keyGroup(), outStream.getPos());</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public KeyGroupRangeOffsets getIntersection(KeyGroupRange keyGroupRange) &#123;</span><br><span class="line">		Preconditions.checkNotNull(keyGroupRange);</span><br><span class="line">		KeyGroupRange intersection = this.keyGroupRange.getIntersection(keyGroupRange);</span><br><span class="line">		long[] subOffsets = new long[intersection.getNumberOfKeyGroups()];</span><br><span class="line">		if(subOffsets.length &gt; 0) &#123;</span><br><span class="line">			System.arraycopy(</span><br><span class="line">					offsets,</span><br><span class="line">					computeKeyGroupIndex(intersection.getStartKeyGroup()),</span><br><span class="line">					subOffsets,</span><br><span class="line">					0,</span><br><span class="line">					subOffsets.length);</span><br><span class="line">		&#125;</span><br><span class="line">		return new KeyGroupRangeOffsets(intersection, subOffsets);</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p>KeyGroupsStateHandle是一个subtask的所有state的一个handle KeyGroupsStateHandle维护一个KeyGroupRangeOffsets， KeyGroupRangeOffsets维护一个KeyGroupRange和offsets KeyGroupRange维护多个KeyGroup KeyGroup维护多个key</p><p>KeyGroupsStateHandle和operatorStateHandle还有一个不同点，operatorStateHandle维护了metainfo中的offset信息用在restore时的reassign，原因在于KeyGroupsStateHandle的reassign不依赖这些信息，当然在restore的时候也需要keygroupOffset中的offset信息来重新构建keyGroupsStateHandle来进行各个task的状态分配。</p><p>参考：</p><p><a href="https://flink.apache.org/features/2017/07/04/flink-rescalable-state.html" target="_blank" rel="noopener">https://flink.apache.org/features/2017/07/04/flink-rescalable-state.html</a></p><p><a href="http://chenyuzhao.me/2017/12/24/Flink-%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E7%9A%84%E8%AE%BE%E8%AE%A1-%E5%AD%98%E5%82%A8/" target="_blank" rel="noopener">http://chenyuzhao.me/2017/12/24/Flink-分布式快照的设计-存储/</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;本文是源于要在内部分享，所以提前整理了一些flink中的状态的一些知识，flink状态所包含的东西很多，在下面列举了一些，还有一 些在本文没有体现，后
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Protobuf深入理解</title>
    <link href="http://www.aitozi.com/dig-protobuf.html"/>
    <id>http://www.aitozi.com/dig-protobuf.html</id>
    <published>2018-07-28T15:28:01.000Z</published>
    <updated>2019-03-14T17:06:49.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>本文带你深入理解和使用protobuf</p><p>&lt;!--more--&gt;</p><h2>简介</h2><p>Protocol Buffers 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python、Go 等语言的 API</p><p>使用protobuf需要这样几个步骤：</p><ul><li>在<code>.proto</code>文件中定义消息的格式</li><li>通过protoBuffer compiler编译生成相应的java类</li><li>通过Java protocol buffer Api来write和read相关的对象</li></ul><p>关于PB的操作方式见： <a href="https://developers.google.com/protocol-buffers/docs/javatutorial" target="_blank" rel="noopener">https://developers.google.com/protocol-buffers/docs/javatutorial</a></p><hr><p>protobuf的序列化快的原因主要在于其编码实现和封解包的速度</p><h2>Protobuf编码</h2><h3>Base 128 Varints 编码</h3><p>数据传输中出于IO的考虑，我们会希望尽可能的对数据进行压缩。 Varint就是一种对数字进行编码的方法，编码后二进制数据是不定长的，数值越小的数字使用的字节数越少。例如对于int32_t，采用Varint编码后需要1~5个bytes，小的数字使用1个byte，大的数字使用5个bytes。基于实际场景中小数字的使用远远多于大数字，因此通过Varint编码对于大部分场景都可以起到一个压缩的效果。Varint的主要想法就是以标志位替换掉高字节的若干个0</p><p>下图是数字131415的variant编码,通过3个字节来表示131415 <img src="http://or0igopk2.bkt.clouddn.com/18-7-28/50478975.jpg" alt></p><p>其中第一个字节的高位msb（Most Significant Bit ）为1表示下一个字节还有有效数据，msb为0表示该字节中的后7为是最后一组有效数字。踢掉最高位后的有效位组成真正的数字。注意到最终计算前将两个 byte 的位置相互交换过一次，这是因为 Google Protocol Buffer 字节序采用 little-endian（即低位字节排放在内存的低地址端） 的方式</p><p>从上面可以看出，variant编码存储比较小的整数时很节省空间，小于等于127的数字可以用一个字节存储。但缺点是对于大于</p><p>268,435,455（0xfffffff）的整数需要5个字节来存储。但是一般情况下（尤其在tag编码中）不会存储这么大的整数。</p><p>关于int32的varint编码代码</p><p></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span>* <span class="title">EncodeVarint32</span><span class="params">(<span class="keyword">char</span>* dst, <span class="keyword">uint32_t</span> v)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Operate on characters as unsigneds</span></span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">char</span>* ptr = <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">char</span>*&gt;(dst);</span><br><span class="line">  <span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> B = <span class="number">128</span>;</span><br><span class="line">  <span class="keyword">if</span> (v &lt; (<span class="number">1</span>&lt;&lt;<span class="number">7</span>)) &#123;</span><br><span class="line">    *(ptr++) = v;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span>&lt;&lt;<span class="number">14</span>)) &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = v&gt;&gt;<span class="number">7</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span>&lt;&lt;<span class="number">21</span>)) &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = (v&gt;&gt;<span class="number">7</span>) | B;</span><br><span class="line">    *(ptr++) = v&gt;&gt;<span class="number">14</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span>&lt;&lt;<span class="number">28</span>)) &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = (v&gt;&gt;<span class="number">7</span>) | B;</span><br><span class="line">    *(ptr++) = (v&gt;&gt;<span class="number">14</span>) | B;</span><br><span class="line">    *(ptr++) = v&gt;&gt;<span class="number">21</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    *(ptr++) = v | B;</span><br><span class="line">    *(ptr++) = (v&gt;&gt;<span class="number">7</span>) | B;</span><br><span class="line">    *(ptr++) = (v&gt;&gt;<span class="number">14</span>) | B;</span><br><span class="line">    *(ptr++) = (v&gt;&gt;<span class="number">21</span>) | B;</span><br><span class="line">    *(ptr++) = v&gt;&gt;<span class="number">28</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">char</span>*&gt;(ptr);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>Message Structure编码</h3><p>protocol buffer 中 message 是一系列键值对。message 的二进制版本只是使用字段号(field's number 和 wire_type)作为 key。每个字段的名称和声明类型只能在解码端通过引用消息类型的定义（即 .proto 文件）来确定。这一点也是人们常常说的 protocol buffer 比 JSON，XML 安全一点的原因，如果没有数据结构描述 .proto 文件，拿到数据以后是无法解释成正常的数据的。</p><p>当消息编码时，键和值被连接成一个字节流。当消息被解码时，解析器需要能够跳过它无法识别的字段。这样，可以将新字段添加到消息中，而不会破坏不知道它们的旧程序。这就是所谓的 “向后”兼容性。</p><p>为此，线性的格式消息中每对的“key”实际上是两个值，其中一个是来自.proto文件的字段编号，加上提供正好足够的信息来查找下一个值的长度。在大多数语言实现中，这个 key 被称为 tag</p><p>wireType</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-7-28/87891770.jpg" alt></p><p>key 的计算方法是 (field_number &lt;&lt; 3) | wire_type，换句话说，key 的最后 3 位表示的就是 wire_type。因此这里也涉及到前面proto文件定义的时候的宗旨，尽量将频繁使用的字段的字段号设置成1-15之间的数值，避免位数开销。这里的key的存储也是用了varint的方式</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-7-28/94576622.jpg" alt></p><p>举例，一般 message 的字段号都是 1 开始的，所以对应的 tag 可能是这样的：</p><p><code>000 1000</code></p><p>末尾3位表示的是value的类型，这里是000，即0，代表的是varint值。右移3位，即0001，这代表的就是字段号(field number)。tag的例子就举这么多，接下来举一个 value的例子，还是用varint来举例：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">96 01 = 1001 0110  0000 0001</span><br><span class="line">       → 000 0001  ++  001 0110 (drop the msb and reverse the groups of 7 bits)</span><br><span class="line">       → 10010110</span><br><span class="line">       → 128 + 16 + 4 + 2 = 150</span><br></pre></td></tr></table></figure><p></p><p>所以 96 01 代表的数据就是 150 。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">message Test1 &#123;</span><br><span class="line">  required int32 a = 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>如果存在上面这样的一个 message 的结构，如果存入 150，在 Protocol Buffer 中显示的二进制应该为 08 96 01 <code>varint(1 &lt;&lt; 3 | 0) = 0x08</code>.</p><p>注意到varint编码也应用在了key的计算上，使用非常频繁，或许是基于这个原因，pb里实现了一种性能更高的方案（coded_stream.cc）</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">inline uint8* CodedOutputStream::WriteVarint32FallbackToArrayInline(</span><br><span class="line">    uint32 value, uint8* target) &#123;</span><br><span class="line">  target[0] = static_cast&lt;uint8&gt;(value | 0x80);</span><br><span class="line">  if (value &gt;= (1 &lt;&lt; 7)) &#123;</span><br><span class="line">    target[1] = static_cast&lt;uint8&gt;((value &gt;&gt;  7) | 0x80);</span><br><span class="line">    if (value &gt;= (1 &lt;&lt; 14)) &#123;</span><br><span class="line">      target[2] = static_cast&lt;uint8&gt;((value &gt;&gt; 14) | 0x80);</span><br><span class="line">      if (value &gt;= (1 &lt;&lt; 21)) &#123;</span><br><span class="line">        target[3] = static_cast&lt;uint8&gt;((value &gt;&gt; 21) | 0x80);</span><br><span class="line">        if (value &gt;= (1 &lt;&lt; 28)) &#123;</span><br><span class="line">          target[4] = static_cast&lt;uint8&gt;(value &gt;&gt; 28);</span><br><span class="line">          return target + 5;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          target[3] &amp;= 0x7F;</span><br><span class="line">          return target + 4;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        target[2] &amp;= 0x7F;</span><br><span class="line">        return target + 3;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      target[1] &amp;= 0x7F;</span><br><span class="line">      return target + 2;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    target[0] &amp;= 0x7F;</span><br><span class="line">    return target + 1;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>测试了1kw条数据，两种方案的时间对比为 196742us vs 269806us，在pb序列化反序列化大量使用varint的前提下，这个性能提升就很有必要了(这是原作者做的测试)</p><p>type 需要注意的是 type = 2 的情况，tag 里面除了包含 field number 和 wire_type ，还需要再包含一个 length，决定 value 从那一段取出来</p><h3>负数使用varint编码的问题</h3><p>varint编码希望以标志位能够节省掉高字节的0，但是负数的最高位一定是1， 所以varint在处理32位负数时会固定的占用5个字节。比如我们修改下之前的程序test.set_a(-1)，序列化之后的数据为</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">08ff ffff ffff ffff ffff 01</span><br></pre></td></tr></table></figure><p></p><p>有11个字节之多！除了key=0x08占用的1个字节，value=-1占用了10个字节。</p><p>对应的代码（coded_stream.h）</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">inline void CodedOutputStream::WriteVarint32SignExtended(int32 value) &#123;</span><br><span class="line">  if (value &lt; 0) &#123;</span><br><span class="line">    WriteVarint64(static_cast&lt;uint64&gt;(value));</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    WriteVarint32(static_cast&lt;uint32&gt;(value));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>int32被转换成了uint64(为什么？)原作者这里问为什么== 原因在文档中有提及</p><blockquote><p>If you use int32 or int64 as the type for a negative number, the resulting varint is always ten bytes long – it is, effectively, treated like a very large unsigned integer,即uint64，也就是上面代码写的那样。但是为什么生成是10个字节呢? 因为uint64是10个?</p></blockquote><p>再经过varint编码。这就是10个字节的原因了。当然如果你使用了signed types那么产出的varint编码结果使用了Zigzag编码就会相当的高效。</p><h3>Zigzag编码</h3><p>ZigZag是将有符号数统一映射到无符号数的一种编码方案，对于无符号数0 1 2 3 4，映射前的有符号数分别为0 -1 1 -2 2，负数以及对应的正数来回映射到从0变大的数字序列里，这也是”zig-zag”的名字来源。将所有整数映射成无符号整数，然后再采用 varint 编码方式编码，这样，绝对值小的整数，编码后也会有一个较小的 varint 编码值。</p><p>Zigzag 映射函数为：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Zigzag(n) = (n &lt;&lt; 1) ^ (n &gt;&gt; 31), n 为 sint32 时</span><br><span class="line">Zigzag(n) = (n &lt;&lt; 1) ^ (n &gt;&gt; 63), n 为 sint64 时</span><br></pre></td></tr></table></figure><p></p><p>按照这种方法，-1 将会被编码成 1，1 将会被编码成 2，-2 会被编码成 3，如下表所示：</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-7-28/17159197.jpg" alt></p><p>存疑？</p><p>目前仍有一个地方不大清楚，就是对于int32类型的负数，protobuf强制编码成10个字节，理论上5个字节就够了。 （来自别人的问题，我也没懂，确实想了下int32的负数5个就够了，int64的负数才需要10个？）</p><h3>负数及大整数的解决方案</h3><p>protobuf里提供了一种sint32/sint64来使用ZigZag编码。</p><p>修改proto:optional sint32 a = 1，这样在test.set_a(-1)并序列化后只有两个字节08 01</p><p>同理对于大整数，optional int32 a = 1;，test.set_a(1 &lt;&lt; 28)序列化后可以看到占用了6个字节0880 8080 8001，解决方案也是使用不同的类型定义optional <strong>fixed32</strong> a = 1来解决，使用这种方案后int32固定的占用4个字节。这种其实就是官网中的<code>Non-varint Numbers</code></p><h3>字符串</h3><p>wire_type 类型为 2 的数据，是一种指定长度的编码方式：key + length + content，key 的编码方式是统一的，length 采用 varints 编码方式，content 就是由 length 指定长度的 Bytes</p><p>举例，假设定义如下的 message 格式：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">message Test2 &#123;</span><br><span class="line">  optional string b = 2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>设置该值为&quot;testing&quot;，二进制格式查看：<code>12 07 74 65 73 74 69 6e 67</code>, <code>74 65 73 74 69 6e 67</code> 是“testing”的 UTF8 代码。</p><p>12 -&gt; 0001 0010，后三位 010 为 wire type = 2，0001 0010 右移三位为 0000 0010，即 tag = 2。</p><p>length 此处为 7，后边跟着 7 个bytes，即我们的字符串&quot;testing&quot;。</p><p>所以 wire_type 类型为 2 的数据，编码的时候会默认转换为 T-L-V (Tag - Length - Value)的形式. TLV的模式减少了分隔符的使用，数据存储更加紧凑。需要转变为 T - L - V 形式的还有 string, bytes, embedded messages, packed repeated fields。</p><h3>小结</h3><ul><li>Protocol Buffer 利用 varint 原理压缩数据以后，二进制数据非常紧凑，option 也算是压缩体积的一个举措。所以 pb 体积更小，如果选用它作为网络数据传输，势必相同数据，消耗的网络流量更少。但是并没有压缩到极限，float、double 浮点型都没有压缩。</li><li>Protocol Buffer 比 JSON 和 XML 少了 {、}、: 这些符号，体积也减少一些。再加上 varint 压缩，gzip 压缩以后体积更小！</li><li>Protocol Buffer 是 Tag - Value (Tag - Length - Value)的编码方式的实现，减少了分隔符的使用，数据存储更加紧凑。</li><li>Protocol Buffer 另外一个核心价值在于提供了一套工具，一个编译工具，自动化生成 get/set 代码。简化了多语言交互的复杂度，使得编码解码工作有了生产力。</li><li>Protocol Buffer 不是自我描述的，离开了数据描述 .proto 文件，就无法理解二进制数据流。这点即是优点，使数据具有一定的“加密性”，也是缺点，数据可读性极差。所以 Protocol Buffer 非常适合内部服务之间 RPC 调用和传递数据。</li><li>Protocol Buffer 具有向后兼容的特性，更新数据结构以后，老版本依旧可以兼容，这也是 Protocol Buffer 诞生之初被寄予解决的问题。因为编译器对不识别的新增字段会跳过不处理</li></ul><h2>Protobuf反序列化</h2><p><a href="https://halfrost.com/protobuf_encode/" target="_blank" rel="noopener">https://halfrost.com/protobuf_encode/</a></p><p>整个解析过程需要 Protobuf 本身的框架代码和由 Protobuf 编译器生成的代码共同完成。Protobuf 提供了基类 Message 以及 Message_lite 作为通用的 Framework，，CodedInputStream 类，WireFormatLite 类等提供了对二进制数据的 decode 功能，从 5.1 节的分析来看，Protobuf 的解码可以通过几个简单的数学运算完成，无需复杂的词法语法分析，因此 ReadTag() 等方法都非常快。 在这个调用路径上的其他类和方法都非常简单，感兴趣的读者可以自行阅读。 相对于 XML 的解析过程，以上的流程图实在是非常简单吧？这也就是 Protobuf 效率高的第二个原因了</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-7-28/17159197.jpg" alt></p><h2>与json thift的性能比较</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzA3NDcyMTQyNQ==&amp;mid=2649257430&amp;idx=1&amp;sn=975b6123d8256221f6bac3b99e52af9a&amp;chksm=8767a428b0102d3e6ab7abdf797c481da570cb29e274aa4ff6ecd931f535166b776e6548941d&amp;scene=0&amp;key=399a205ce674169cbedcc1c459650908e22d6a2b81674195c3b251114acdf821dbde7bb49102c6b47f61b26a7a404d74e0e8440cea3675a7ea8f49eafd8639bfb733183a1bfb4603232d6cb8ecd230e5&amp;ascene=0&amp;uin=NTkxMDk2NjU=&amp;devicetype=iMac+MacBookPro12,1+OSX+OSX+10.12.4+build(16E195)&amp;version=12020510&amp;nettype=WIFI&amp;fontScale=100&amp;pass_ticket=wHPj0w18CV8zHl6HCfd9t9LQfs3I0ZULhUILuOHgL0E=" target="_blank" rel="noopener">Protobuf有没有比JSON快5倍？用代码来击破pb性能神话</a></p><h2>总结</h2><p>protobuf的性能来源于对存储的压缩，避免一切不必要的字节开销。flink中如过将大多数需要存储到state中的对象先转成PB格式会得到很大的性能提升。</p><p>和protobuf通常被同时提及的有Apache Thrift / Avro 本文已经没有空间介绍，待后续深入了解。</p><p>这篇文章介绍了3中数据结构如何做到对消息体格式演变的透明 <a href="https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html" target="_blank" rel="noopener">https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html</a></p><p>参考：</p><p><a href="https://developers.google.com/protocol-buffers/docs/javatutorial" target="_blank" rel="noopener">https://developers.google.com/protocol-buffers/docs/javatutorial</a> <a href="https://developers.google.com/protocol-buffers/docs/encoding#structure" target="_blank" rel="noopener">https://developers.google.com/protocol-buffers/docs/encoding#structure</a> <a href="https://halfrost.com/protobuf_encode/" target="_blank" rel="noopener">https://halfrost.com/protobuf_encode/</a> <a href="https://izualzhy.cn/protobuf-encode-varint-and-zigzag" target="_blank" rel="noopener">https://izualzhy.cn/protobuf-encode-varint-and-zigzag</a> <a href="https://izualzhy.cn/protobuf-encoding" target="_blank" rel="noopener">https://izualzhy.cn/protobuf-encoding</a> <a href="https://www.ibm.com/developerworks/cn/linux/l-cn-gpb/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/l-cn-gpb/index.html</a> <a href="https://segmentfault.com/a/1190000004891020" target="_blank" rel="noopener">https://segmentfault.com/a/1190000004891020</a> protobufstuff <a href="https://mp.weixin.qq.com/s?__biz=MzA3NDcyMTQyNQ==&amp;mid=2649257430&amp;idx=1&amp;sn=975b6123d8256221f6bac3b99e52af9a&amp;chksm=8767a428b0102d3e6ab7abdf797c481da570cb29e274aa4ff6ecd931f535166b776e6548941d&amp;scene=0&amp;key=399a205ce674169cbedcc1c459650908e22d6a2b81674195c3b251114acdf821dbde7bb49102c6b47f61b26a7a404d74e0e8440cea3675a7ea8f49eafd8639bfb733183a1bfb4603232d6cb8ecd230e5&amp;ascene=0&amp;uin=NTkxMDk2NjU=&amp;devicetype=iMac+MacBookPro12,1+OSX+OSX+10.12.4+build(16E195)&amp;version=12020510&amp;nettype=WIFI&amp;fontScale=100&amp;pass_ticket=wHPj0w18CV8zHl6HCfd9t9LQfs3I0ZULhUILuOHgL0E=" target="_blank" rel="noopener">1</a>: &quot;https://mp.weixin.qq.com/s&quot;</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;本文带你深入理解和使用protobuf&lt;/p&gt;&lt;p&gt;&amp;lt;!--more--&amp;gt;&lt;/p&gt;&lt;h2&gt;简介&lt;/h2&gt;&lt;p&gt;Protocol Buffe
    
    </summary>
    
      <category term="刨根问底" scheme="http://www.aitozi.com/categories/%E5%88%A8%E6%A0%B9%E9%97%AE%E5%BA%95/"/>
    
    
      <category term="protobuf" scheme="http://www.aitozi.com/tags/protobuf/"/>
    
  </entry>
  
  <entry>
    <title>Java序列化拾掇</title>
    <link href="http://www.aitozi.com/java-serialization.html"/>
    <id>http://www.aitozi.com/java-serialization.html</id>
    <published>2018-07-27T16:51:18.000Z</published>
    <updated>2019-03-14T17:06:29.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>Java序列化拾掇</p><p>&lt;!-- more --&gt;</p><blockquote><p>本来想总结一下对google protobuf的用法总结，然而搜资料的过程中发现对很多java序列化的知识不足，故做了一些拾掇，在flink中关于类型序列化的地方其实也涉及很多，待以后看到，如有新的思考再来补充。</p></blockquote><h2>java序列化</h2><ol><li>在Java中，只要一个类实现了java.io.Serializable接口，那么它就可以被序列化</li><li>若父类未实现Serializable,而子类序列化了，父类属性值不会被保存，反序列化后父类属性值丢失</li><li>通过ObjectOutputStream和ObjectInputStream对对象进行序列化及反序列化</li><li>在deserialized的时候类的构造器是不会被调用的，只会调用没有实现Serializabe接口的父类的无参构造方法，如果其父类不可序列化，并且没有无参构造函数就会导致<code>InvalidClassException</code></li><li>只有non-static的成员并且没有标记为transient才会被序列化</li><li>类所包含的成员变量也必须是可序列化的</li><li>transient 关键字的作用是控制变量的序列化，在变量声明前加上该关键字，可以阻止该变量被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 型的是 0，对象型的是 null</li><li>java的serialVersionUID用来表明类的不同版本间的兼容性，必须被定义成final static long才能生效，否则会报错</li><li>在使用Externalizable进行序列化的时候，在读取对象时，会调用被序列化类的无参构造器去创建一个新的对象，然后再将被保存对象的字段的值分别填充到新对象中。所以，实现Externalizable接口的类必须要提供一个public的无参的构造器，如果一个Java类没有定义任何构造函数，编译器会帮我们自动添加一个无参的构造方法，可是，如果我们在类中定义了一个有参数的构造方法了，编译器便不会再帮我们创建无参构造方法，这点需要注意</li><li>用户自定义的 writeObject 和 readObject 方法可以允许用户控制序列化的过程</li></ol><h3>serialVersionUID</h3><p>Java的序列化机制是通过在运行时判断类的serialVersionUID来验证版本一致性的。在进行反序列化时，JVM会把传来 的字节流中的serialVersionUID与本地相应实体（类）的serialVersionUID进行比较，如果相同就认为是一致的，可以进行反序 列化，否则就会出现序列化版本不一致的异常。</p><p>当实现java.io.Serializable接口的实体（类）没有显式地定义一个名为serialVersionUID，类型为long的变 量时，Java序列化机制会根据编译的class自动生成一个serialVersionUID作序列化版本比较用，这种情况下，只有同一次编译生成的 class才会生成相同的serialVersionUID 。</p><p>如果我们不希望通过编译来强制划分软件版本，即实现序列化接口的实体能够兼容先前版本，未作更改的类，就需要显式地定义一个名为serialVersionUID，类型为long的变量，不修改这个变量值的序列化实体都可以相互进行串行化和反串行化。</p><h3>在内部类使用中带来的困扰</h3><blockquote><p>A class that is serializable with an enclosing class that is not serializable causes serialization to fail. Non-static nested classes that implement Serializable must be defined in an enclosing class that is also serializable. Non-static nested classes retain an implicit reference to an instance of their enclosing class. If the enclosing class is not serializable, the Java serialization mechanism fails with a java.io.NotSerializableException.</p></blockquote><p>一个非静态的内部类实现了Serializable接口，要求其外部类也同样实现Serializable接口。这是因为一个非静态内部类包含有一个隐式的指向外部包装类实例对象的一个指针，如上面指出的规则，序列化的时候要求类的非静态成员也需要是可序列化的，如果外部类没有声明Serializable，java序列化机制就会报错，解法通常是</p><ul><li>将内部类声明为static，这样就不包含隐式指针了</li><li>将外部类声明为Serializable</li></ul><p>在flink中采用了另一种解法，用户通过匿名内部类来定义一个userFuntion，通常userFunction需要被序列化来分发到各个task节点来执行，定义成static不如匿名类方便，外部主类定义成Serializable的代价又比较大，因此采用另一种解法：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">for (Field f: cls.getDeclaredFields()) &#123;</span><br><span class="line">	if (f.getName().startsWith(&quot;this$&quot;)) &#123;</span><br><span class="line">		// found a closure referencing field - now try to clean</span><br><span class="line">		closureAccessed |= cleanThis0(func, cls, f.getName());</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private static boolean cleanThis0(Object func, Class&lt;?&gt; cls, String this0Name) &#123;</span><br><span class="line"></span><br><span class="line">	This0AccessFinder this0Finder = new This0AccessFinder(this0Name);</span><br><span class="line">	getClassReader(cls).accept(this0Finder, 0);</span><br><span class="line"></span><br><span class="line">	final boolean accessesClosure = this0Finder.isThis0Accessed();</span><br><span class="line"></span><br><span class="line">	if (LOG.isDebugEnabled()) &#123;</span><br><span class="line">		LOG.debug(this0Name + &quot; is accessed: &quot; + accessesClosure);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (!accessesClosure) &#123;</span><br><span class="line">		Field this0;</span><br><span class="line">		try &#123;</span><br><span class="line">			this0 = func.getClass().getDeclaredField(this0Name);</span><br><span class="line">		&#125; catch (NoSuchFieldException e) &#123;</span><br><span class="line">			// has no this$0, just return</span><br><span class="line">			throw new RuntimeException(&quot;Could not set &quot; + this0Name + &quot;: &quot; + e);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		try &#123;</span><br><span class="line">			this0.setAccessible(true);</span><br><span class="line">			this0.set(func, null);</span><br><span class="line">		&#125;</span><br><span class="line">		catch (Exception e) &#123;</span><br><span class="line">			// should not happen, since we use setAccessible</span><br><span class="line">			throw new RuntimeException(&quot;Could not set &quot; + this0Name + &quot; to null. &quot; + e.getMessage(), e);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	return accessesClosure;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>对每一个userFunction(可能实现自一个匿名内部类)有一个clean的机制。</p><ul><li>检查其声明字段有没有<code>this$</code>开始的，即指向外部类的引用</li><li>如果有将对应的字段通过反射置成null,这样就不会受第三条规则的困扰了</li></ul><h3>自定义序列化</h3><p>在序列化过程中，如果被序列化的类中定义了writeObject 和 readObject 方法，虚拟机会试图调用对象类里的 writeObject 和 readObject 方法，进行用户自定义的序列化和反序列化。并且这两个方法的signature必须是以下这样才会生效，否则就是默认的序列化方式</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">private void readObject(java.io.ObjectInputStream in)</span><br><span class="line">     throws IOException, ClassNotFoundException;</span><br><span class="line">private void writeObject(java.io.ObjectOutputStream out)</span><br><span class="line">     throws IOException;</span><br></pre></td></tr></table></figure><p></p><p>如果没有这样的方法，则默认调用是 ObjectOutputStream 的 defaultWriteObject 方法以及 ObjectInputStream 的 defaultReadObject 方法。这两个方法没有覆写，也没有被显式调用，为什么会生效呢？ 具体可以参见<a href="https://mp.weixin.qq.com/s/ABtxdNpr4bLpXtFiOK47hA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/ABtxdNpr4bLpXtFiOK47hA</a></p><blockquote><p>在使用ObjectOutputStream的writeObject方法和ObjectInputStream的readObject方法时，会通过反射的方式调用</p></blockquote><p>关于序列化的困惑可以在这个源码中得到解答</p><blockquote><p>ObjectOutputStream#writeObject</p><p>writeObject =&gt; writeObject0 =&gt; writeOrdinaryObject =&gt; writeSerialData =&gt; invokeWriteObject</p></blockquote><p>可以参见ArrayList使用了这种自定义序列化的方法，ArrayList实际上是动态数组，每次在放满以后自动增长设定的长度值，如果数组自动增长长度设为100，而实际只放了一个元素，那就会序列化99个null元素。为了保证在序列化的时候不会将这么多null同时进行序列化，ArrayList把元素数组设置为transient。</p><p><em>Ps：在两个方法的开始处，你会发现调用了defaultWriteObject()和defaultReadObject()。它们做的是默认的序列化进程，就像写/读所有的non-transient和 non-static字段(但他们不会去做serialVersionUID的检查).通常说来，所有我们想要自己处理的字段都应该声明为transient。这样的话，defaultWriteObject/defaultReadObject便可以专注于其余字段，而我们则可为这些特定的字段(译者：指transient)定制序列化。使用那两个默认的方法并不是强制的，而是给予了处理复杂应用时更多的灵活性</em></p><h3>关于反序列化的时候构造方法是否会被调用（反序列化是怎么做的）</h3><blockquote><p>A non-serializable, immediate superclass of a serializable class that does not itself declare an accessible, no-argument constructor causes deserialization to fail To allow subtypes of non-serializable classes to be serialized, the subtype may assume responsibility for saving and restoring the state of the supertype's public, protected, and (if accessible) package fields. The subtype may assume this responsibility only if the class it extends has an accessible no-arg constructor to initialize the class's state. It is an error to declare a class Serializable if this is not the case. The error will be detected at runtime.</p></blockquote><p>反序列化其实是将先前序列化生成的byte流重新构建成一个对象，byte流包含了所有重构对象的信息，包括class的元数据，实例的变量的类型信息，以及相应的值。然后在反序列化的时候它要求<strong>all the parent classes of instance should be Serializable; and if any super class in hirarchy is not Serializable then it must have a default constructor</strong>。在反序列化的时候会一直搜寻其父类，直到找到第一个不可序列化的类，就尝试调用其无参构造函数创建对象，如果所有的父类都是可序列化的，最终找到的就是Object类，然后首先创建一个Object对象。接着JVM就继续读取byte流，设置相关的类型信息，一个空对象创建完成后，jvm就设置相关的static字段，并调用<code>readObject</code>方法进行赋值</p><p>由于本类的构造方法不会被调用，所以你期望某个变量的初始化在构造方法中完成得到的只会是null。</p><p>在构造函数的调用上<code>Externalizable</code>和<code>Serializable</code>的表现不同，Externalizable依赖于本身类的无参构造函数</p><h3>利用序列化来做deepCopy</h3><p>主要用到了<code>ByteArrayOutputStream</code>,存储在内存中，不做持久化</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">public SerializableClass deepCopy() throws Exception&#123;</span><br><span class="line">    //Serialization of object</span><br><span class="line">    ByteArrayOutputStream bos = new ByteArrayOutputStream();</span><br><span class="line">    ObjectOutputStream out = new ObjectOutputStream(bos);</span><br><span class="line">    out.writeObject(this);</span><br><span class="line"></span><br><span class="line">    //De-serialization of object</span><br><span class="line">    ByteArrayInputStream bis = new   ByteArrayInputStream(bos.toByteArray());</span><br><span class="line">    ObjectInputStream in = new ObjectInputStream(bis);</span><br><span class="line">    SerializableClass copied = (SerializableClass) in.readObject();</span><br><span class="line">    return copied;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>更多关于java clone 可参考:</p><p><a href="https://howtodoinjava.com/core-java/cloning/a-guide-to-object-cloning-in-java/" target="_blank" rel="noopener">https://howtodoinjava.com/core-java/cloning/a-guide-to-object-cloning-in-java/</a></p><h3>如果对象状态需要同步，则对象序列化也需要同步</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">private synchronized void writeObject(ObjectOutputStream s) throws IOException &#123;</span><br><span class="line">        s.defaultWriteObject();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>单例模式序列化</h3><p>参考：</p><ul><li><a href="http://www.hollischuang.com/archives/1144" target="_blank" rel="noopener">http://www.hollischuang.com/archives/1144</a></li><li>枚举实现可序列化单例 <a href="http://www.cnblogs.com/cielosun/p/6596475.html" target="_blank" rel="noopener">http://www.cnblogs.com/cielosun/p/6596475.html</a></li><li><a href="https://leokongwq.github.io/2017/08/21/why-enum-singleton-are-serialization-safe.html" target="_blank" rel="noopener">https://leokongwq.github.io/2017/08/21/why-enum-singleton-are-serialization-safe.html</a></li></ul><h3>在不同的classloader之间进行对象的序列化和反序列化</h3><p>如上所说，在同一个classloader中，利用如下的方法serializabale和deserializable对象：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ByteArrayOutputStream bo=new ByteArrayOutputStream();</span><br><span class="line">ObjectOutputStream oo=new ObjectOutputStream(bo);</span><br><span class="line">oo.writeObject(outObject);</span><br><span class="line">ByteArrayInputStream bi=new ByteArrayInputStream(bo.toByteArray());</span><br><span class="line">ObjectInputStream oi=new ObjectInputStream(bi);</span><br><span class="line">Object inObject = oi.readObject();</span><br></pre></td></tr></table></figure><p></p><p>当序列化的对象和反序列化的对象不在同一个classloader中时，以上的代码执行时，就会报无法把属性付给对象的错误，此时应当，通过设置反序列化得classloader，来解决这个问题。</p><p>首先，从ObjectInputStream继承一个自己的ObjectInputStream</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">public class CustomObjectInputStream extends ObjectInputStream &#123;</span><br><span class="line"></span><br><span class="line">    protected ClassLoader classLoader = this.getClass().getClassLoader();</span><br><span class="line"></span><br><span class="line">    public CustomObjectInputStream(InputStream in) throws IOException &#123;</span><br><span class="line">        super(in);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public CustomObjectInputStream(InputStream in, ClassLoader cl)</span><br><span class="line">            throws IOException &#123;</span><br><span class="line">        super(in);</span><br><span class="line">        this.classLoader = cl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected Class&lt;?&gt; resolveClass(ObjectStreamClass desc) throws IOException,</span><br><span class="line">            ClassNotFoundException &#123;</span><br><span class="line">        // TODO Auto-generated method stub</span><br><span class="line">        String name = desc.getName();</span><br><span class="line">        try &#123;</span><br><span class="line">            return Class.forName(name, false, this.classLoader);</span><br><span class="line">        &#125; catch (ClassNotFoundException ex) &#123;</span><br><span class="line">            return super.resolveClass(desc);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>比较重要的是这里的resolveClass方法传入classloader,反序列化时将classloader传入</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> ByteArrayOutputStream bo=new ByteArrayOutputStream();</span><br><span class="line">ObjectOutputStream oo=new ObjectOutputStream(bo);</span><br><span class="line">oo.writeObject(outObject);</span><br><span class="line">ByteArrayInputStream bi=new ByteArrayInputStream(bo.toByteArray());</span><br><span class="line">CustomObjectInputStream oi=new CustomObjectInputStream(bi, outObject.getClass().getClassLoader());</span><br><span class="line">Object = oi.readObject();</span><br><span class="line">// flink源码中也有多次类似的使用userClassloader和FlinkClassLoader的切换</span><br><span class="line">https://issues.apache.org/jira/browse/FLINK-9122 这个bug曾经就是因为这个原因引起的</span><br></pre></td></tr></table></figure><p></p><h3>序列化相关方法</h3><p>writeObject、readObject、readObjectNoData、writeReplace和readResolve 待补充</p><h2>参考</h2><ul><li><a href="https://help.semmle.com/wiki/display/JAVA/Serializable+inner+class+of+non-serializable+class," title="Serializable inner class of non-serializable class" target="_blank" rel="noopener">Serializable inner class of non-serializable class</a></li><li><a href="https://mp.weixin.qq.com/s/-D9N9_9IDqSbuIjuADJ7ZA" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/-D9N9_9IDqSbuIjuADJ7ZA</a></li><li><a href="https://howtodoinjava.com/core-java/serialization/how-deserialization-process-happen-in-java/" target="_blank" rel="noopener">https://howtodoinjava.com/core-java/serialization/how-deserialization-process-happen-in-java/</a></li><li><a href="https://www.quora.com/Why-are-enum-singleton-serialization-safe" target="_blank" rel="noopener">https://www.quora.com/Why-are-enum-singleton-serialization-safe</a></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;Java序列化拾掇&lt;/p&gt;&lt;p&gt;&amp;lt;!-- more --&amp;gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;本来想总结一下对google protob
    
    </summary>
    
      <category term="刨根问底" scheme="http://www.aitozi.com/categories/%E5%88%A8%E6%A0%B9%E9%97%AE%E5%BA%95/"/>
    
    
      <category term="serialization" scheme="http://www.aitozi.com/tags/serialization/"/>
    
  </entry>
  
  <entry>
    <title>flink中jobgraph的生成逻辑</title>
    <link href="http://www.aitozi.com/flink-jobgraph-generate.html"/>
    <id>http://www.aitozi.com/flink-jobgraph-generate.html</id>
    <published>2018-05-26T10:43:23.000Z</published>
    <updated>2018-05-27T12:15:46.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>flink中jobgraph的生成逻辑，接前面的文章<a href="http://aitozi.com/2018/04/11/flink-streamGraph/" target="_blank" rel="noopener">flink图流转之StreamGraph</a></p><p>&lt;!-- more --&gt;</p><blockquote><p>今天想了一下源码分析类的文章应该是直接在源码上做注释来的直接，然后再抛开细节概括总体流程和关键点，今天这篇分析jobgraph生成的文章就以这个形式展开</p></blockquote><ol><li>先生成各个节点streamnode的hash值 主体代码在：<code>StreamGraphHasherV2.java</code></li><li>设置chaining<ul><li>找到节点中能chain和不能chain的边</li><li>生成相应的JobVertex节点，并设置StreamConfig（资源，名称，chain的节点），这个streamConfig是在部署期间比较重要的一个配置项，并拼接物理执行顺序，主要在connect函数</li></ul></li><li>设置inEdges配置项</li><li>设置slotsharingGroup</li><li>配置checkpoint，这里主要设置需要发送barrier的节点即source节点</li></ol><p><em>其实总结就是分两步：</em></p><ol><li>在createChain过程中创建JobVertex</li><li>设置各个StreamConfig需要的信息用作生成物理执行图的时候使用</li></ol><p>主要涉及的代码为两块，如下：</p><p><script src="//gist.github.com/2e021e923ed394155b191853e0975a71.js?file=StreamGraphHasherV2.java"></script><script src="//gist.github.com/2e021e923ed394155b191853e0975a71.js?file=StreamingJobGraphGenerator.java"></script></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;flink中jobgraph的生成逻辑，接前面的文章&lt;a href=&quot;http://aitozi.com/2018/04/11/flink-strea
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink cep源码分析</title>
    <link href="http://www.aitozi.com/flink-cep-code.html"/>
    <id>http://www.aitozi.com/flink-cep-code.html</id>
    <published>2018-05-25T13:54:23.000Z</published>
    <updated>2019-03-14T17:03:52.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>关于Flink中cep实现原理的分析</p><p>&lt;!-- more --&gt;</p><blockquote><p>最近一直在搞动态cep的事情，有点焦头烂额很久没有更新博客了。其实有时候也在思考写博客的意义，因为写博客也是时间成本很高的一件事，如果得不到相应的收益其实是划不来的。那么写博客的收益到底是什么呢？两点：笔记记录和传播知识的作用，整理记录的功能一个web博客不会强于一个终端笔记例如：为知笔记。所以博客的真正意义在于传播知识观点。有时候你遇到一个百思不得其解的问题的时候，google一下找到一篇博客，竟然能够解答心中所惑的时候你是不是心中会很感谢博主呢，我认为这样的一篇文章就是有价值的文章，所以我希望我也能做好这样一件有价值的事情。</p></blockquote><h2>引言</h2><p>好了，进入本文的主题flink cep原理的深入理解，很多人可能还不知道flink cep是什么，flink cep其实实现自一篇论文，具体论文细节见我之前的一篇文章的分享<a href="http://aitozi.com/2018/02/25/flink-cep-paper/" target="_blank" rel="noopener">flink-cep-paper</a>. flink cep的全称是Complex Event Processing，在我看来它主要能做的是在一个连续不断的事件中提取出用户所关心的事件序列，他和flink的filter算子的区别在于filter只能去实现单个元素的过滤，而cep是能完成先后顺序事件的过滤。下面让我们来走进他的源码实现原理吧。以下代码基于社区1.4.2分支分析。</p><p>我们的文章以一系列问题来展开：</p><ol><li>用户定义的Pattern最后会以什么形式工作</li><li>当CEP Operator获取到上游一个算子的时候会做什么事情？</li><li>在ProcessingTime和Eventtime的语义下处理逻辑有什么不同点？</li><li>匹配成功的元素如何存储，状态机转化流程是怎么样的？</li><li>超时未匹配成功的元素会做什么？</li></ol><h2>问题一</h2><p>用户在定义Pattern和condition之后，会通过NFAcompiler将Pattern翻译成一个一个相关联的State，表明了这一组规则的状态机的走向流程。</p><p>State包括<code>Start、Normal、Final、Stop</code>，start表示一个起始状态例如<code>begin('A').followedBy('B')</code> 这里面A就是一个Start状态。Final状态表示整个序列已经匹配完成可以向下游发送了，Stop状态是用来处理否定类型的规则，一旦到达Stop状态即意味着整个匹配过程失败。各个状态之间通过<code>StateTransition</code>来连接，连接方式有：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ignore: 忽略此次匹配的元素</span><br><span class="line">proceed: 相当于forward的意思，到达下一个状态，不存储元素，继续做下一个状态的condition判断</span><br><span class="line">take： 存储本次的元素</span><br></pre></td></tr></table></figure><p></p><p>这是一段创建中间状态的代码</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">private State&lt;T&gt; createMiddleStates(final State&lt;T&gt; sinkState) &#123;</span><br><span class="line">			State&lt;T&gt; lastSink = sinkState;</span><br><span class="line">			// 不断往上遍历pattern进行state的生成</span><br><span class="line">			while (currentPattern.getPrevious() != null) &#123;</span><br><span class="line"></span><br><span class="line">				if (currentPattern.getQuantifier().getConsumingStrategy() == Quantifier.ConsumingStrategy.NOT_FOLLOW) &#123;</span><br><span class="line">					//skip notFollow patterns, they are converted into edge conditions</span><br><span class="line">				&#125; else if (currentPattern.getQuantifier().getConsumingStrategy() == Quantifier.ConsumingStrategy.NOT_NEXT) &#123;</span><br><span class="line">					final State&lt;T&gt; notNext = createState(currentPattern.getName(), State.StateType.Normal);</span><br><span class="line">					final IterativeCondition&lt;T&gt; notCondition = getTakeCondition(currentPattern);</span><br><span class="line">					// 否定类型的pattern需要创建一个stop state</span><br><span class="line">					final State&lt;T&gt; stopState = createStopState(notCondition, currentPattern.getName());</span><br><span class="line"></span><br><span class="line">					if (lastSink.isFinal()) &#123;</span><br><span class="line">						//so that the proceed to final is not fired</span><br><span class="line">						结尾状态不用proceed过去做下一次计算了，可以直接ignore到Final，然后输出结果</span><br><span class="line">						notNext.addIgnore(lastSink, new NotCondition&lt;&gt;(notCondition));</span><br><span class="line">					&#125; else &#123;</span><br><span class="line">						notNext.addProceed(lastSink, new NotCondition&lt;&gt;(notCondition));</span><br><span class="line">					&#125;</span><br><span class="line">					// 在满足Not_NEXT的条件的时候就转化成stop状态即匹配失败</span><br><span class="line">					notNext.addProceed(stopState, notCondition);</span><br><span class="line">					lastSink = notNext;</span><br><span class="line">				&#125; else &#123;</span><br><span class="line">					// 非否定类型的状态的处理逻辑都在这个方法中</span><br><span class="line">					lastSink = convertPattern(lastSink);</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				// we traverse the pattern graph backwards</span><br><span class="line">				followingPattern = currentPattern;</span><br><span class="line">				currentPattern = currentPattern.getPrevious();</span><br><span class="line"></span><br><span class="line">				final Time currentWindowTime = currentPattern.getWindowTime();</span><br><span class="line">				if (currentWindowTime != null &amp;&amp; currentWindowTime.toMilliseconds() &lt; windowTime) &#123;</span><br><span class="line">					// the window time is the global minimum of all window times of each state</span><br><span class="line">					windowTime = currentWindowTime.toMilliseconds();</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">			return lastSink;</span><br><span class="line">		&#125;</span><br></pre></td></tr></table></figure><p></p><p>生成这样的state列表之后，最终会创建一个NFA，一个NFA中包含了两个重要组件： 一个是SharedBuffer用于存储中间匹配命中的数据，这是一个基于论文实现的带版本的内存共享，主要解决的事情是在同一个元素触发多个分支的时候避免存储多次。 另一个是ComputationState队列表示的是一系列当前匹配到的计算状态，每一个状态在拿到下一个元素的时候都会根据condition判断自己是能够继续往下匹配生成下一个computation state还是匹配失败。</p><h2>问题二、三</h2><p>问题二和问题三一起解释，在消费到上游一个元素之后会判断时间语义，这里主要是为了处理乱序问题，如果是processingtime的话就会直接经由nfa#process进行处理，因为processing time不需要考虑事件是否乱序，他给每个事件都打上了当前的时间戳。而event语义下，会先将该数据buffer到rocksdb中，并且注册一个比当前时间戳大1的eventimer，用以触发真正的计算，也就是说，eventtime其实是每毫秒获取过去存储的数据做一次匹配计算。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">protected void saveRegisterWatermarkTimer() &#123;</span><br><span class="line">	long currentWatermark = timerService.currentWatermark();</span><br><span class="line">	// protect against overflow</span><br><span class="line">	if (currentWatermark + 1 &gt; currentWatermark) &#123;</span><br><span class="line">		timerService.registerEventTimeTimer(VoidNamespace.INSTANCE, currentWatermark + 1);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h2>问题四、五</h2><p>nfa#process做了什么？取出前面说到的nfa中所有的当前computationState去做计算，当然计算之前会先判断时间和computation的starttime比较匹配是否超出时间，即within算子所做的时间，如果设置了超时处理的方式，就会将超时未匹配完成，已匹配到的部分元素向下游发送，并做sharebuffer的清理工作</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">if (!computationState.isStartState() &amp;&amp;</span><br><span class="line">	windowTime &gt; 0L &amp;&amp;</span><br><span class="line">	timestamp - computationState.getStartTimestamp() &gt;= windowTime) &#123;</span><br><span class="line"></span><br><span class="line">	if (handleTimeout) &#123;</span><br><span class="line">		// extract the timed out event pattern</span><br><span class="line">		Map&lt;String, List&lt;T&gt;&gt; timedOutPattern = extractCurrentMatches(computationState);</span><br><span class="line">		timeoutResult.add(Tuple2.of(timedOutPattern, timestamp));</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	eventSharedBuffer.release(</span><br><span class="line">			NFAStateNameHandler.getOriginalNameFromInternal(computationState.getPreviousState().getName()),</span><br><span class="line">			computationState.getEvent(),</span><br><span class="line">			computationState.getTimestamp(),</span><br><span class="line">			computationState.getCounter());</span><br><span class="line"></span><br><span class="line">	newComputationStates = Collections.emptyList();</span><br><span class="line">	nfaChanged = true;</span><br><span class="line">&#125; else if (event != null) &#123;</span><br><span class="line">   // 在computeNextState的时候判断成功的take条件会将元素put到eventSharedBuffer中</span><br><span class="line">	newComputationStates = computeNextStates(computationState, event, timestamp);</span><br><span class="line"></span><br><span class="line">	if (newComputationStates.size() != 1) &#123;</span><br><span class="line">		nfaChanged = true;</span><br><span class="line">	&#125; else if (!newComputationStates.iterator().next().equals(computationState)) &#123;</span><br><span class="line">		nfaChanged = true;</span><br><span class="line">	&#125;</span><br><span class="line">&#125; else &#123;</span><br><span class="line">	newComputationStates = Collections.singleton(computationState);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>在完成匹配之后达到final状态将数据提取出来向下游发送完成匹配。</p><p>以上便是cep的大致原理，说白了其实这个就是基于flink runntime开发出来的一个衍生lib，flink runtime其实是一个分布式的阻塞队列，通过这个概念可以在上面开发出很多有意思的产品，cep就是其中一个。 分析结束，欢迎拍砖~</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;关于Flink中cep实现原理的分析&lt;/p&gt;&lt;p&gt;&amp;lt;!-- more --&amp;gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;最近一直在搞动态cep的
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
      <category term="CEP" scheme="http://www.aitozi.com/tags/CEP/"/>
    
  </entry>
  
  <entry>
    <title>flink图流转之StreamGraph</title>
    <link href="http://www.aitozi.com/flink-streamGraph.html"/>
    <id>http://www.aitozi.com/flink-streamGraph.html</id>
    <published>2018-04-10T23:13:58.000Z</published>
    <updated>2018-04-10T23:19:36.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>flink DAG图流转分析 &lt;!-- more --&gt;</p><h2>前言</h2><p>每次我们编写完flink作业，跑任务的时候都会在flink-ui上展示一个作业的DAG图，那么这个图是如何形成的呢？本文就和你一起来揭开flink执行图生成的神秘面纱~</p><p>##总览 在flink中的执行图可以分为4层StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图。</p><ul><li>StreamGraph：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。</li><li>JobGraph：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。</li><li>ExecutionGraph：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。</li><li>物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</li></ul><p>今天我们就来看下streamGraph的生成</p><h2>StreamGraph的生成</h2><h3>组件</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">StreamGraph：根据用户通过 Stream API 编写的代码生成的最初的图。</span><br><span class="line">StreamNode：用来代表 operator 的类，并具有所有相关的属性，如并发度、入边和出边等。</span><br><span class="line">StreamEdge：表示连接两个StreamNode的边。</span><br></pre></td></tr></table></figure><p></p><p>flink任务从定义一个运行环境开始<code>streamExecutionEnvironment</code>，流计算任务起始于addSource,我们来看这个函数，addSource之后生成了一个<code>DataStream</code>. <code>DataStream</code>的构造函数参数接收一个<code>StreamTransformation</code>类型的对象，这个对象反映了流之间的转换操作。但是这个transformation和<code>operation</code>不是一一对应的。一些分区操作：union，split/select，partition只是逻辑概念，并不会在最后的dag图上显示出来。 在生成datastream之后，经历<code>DataStream.java</code>中定义的一些api算子，完成业务逻辑的定义，在这之中可能包含以下的转化：</p><p>假设一个场景：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">addsource -&gt; map -&gt; filter -&gt; connect -&gt; flatmap </span><br><span class="line">-&gt; keyby -&gt; window -&gt; apply -&gt; addSink -&gt; excute</span><br></pre></td></tr></table></figure><p></p><ol><li>addSource创建生成一个<code>SingleOutputStreamOperator</code> 本质上是一个带有transformation=&quot;SourceTransformation&quot;的datastream</li><li>map创建生成一个<code>OneInputTransformation</code> 并调用getExecutionEnvironment().addOperator(resultTransform)将其添加入env的<code>List&lt;StreamTransformation&lt;?&gt;&gt;</code>中</li><li>filter 通过 map相同操作</li><li>connect 直接返回一个<code>ConnectedStreams</code>不是<code>Datastream</code>的子类</li><li>flatmap 生成一个<code>TwoInputTransformation</code>将其添加入env的<code>List&lt;StreamTransformation&lt;?&gt;&gt;</code>中，并返回一个<code>SingleOutputStreamOperator</code>,并且返回的Datastream中包含的是当前这个transformation</li><li>keyby 生成一个keyedStream，这里直接生成一个<code>PartitionTransformation</code> 替代了父类<code>DataStream</code>中的transformation</li><li>window 生成windowStream</li><li>apply调用将生成一个<code>OneInputTransformation</code>,增加至<code>List&lt;StreamTransformation&lt;?&gt;&gt;</code></li><li>addSink 获取 <code>SinkTransformation</code></li></ol><p>其中每一次创建<code>OneInputTransformation</code>都是基于Datastream的当前的transformation来创建的，也就是说keyby之后的PartitionTransformation信息也加入了.</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">new OneInputTransformation&lt;&gt;(</span><br><span class="line">			this.transformation,</span><br><span class="line">			operatorName,</span><br><span class="line">			operator,</span><br><span class="line">			outTypeInfo,</span><br><span class="line">			environment.getParallelism());</span><br></pre></td></tr></table></figure><p></p><p>好了到这里已经获取了各个流程的streamtransformation,最后调用execute方法，截取了流式环境下的实现：</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public JobExecutionResult execute(String jobName) throws Exception &#123;</span><br><span class="line">	Preconditions.checkNotNull(&quot;Streaming Job name should not be null.&quot;);</span><br><span class="line"></span><br><span class="line">	StreamGraph streamGraph = this.getStreamGraph();</span><br><span class="line">	streamGraph.setJobName(jobName);</span><br><span class="line"></span><br><span class="line">	transformations.clear();</span><br><span class="line"></span><br><span class="line">	// execute the programs</span><br><span class="line">	if (ctx instanceof DetachedEnvironment) &#123;</span><br><span class="line">		LOG.warn(&quot;Job was executed in detached mode, the results will be available on completion.&quot;);</span><br><span class="line">		((DetachedEnvironment) ctx).setDetachedPlan(streamGraph);</span><br><span class="line">		return DetachedEnvironment.DetachedJobExecutionResult.INSTANCE;</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		return ctx</span><br><span class="line">			.getClient()</span><br><span class="line">			.run(streamGraph, ctx.getJars(), ctx.getClasspaths(), ctx.getUserCodeClassLoader(), ctx.getSavepointRestoreSettings())</span><br><span class="line">			.getJobExecutionResult();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>其实主要调用的就是</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StreamGraphGenerator.generate(this, transformations);</span><br></pre></td></tr></table></figure><p></p><p>每一个<code>OneInputTransformation</code>都会记录他的上游的input的transformation，在<code>StreamGraphGenerator.generate</code>主要针对不同的transformation进行不同的转化</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> Collection&lt;Integer&gt; <span class="title">transform</span><span class="params">(StreamTransformation&lt;?&gt; transform)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">			<span class="keyword">return</span> alreadyTransformed.get(transform);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		LOG.debug(<span class="string">"Transforming "</span> + transform);</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (transform.getMaxParallelism() &lt;= <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// if the max parallelism hasn't been set, then first use the job wide max parallelism</span></span><br><span class="line">			<span class="comment">// from theExecutionConfig.</span></span><br><span class="line">			<span class="keyword">int</span> globalMaxParallelismFromConfig = env.getConfig().getMaxParallelism();</span><br><span class="line">			<span class="keyword">if</span> (globalMaxParallelismFromConfig &gt; <span class="number">0</span>) &#123;</span><br><span class="line">				transform.setMaxParallelism(globalMaxParallelismFromConfig);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// call at least once to trigger exceptions about MissingTypeInfo</span></span><br><span class="line">		transform.getOutputType();</span><br><span class="line"></span><br><span class="line">		Collection&lt;Integer&gt; transformedIds;</span><br><span class="line">		<span class="keyword">if</span> (transform <span class="keyword">instanceof</span> OneInputTransformation&lt;?, ?&gt;) &#123;</span><br><span class="line">			transformedIds = transformOneInputTransform((OneInputTransformation&lt;?, ?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> TwoInputTransformation&lt;?, ?, ?&gt;) &#123;</span><br><span class="line">			transformedIds = transformTwoInputTransform((TwoInputTransformation&lt;?, ?, ?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SourceTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSource((SourceTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SinkTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSink((SinkTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> UnionTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformUnion((UnionTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SplitTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSplit((SplitTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SelectTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSelect((SelectTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> FeedbackTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformFeedback((FeedbackTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> CoFeedbackTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformCoFeedback((CoFeedbackTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> PartitionTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformPartition((PartitionTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SideOutputTransformation&lt;?&gt;) &#123;</span><br><span class="line">			transformedIds = transformSideOutput((SideOutputTransformation&lt;?&gt;) transform);</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Unknown transformation: "</span> + transform);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// need this check because the iterate transformation adds itself before</span></span><br><span class="line">		<span class="comment">// transforming the feedback edges</span></span><br><span class="line">		<span class="keyword">if</span> (!alreadyTransformed.containsKey(transform)) &#123;</span><br><span class="line">			alreadyTransformed.put(transform, transformedIds);</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (transform.getBufferTimeout() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">			streamGraph.setBufferTimeout(transform.getId(), transform.getBufferTimeout());</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (transform.getUid() != <span class="keyword">null</span>) &#123;</span><br><span class="line">			streamGraph.setTransformationUID(transform.getId(), transform.getUid());</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">if</span> (transform.getUserProvidedNodeHash() != <span class="keyword">null</span>) &#123;</span><br><span class="line">			streamGraph.setTransformationUserHash(transform.getId(), transform.getUserProvidedNodeHash());</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (transform.getMinResources() != <span class="keyword">null</span> &amp;&amp; transform.getPreferredResources() != <span class="keyword">null</span>) &#123;</span><br><span class="line">			streamGraph.setResources(transform.getId(), transform.getMinResources(), transform.getPreferredResources());</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> transformedIds;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p>可以看到他里面的方法都是递归调用<code>transform(input)</code>方法，然后通过<code>alreadyTransformed</code>数据结构，避免重复计算，所以我们最终看的时候最先是从source处进行的，也就是从上游到下游进行转化</p><ol><li>如果已经在<code>alreadyTransformed</code>数据结构中那么就直接返回transformation的id</li><li>分别有addSource，addOperator，addSink，addCoOperator，addEdge的不同操作来生成streamGraph中的不同节点</li><li>addEdge建立每一个transformation和他所有上游输入节点的连线</li></ol><p>在streamgraph中还建立了几个虚拟的节点，这几个节点主要针对的是partition，split/select，sideoutput的操作。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">private Map&lt;Integer, Tuple2&lt;Integer, List&lt;String&gt;&gt;&gt; virtualSelectNodes;</span><br><span class="line">private Map&lt;Integer, Tuple2&lt;Integer, OutputTag&gt;&gt; virtualSideOutputNodes;</span><br><span class="line">private Map&lt;Integer, Tuple2&lt;Integer, StreamPartitioner&lt;?&gt;&gt;&gt; virtualPartitionNodes;</span><br></pre></td></tr></table></figure><p></p><p>在进行这些操作时，会添加一个唯一的虚拟节点</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//记录了上游某个transformId到下游的partition方式</span></span><br><span class="line">virtualPartitionNodes.put(virtualId, <span class="keyword">new</span> Tuple2&lt;Integer, StreamPartitioner&lt;?&gt;&gt;(originalId, partitioner));  </span><br><span class="line"><span class="comment">//记录上游的不同outputTag，用以将部分数据从该tag输出</span></span><br><span class="line">virtualSideOutputNodes.put(virtualId, <span class="keyword">new</span> Tuple2&lt;&gt;(originalId, outputTag));</span><br><span class="line"><span class="comment">//记录一个上游的select虚拟节点</span></span><br><span class="line">virtualSelectNodes.put(virtualId, <span class="keyword">new</span> Tuple2&lt;Integer, List&lt;String&gt;&gt;(originalId, selectedNames));</span><br></pre></td></tr></table></figure><p></p><p>经过一些列的<code>addNode</code>以及<code>addEdge</code>之后，streamGraph已经生成。关于其他几个graph的生成请听下回的分解</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;flink DAG图流转分析 &amp;lt;!-- more --&amp;gt;&lt;/p&gt;&lt;h2&gt;前言&lt;/h2&gt;&lt;p&gt;每次我们编写完flink作业，跑任务的时候都会
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink-ui中的反压采样</title>
    <link href="http://www.aitozi.com/flink-backpressure-sample.html"/>
    <id>http://www.aitozi.com/flink-backpressure-sample.html</id>
    <published>2018-04-10T23:02:51.000Z</published>
    <updated>2018-04-10T23:08:08.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>flink反压值采样计算原理</p><p>&lt;!-- more --&gt;</p><p>我们在点击flink ui的<code>operator-&gt;backpressure</code>之后，会触发Backpressure采样：每隔<code>BACK_PRESSURE_REFRESH_INTERVAL</code>的间隔进行一次采样。</p><h3>BackPressureStatsTracker#triggerStackTraceSample</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Triggers a stack trace sample for a operator to gather the back pressure</span></span><br><span class="line"><span class="comment"> * statistics. If there is a sample in progress for the operator, the call</span></span><br><span class="line"><span class="comment"> * is ignored.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> vertex Operator to get the stats for. 代表要采样的Operator节点</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> Flag indicating whether a sample with triggered.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">triggerStackTraceSample</span><span class="params">(ExecutionJobVertex vertex)</span> </span>&#123;</span><br><span class="line">	<span class="comment">// 保证没有并行triggerSimple</span></span><br><span class="line">	<span class="keyword">synchronized</span> (lock) &#123;</span><br><span class="line">		<span class="keyword">if</span> (shutDown) &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">//排除掉已经pending在采样和结束的Operator</span></span><br><span class="line">		<span class="keyword">if</span> (!pendingStats.contains(vertex) &amp;&amp;</span><br><span class="line">				!vertex.getGraph().getState().isGloballyTerminalState()) &#123;</span><br><span class="line">				</span><br><span class="line">			<span class="comment">// 拿到对应ExecutionGraph的FutureExecutor，这是用以在相应的ExecutionGraph发起任务的</span></span><br><span class="line">			Executor executor = vertex.getGraph().getFutureExecutor();</span><br><span class="line"></span><br><span class="line">			<span class="comment">// Only trigger if still active job</span></span><br><span class="line">			<span class="keyword">if</span> (executor != <span class="keyword">null</span>) &#123;</span><br><span class="line">				pendingStats.add(vertex);</span><br><span class="line"></span><br><span class="line">				<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">					LOG.debug(<span class="string">"Triggering stack trace sample for tasks: "</span> + Arrays.toString(vertex.getTaskVertices()));</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="comment">// 核心方法 通过StackTraceSampleCoordinator 去发起相应的trigger流程, 这里的Future是Flink内部自己定义的异步结果接口 具体可查看FlinkFuture.java(可以深入了解下)</span></span><br><span class="line">				Future&lt;StackTraceSample&gt; sample = coordinator.triggerStackTraceSample(</span><br><span class="line">						vertex.getTaskVertices(),</span><br><span class="line">						numSamples,</span><br><span class="line">						delayBetweenSamples,</span><br><span class="line">						MAX_STACK_TRACE_DEPTH);</span><br><span class="line">				<span class="comment">// 指定异步回调函数（采样结果分析的函数）</span></span><br><span class="line">				sample.handleAsync(<span class="keyword">new</span> StackTraceSampleCompletionCallback(vertex), executor);</span><br><span class="line"></span><br><span class="line">				<span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>StackTraceSampleCoordinator#triggerStackTraceSample</h3><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">	 * Triggers a stack trace sample to all tasks.</span><br><span class="line">	 *</span><br><span class="line">	 * @param tasksToSample       Tasks to sample.</span><br><span class="line">	 * @param numSamples          Number of stack trace samples to collect.</span><br><span class="line">	 * @param delayBetweenSamples Delay between consecutive samples.</span><br><span class="line">	 * @param maxStackTraceDepth  Maximum depth of the stack trace. 0 indicates</span><br><span class="line">	 *                            no maximum and keeps the complete stack trace.</span><br><span class="line">	 * @return A future of the completed stack trace sample</span><br><span class="line">	 */</span><br><span class="line">	@SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">	public Future&lt;StackTraceSample&gt; triggerStackTraceSample(</span><br><span class="line">			ExecutionVertex[] tasksToSample,</span><br><span class="line">			int numSamples,</span><br><span class="line">			Time delayBetweenSamples,</span><br><span class="line">			int maxStackTraceDepth) &#123;</span><br><span class="line"></span><br><span class="line">		checkNotNull(tasksToSample, &quot;Tasks to sample&quot;);</span><br><span class="line">		checkArgument(tasksToSample.length &gt;= 1, &quot;No tasks to sample&quot;);</span><br><span class="line">		checkArgument(numSamples &gt;= 1, &quot;No number of samples&quot;);</span><br><span class="line">		checkArgument(maxStackTraceDepth &gt;= 0, &quot;Negative maximum stack trace depth&quot;);</span><br><span class="line"></span><br><span class="line">		// 通过ExecutionVertex获取ExecutionAttemptID和Execution，并最后做存活判断</span><br><span class="line">		// Execution IDs of running tasks</span><br><span class="line">		ExecutionAttemptID[] triggerIds = new ExecutionAttemptID[tasksToSample.length];</span><br><span class="line">		Execution[] executions = new Execution[tasksToSample.length];</span><br><span class="line"></span><br><span class="line">		// Check that all tasks are RUNNING before triggering anything. The</span><br><span class="line">		// triggering can still fail.</span><br><span class="line">		for (int i = 0; i &lt; triggerIds.length; i++) &#123;</span><br><span class="line">			Execution execution = tasksToSample[i].getCurrentExecutionAttempt();</span><br><span class="line">			if (execution != null &amp;&amp; execution.getState() == ExecutionState.RUNNING) &#123;</span><br><span class="line">				executions[i] = execution;</span><br><span class="line">				triggerIds[i] = execution.getAttemptId();</span><br><span class="line">			&#125; else &#123;</span><br><span class="line">				return FlinkCompletableFuture.completedExceptionally(</span><br><span class="line">					new IllegalStateException(&quot;Task &quot; + tasksToSample[i]</span><br><span class="line">					.getTaskNameWithSubtaskIndex() + &quot; is not running.&quot;));</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		synchronized (lock) &#123;</span><br><span class="line">			if (isShutDown) &#123;</span><br><span class="line">				return FlinkCompletableFuture.completedExceptionally(new IllegalStateException(&quot;Shut down&quot;));</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			final int sampleId = sampleIdCounter++;</span><br><span class="line"></span><br><span class="line">			LOG.debug(&quot;Triggering stack trace sample &#123;&#125;&quot;, sampleId);</span><br><span class="line">			</span><br><span class="line">			// 包含采样id和ExecutionAttemptID</span><br><span class="line">			final PendingStackTraceSample pending = new PendingStackTraceSample(</span><br><span class="line">					sampleId, triggerIds);</span><br><span class="line"></span><br><span class="line">			// Discard the sample if it takes too long. We don&apos;t send cancel</span><br><span class="line">			// messages to the task managers, but only wait for the responses</span><br><span class="line">			// and then ignore them.</span><br><span class="line">			long expectedDuration = numSamples * delayBetweenSamples.toMilliseconds();</span><br><span class="line">			Time timeout = Time.milliseconds(expectedDuration + sampleTimeout);</span><br><span class="line"></span><br><span class="line">			// Add the pending sample before scheduling the discard task to</span><br><span class="line">			// prevent races with removing it again.</span><br><span class="line">			pendingSamples.put(sampleId, pending);</span><br><span class="line"></span><br><span class="line">			// Trigger all samples</span><br><span class="line">			// execution是executionVertex的多次执行（recovery...）</span><br><span class="line">			for (Execution execution: executions) &#123;</span><br><span class="line">			    // 对相应的execution进行多次numSamples采样，但是都是同一个sampleId</span><br><span class="line">				final Future&lt;StackTraceSampleResponse&gt; stackTraceSampleFuture = execution.requestStackTraceSample(</span><br><span class="line">					sampleId,</span><br><span class="line">					numSamples,</span><br><span class="line">					delayBetweenSamples,</span><br><span class="line">					maxStackTraceDepth,</span><br><span class="line">					timeout);</span><br><span class="line"></span><br><span class="line">				stackTraceSampleFuture.handleAsync(new BiFunction&lt;StackTraceSampleResponse, Throwable, Void&gt;() &#123;</span><br><span class="line">					@Override</span><br><span class="line">					public Void apply(StackTraceSampleResponse stackTraceSampleResponse, Throwable throwable) &#123;</span><br><span class="line">						if (stackTraceSampleResponse != null) &#123;</span><br><span class="line">						    // 收集返回的List&lt;StackTraceElement[]&gt; 到PendingStackTraceSample</span><br><span class="line">							collectStackTraces(</span><br><span class="line">								stackTraceSampleResponse.getSampleId(),</span><br><span class="line">								stackTraceSampleResponse.getExecutionAttemptID(),</span><br><span class="line">								// 返回的堆栈信息包含所有的采样结果</span><br><span class="line">								stackTraceSampleResponse.getSamples());</span><br><span class="line">						&#125; else &#123;</span><br><span class="line">							cancelStackTraceSample(sampleId, throwable);</span><br><span class="line">						&#125;</span><br><span class="line"></span><br><span class="line">						return null;</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;, executor);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			return pending.getStackTraceSampleFuture();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><h3>BackPressureStatsTracker#StackTraceSampleCompletionCallback</h3><p>采样的结果就是<code>StackTraceSample</code></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">* java.lang.Object.wait(Native Method)</span><br><span class="line">* o.a.f.[...].LocalBufferPool.requestBuffer(LocalBufferPool.java:163)</span><br><span class="line">* o.a.f.[...].LocalBufferPool.requestBufferBlocking(LocalBufferPool.java:133) &lt;--- BLOCKING</span><br><span class="line">* request</span><br><span class="line">* [...]</span><br></pre></td></tr></table></figure><p></p><p>利用这样的线程堆栈类型来判断是否block住了。</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">		 * Creates the back pressure stats from a stack trace sample.</span><br><span class="line">		 *</span><br><span class="line">		 * @param sample Stack trace sample to base stats on.</span><br><span class="line">		 *</span><br><span class="line">		 * @return Back pressure stats</span><br><span class="line">		 */</span><br><span class="line">		private OperatorBackPressureStats createStatsFromSample(StackTraceSample sample) &#123;</span><br><span class="line">			Map&lt;ExecutionAttemptID, List&lt;StackTraceElement[]&gt;&gt; traces = sample.getStackTraces();</span><br><span class="line"></span><br><span class="line">			// Map task ID to subtask index, because the web interface expects</span><br><span class="line">			// it like that.</span><br><span class="line">			// 方便下面根据executionId查询相应的并发度</span><br><span class="line">			Map&lt;ExecutionAttemptID, Integer&gt; subtaskIndexMap = Maps</span><br><span class="line">					.newHashMapWithExpectedSize(traces.size());</span><br><span class="line"></span><br><span class="line">			Set&lt;ExecutionAttemptID&gt; sampledTasks = sample.getStackTraces().keySet();</span><br><span class="line"></span><br><span class="line">			for (ExecutionVertex task : vertex.getTaskVertices()) &#123;</span><br><span class="line">				ExecutionAttemptID taskId = task.getCurrentExecutionAttempt().getAttemptId();</span><br><span class="line">				if (sampledTasks.contains(taskId)) &#123;</span><br><span class="line">					subtaskIndexMap.put(taskId, task.getParallelSubtaskIndex());</span><br><span class="line">				&#125; else &#123;</span><br><span class="line">					LOG.debug(&quot;Outdated sample. A task, which is part of the &quot; +</span><br><span class="line">							&quot;sample has been reset.&quot;);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			// Ratio of blocked samples to total samples per sub task. Array</span><br><span class="line">			// position corresponds to sub task index.</span><br><span class="line">			// 数组的index和task的并发度相绑定</span><br><span class="line">			double[] backPressureRatio = new double[traces.size()];</span><br><span class="line"></span><br><span class="line">			for (Entry&lt;ExecutionAttemptID, List&lt;StackTraceElement[]&gt;&gt; entry : traces.entrySet()) &#123;</span><br><span class="line">				int backPressureSamples = 0;</span><br><span class="line"></span><br><span class="line">				List&lt;StackTraceElement[]&gt; taskTraces = entry.getValue();</span><br><span class="line"></span><br><span class="line">				for (StackTraceElement[] trace : taskTraces) &#123;</span><br><span class="line">					for (int i = trace.length - 1; i &gt;= 0; i--) &#123;</span><br><span class="line">						StackTraceElement elem = trace[i];</span><br><span class="line"></span><br><span class="line">						if (elem.getClassName().equals(EXPECTED_CLASS_NAME) &amp;&amp;</span><br><span class="line">								elem.getMethodName().equals(EXPECTED_METHOD_NAME)) &#123;</span><br><span class="line"></span><br><span class="line">							backPressureSamples++;</span><br><span class="line">							break; // Continue with next stack trace</span><br><span class="line">						&#125;</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				int subtaskIndex = subtaskIndexMap.get(entry.getKey());</span><br><span class="line"></span><br><span class="line">				int size = taskTraces.size();</span><br><span class="line">				double ratio = (size &gt; 0)</span><br><span class="line">						? ((double) backPressureSamples) / size</span><br><span class="line">						: 0;</span><br><span class="line"></span><br><span class="line">				backPressureRatio[subtaskIndex] = ratio;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			return new OperatorBackPressureStats(</span><br><span class="line">					sample.getSampleId(),</span><br><span class="line">					sample.getEndTime(),</span><br><span class="line">					backPressureRatio);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p>至此完成采样</p><p>待学习</p><ol><li>Flink反压原理</li><li>理解清里面很多的Future使用方法</li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;flink反压值采样计算原理&lt;/p&gt;&lt;p&gt;&amp;lt;!-- more --&amp;gt;&lt;/p&gt;&lt;p&gt;我们在点击flink ui的&lt;code&gt;operator
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>maven-shade-plugin插件高级用法</title>
    <link href="http://www.aitozi.com/advance-maven-shade-plugin.html"/>
    <id>http://www.aitozi.com/advance-maven-shade-plugin.html</id>
    <published>2018-04-10T14:06:03.000Z</published>
    <updated>2019-03-14T16:58:55.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>工作中的maven-shade-plugin插件高级用法小记</p><p>&lt;!-- more --&gt;</p><h3>问题背景</h3><p>flink-table 库中使用了org.apache.calcite 1.12版本，我在开发flink-cep的过程中引入了org.apache-calcite 1.6版本，服务端要同时引入这两个版本的包，如何解决这个版本冲突问题呢？</p><h3>maven-shade-plugin</h3><p>使用maven-shade-plugin的relocate功能可以将包名打包成一个别名</p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&lt;plugin&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">    &lt;executions&gt;</span><br><span class="line">        &lt;execution&gt;</span><br><span class="line">            &lt;goals&gt;</span><br><span class="line">                &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">            &lt;/goals&gt;</span><br><span class="line">			&lt;configuration&gt;</span><br><span class="line">				&lt;filters&gt;</span><br><span class="line">					&lt;filter&gt;</span><br><span class="line">						&lt;artifact&gt;*:*&lt;/artifact&gt;</span><br><span class="line">						&lt;excludes&gt;</span><br><span class="line">							&lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;</span><br><span class="line">							&lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;</span><br><span class="line">							&lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;</span><br><span class="line">						&lt;/excludes&gt;</span><br><span class="line">					&lt;/filter&gt;</span><br><span class="line">				&lt;/filters&gt;</span><br><span class="line">				&lt;relocations&gt;</span><br><span class="line">				   &lt;!--通过relocation配置将包名进行了替换--&gt;</span><br><span class="line">					&lt;relocation&gt;</span><br><span class="line">						&lt;pattern&gt;org.apache.calcite&lt;/pattern&gt;</span><br><span class="line">						&lt;shadedPattern&gt;org.apache.flink.shaded.cep.calcite&lt;/shadedPattern&gt;</span><br><span class="line">					&lt;/relocation&gt;</span><br><span class="line">				&lt;/relocations&gt;</span><br><span class="line">				&lt;transformers&gt;</span><br><span class="line">					&lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer&quot;/&gt;</span><br><span class="line">				&lt;/transformers&gt;</span><br><span class="line">				&lt;!-- Additional configuration. --&gt;</span><br><span class="line">			&lt;/configuration&gt;</span><br><span class="line">        &lt;/execution&gt;</span><br><span class="line">    &lt;/executions&gt;</span><br><span class="line">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure><p></p><p>看起来一切都ok了，不过思考一个问题，maven-shade-plugin插件修改包名后项目中引用的包名怎么找到的呢？Class.forName(&quot;xxxx.xxx.xxx&quot;)去生成类的方式还奏不奏效呢？Stack Overflow上关于shade插件的描述也提到了对这种类加载的方式是否奏效的疑问</p><p>然而当应用一跑起来之后发现，并不奏效！！！报出异常 <code>No suitable driver found for jdbc:calcite</code> 也就是<code>DriverManager</code>生成相应的driver的时候没找到相应的类。</p><p><strong>相关的issue</strong></p><p><a href="https://github.com/xerial/sqlite-jdbc/issues/145" target="_blank" rel="noopener">https://github.com/xerial/sqlite-jdbc/issues/145</a></p><p>在里面我们看到sqlite数据库有这样的配置文件指定了相应driver的类名</p><p><a href="https://github.com/xerial/sqlite-jdbc/blob/master/src/main/resources/java.sql.Driver" target="_blank" rel="noopener">https://github.com/xerial/sqlite-jdbc/blob/master/src/main/resources/java.sql.Driver</a></p><p>查看calcite也有相应的配置文件指定</p><p><a href="https://github.com/apache/calcite/blob/master/core/src/main/resources/META-INF/services/java.sql.Driver" target="_blank" rel="noopener">https://github.com/apache/calcite/blob/master/core/src/main/resources/META-INF/services/java.sql.Driver</a></p><p>那是不是在shade的过程中将这个文件内容也进行相应的变更就可以了呢? shade也确实有这样的transformation</p><p><a href="http://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#ServicesResourceTransformer" target="_blank" rel="noopener">http://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#ServicesResourceTransformer</a></p><blockquote><p>JAR files providing implementations of some interfaces often ship with a META-INF/services/ directory that maps interfaces to their implementation classes for lookup by the service locator. To relocate the class names of these implementation classes, and to merge multiple implementations of the same interface into one service entry, the ServicesResourceTransformer can be used</p></blockquote><p>最后一个问题: maven-shade-plugin 要在3.0以上才能生效</p><p><a href="https://stackoverflow.com/questions/47476402/maven-shade-plugin-relocation-not-updating-a-entry-in-resource-file" target="_blank" rel="noopener">https://stackoverflow.com/questions/47476402/maven-shade-plugin-relocation-not-updating-a-entry-in-resource-file</a></p><p>效果: <img src="http://or0igopk2.bkt.clouddn.com/18-4-10/84724380.jpg" alt></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;工作中的maven-shade-plugin插件高级用法小记&lt;/p&gt;&lt;p&gt;&amp;lt;!-- more --&amp;gt;&lt;/p&gt;&lt;h3&gt;问题背景&lt;/h3&gt;&lt;p
    
    </summary>
    
      <category term="编程工具" scheme="http://www.aitozi.com/categories/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Maven" scheme="http://www.aitozi.com/tags/Maven/"/>
    
  </entry>
  
  <entry>
    <title>上篇·flink 1.4利用kafka0.11实现完整的一致性语义</title>
    <link href="http://www.aitozi.com/flink-kafka-exactly-once.html"/>
    <id>http://www.aitozi.com/flink-kafka-exactly-once.html</id>
    <published>2018-03-11T14:29:44.000Z</published>
    <updated>2019-03-14T17:04:58.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>flink kafka-connector0.10版本分析，与1.4版本中kafka11对比</p><p>&lt;!-- more --&gt;</p><h1>引言</h1><p>官方文档在出了1.4之后特意发表了一篇blog，通过以下这两个条件实现了真正意义上的exactly once语义</p><ol><li>kafka producer0.11的事务性</li><li><a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol" target="_blank" rel="noopener">two phase commit protocol</a></li></ol><p>我们先看0.10版本的kafka-connector的行为逻辑.</p><h1>Kafka-Connector10</h1><h2>kafkaConsumer10</h2><h3>FlinkKafkaConsumerBase</h3><p>这个抽象类实现了<code>CheckpointedFunction</code>, 这个接口的描述：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">* This is the core <span class="class"><span class="keyword">interface</span> <span class="title">for</span> &lt;<span class="title">i</span>&gt;<span class="title">stateful</span> <span class="title">transformation</span> <span class="title">functions</span>&lt;/<span class="title">i</span>&gt;, <span class="title">meaning</span> <span class="title">functions</span></span></span><br><span class="line"><span class="class">* <span class="title">that</span> <span class="title">maintain</span> <span class="title">state</span> <span class="title">across</span> <span class="title">individual</span> <span class="title">stream</span> <span class="title">records</span>.</span></span><br><span class="line"><span class="class">* <span class="title">While</span> <span class="title">more</span> <span class="title">lightweight</span> <span class="title">interfaces</span> <span class="title">exist</span> <span class="title">as</span> <span class="title">shortcuts</span> <span class="title">for</span> <span class="title">various</span> <span class="title">types</span> <span class="title">of</span> <span class="title">state</span>, <span class="title">this</span> <span class="title">interface</span> <span class="title">offer</span> <span class="title">the</span></span></span><br><span class="line"><span class="class">* <span class="title">greatest</span> <span class="title">flexibility</span> <span class="title">in</span> <span class="title">managing</span> <span class="title">both</span> &lt;<span class="title">i</span>&gt;<span class="title">keyed</span> <span class="title">state</span>&lt;/<span class="title">i</span>&gt; <span class="title">and</span> &lt;<span class="title">i</span>&gt;<span class="title">operator</span> <span class="title">state</span>&lt;/<span class="title">i</span>&gt;.</span></span><br></pre></td></tr></table></figure><p></p><p>这个接口中主要要去做两件事情：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//每一次做checkpoint的时候被调用</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化每一个并发的实例的时候被调用</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception</span>;</span><br></pre></td></tr></table></figure><p></p><p>初始化函数的调用时机是在<code>open</code>之前的：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//StreamTask.java</span></span><br><span class="line">initializeState();</span><br><span class="line">openAllOperators();</span><br></pre></td></tr></table></figure><p></p><p>在初始化的函数中提供了一个<code>FunctionSnapshotContext</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">void snapshotState(FunctionSnapshotContext context) throws Exception;</span><br></pre></td></tr></table></figure><p></p><p>让你既可以注册一个KeyedStateStore，也可以注册一个OperatorStateStore</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ManagedInitializationContext</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Returns true, if state was restored from the snapshot of a previous execution. This returns always false for</span></span><br><span class="line"><span class="comment">	 * stateless tasks.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function"><span class="keyword">boolean</span> <span class="title">isRestored</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Returns an interface that allows for registering operator state with the backend.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function">OperatorStateStore <span class="title">getOperatorStateStore</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">	<span class="comment">/**</span></span><br><span class="line"><span class="comment">	 * Returns an interface that allows for registering keyed state with the backend.</span></span><br><span class="line"><span class="comment">	 */</span></span><br><span class="line">	<span class="function">KeyedStateStore <span class="title">getKeyedStateStore</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>我们可以看到kafka10是怎么利用这个<code>CheckpointedFunction</code>来管理记录内部offset的呢？</p><h3>initializeState</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//初始化过程</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// we might have been restored via restoreState() which restores from legacy operator state</span></span><br><span class="line">		<span class="keyword">if</span> (!restored) &#123;</span><br><span class="line">			restored = context.isRestored();</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		OperatorStateStore stateStore = context.getOperatorStateStore();</span><br><span class="line">		offsetsStateForCheckpoint = stateStore.getSerializableListState(DefaultOperatorStateBackend.DEFAULT_OPERATOR_STATE_NAME);</span><br><span class="line">		<span class="comment">//如果是在重启恢复的过程中</span></span><br><span class="line">		<span class="keyword">if</span> (context.isRestored()) &#123;</span><br><span class="line">			<span class="keyword">if</span> (restoredState == <span class="keyword">null</span>) &#123;</span><br><span class="line">				restoredState = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">				<span class="keyword">for</span> (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : offsetsStateForCheckpoint.get()) &#123;</span><br><span class="line">				<span class="comment">//partition和相应的offset数</span></span><br><span class="line">					restoredState.put(kafkaOffset.f0, kafkaOffset.f1);</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				LOG.info(<span class="string">"Setting restore state in the FlinkKafkaConsumer."</span>);</span><br><span class="line">				<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">					LOG.debug(<span class="string">"Using the following offsets: &#123;&#125;"</span>, restoredState);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			LOG.info(<span class="string">"No restore state for FlinkKafkaConsumer."</span>);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure><p></p><p>那么我们看到恢复或者初始化的时候将&lt;partition,offset&gt;信息保存到了一组HashMap中，但是这个Map怎么作用于消费阶段呢？</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//将从状态中获取到的列表赋予给消费的列表</span></span><br><span class="line">subscribedPartitionsToStartOffsets = restoredState;</span><br></pre></td></tr></table></figure><p></p><h3>snapshot</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="keyword">if</span> (!running) &#123;</span><br><span class="line">		LOG.debug(<span class="string">"snapshotState() called on closed source"</span>);</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line">		offsetsStateForCheckpoint.clear();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">final</span> AbstractFetcher&lt;?, ?&gt; fetcher = <span class="keyword">this</span>.kafkaFetcher;</span><br><span class="line">		<span class="keyword">if</span> (fetcher == <span class="keyword">null</span>) &#123;</span><br><span class="line">			<span class="comment">// fetcher还没有初始化，返回上一次恢复的partition和offset，也就是这里会有个bug。我提了个issue:[FLINK-8869][2]</span></span><br><span class="line">			<span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) &#123;</span><br><span class="line">				offsetsStateForCheckpoint.add(Tuple2.of(subscribedPartition.getKey(), subscribedPartition.getValue()));</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="comment">//</span></span><br><span class="line">			<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">				<span class="comment">// the map cannot be asynchronously updated, because only one checkpoint call can happen</span></span><br><span class="line">				<span class="comment">// on this function at a time: either snapshotState() or notifyCheckpointComplete()</span></span><br><span class="line">				pendingOffsetsToCommit.put(context.getCheckpointId(), restoredState);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			HashMap&lt;KafkaTopicPartition, Long&gt; currentOffsets = fetcher.snapshotCurrentState();</span><br><span class="line"></span><br><span class="line">			<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">				<span class="comment">// the map cannot be asynchronously updated, because only one checkpoint call can happen</span></span><br><span class="line">				<span class="comment">// on this function at a time: either snapshotState() or notifyCheckpointComplete()</span></span><br><span class="line">				pendingOffsetsToCommit.put(context.getCheckpointId(), currentOffsets);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) &#123;</span><br><span class="line">				offsetsStateForCheckpoint.add(</span><br><span class="line">						Tuple2.of(kafkaTopicPartitionLongEntry.getKey(), kafkaTopicPartitionLongEntry.getValue()));</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">			<span class="comment">// truncate the map of pending offsets to commit, to prevent infinite growth</span></span><br><span class="line">			<span class="keyword">while</span> (pendingOffsetsToCommit.size() &gt; MAX_NUM_PENDING_CHECKPOINTS) &#123;</span><br><span class="line">				pendingOffsetsToCommit.remove(<span class="number">0</span>);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>offsetCommitMode</h3><p>如上述代码中的<code>offsetCommitMode</code>,主要有以下几种</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DISABLED,</span><br><span class="line">ON_CHECKPOINTS, <span class="comment">// 完成一次checkpoint后向kafka提交消费offset，只有这种模式下才需要我们手动去提交offset到kafka</span></span><br><span class="line">KAFKA_PERIODIC;</span><br></pre></td></tr></table></figure><p></p><p>这个配置主要改变了commitOffset回kafka的时机. 首先在snapshot的时候会将对应的checkpointId和相应的offset的列表放入<code>pendingOffsetsToCommit</code>, 在checkpoint完成后回调<code>notifyCheckpointComplete</code>，这里面主要完成了offset的commit工作。</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</span><br><span class="line">			<span class="comment">// only one commit operation must be in progress</span></span><br><span class="line">			<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">				LOG.debug(<span class="string">"Committing offsets to Kafka/ZooKeeper for checkpoint "</span> + checkpointId);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">try</span> &#123;</span><br><span class="line">				<span class="keyword">final</span> <span class="keyword">int</span> posInMap = pendingOffsetsToCommit.indexOf(checkpointId);</span><br><span class="line">				<span class="keyword">if</span> (posInMap == -<span class="number">1</span>) &#123;</span><br><span class="line">					LOG.warn(<span class="string">"Received confirmation for unknown checkpoint id &#123;&#125;"</span>, checkpointId);</span><br><span class="line">					<span class="keyword">return</span>;</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				<span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</span><br><span class="line">				HashMap&lt;KafkaTopicPartition, Long&gt; offsets =</span><br><span class="line">					(HashMap&lt;KafkaTopicPartition, Long&gt;) pendingOffsetsToCommit.remove(posInMap);</span><br><span class="line"></span><br><span class="line">				<span class="comment">// remove older checkpoints in map</span></span><br><span class="line">				<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; posInMap; i++) &#123;</span><br><span class="line">					pendingOffsetsToCommit.remove(<span class="number">0</span>);</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				<span class="keyword">if</span> (offsets == <span class="keyword">null</span> || offsets.size() == <span class="number">0</span>) &#123;</span><br><span class="line">					LOG.debug(<span class="string">"Checkpoint state was empty."</span>);</span><br><span class="line">					<span class="keyword">return</span>;</span><br><span class="line">				&#125;</span><br><span class="line"></span><br><span class="line">				fetcher.commitInternalOffsetsToKafka(offsets, offsetCommitCallback);</span><br><span class="line">			&#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">				<span class="keyword">if</span> (running) &#123;</span><br><span class="line">					<span class="keyword">throw</span> e;</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="comment">// else ignore exception if we are no longer running</span></span><br><span class="line">			&#125;</span><br></pre></td></tr></table></figure><p></p><h3>消费partition分配问题</h3><ul><li><p>在从上次点恢复的情况下是直接从state中获取应该读取哪一个partition，offset。如果并发度改变了会做出什么样的反馈呢?会正确做出rescale吗</p></li><li><p>第一次进行读取的时候会初始化处当前task（并发度）所需要订阅的partition</p></li></ul><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> Map&lt;KafkaTopicPartition, Long&gt; <span class="title">initializeSubscribedPartitionsToStartOffsets</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">			List&lt;KafkaTopicPartition&gt; kafkaTopicPartitions, //topic的所有partition</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="keyword">int</span> indexOfThisSubtask, // 当前task的维度</span></span></span><br><span class="line"><span class="function"><span class="params">			<span class="keyword">int</span> numParallelSubtasks, // 总的并发度</span></span></span><br><span class="line"><span class="function"><span class="params">			StartupMode startupMode, // 从哪个offset消费的模式（最新，最老，指定offset）</span></span></span><br><span class="line"><span class="function"><span class="params">			Map&lt;KafkaTopicPartition, Long&gt; specificStartupOffsets)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">		Map&lt;KafkaTopicPartition, Long&gt; subscribedPartitionsToStartOffsets = <span class="keyword">new</span> HashMap&lt;&gt;(kafkaTopicPartitions.size());</span><br><span class="line"></span><br><span class="line">		<span class="keyword">for</span> (KafkaTopicPartition kafkaTopicPartition : kafkaTopicPartitions) &#123;</span><br><span class="line">			<span class="comment">// only handle partitions that this subtask should subscribe to（选取当前subtask所需要订阅的partition）</span></span><br><span class="line">			<span class="keyword">if</span> (KafkaTopicPartitionAssigner.assign(kafkaTopicPartition, numParallelSubtasks) == indexOfThisSubtask) &#123;</span><br><span class="line">				<span class="keyword">if</span> (startupMode != StartupMode.SPECIFIC_OFFSETS) &#123;</span><br><span class="line">					<span class="comment">//StateSentinel都是一串随机的负数占位符(都是一个标记在KafkaConsumerThread中进行判断)</span></span><br><span class="line">					subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, startupMode.getStateSentinel());</span><br><span class="line">				&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">					<span class="keyword">if</span> (specificStartupOffsets == <span class="keyword">null</span>) &#123;</span><br><span class="line">						<span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(</span><br><span class="line">							<span class="string">"Startup mode for the consumer set to "</span> + StartupMode.SPECIFIC_OFFSETS +</span><br><span class="line">								<span class="string">", but no specific offsets were specified"</span>);</span><br><span class="line">					&#125;</span><br><span class="line"></span><br><span class="line">					Long specificOffset = specificStartupOffsets.get(kafkaTopicPartition);</span><br><span class="line">					<span class="keyword">if</span> (specificOffset != <span class="keyword">null</span>) &#123;</span><br><span class="line">						<span class="comment">// since the specified offsets represent the next record to read, we subtract</span></span><br><span class="line">						<span class="comment">// it by one so that the initial state of the consumer will be correct</span></span><br><span class="line">						<span class="comment">// 这里需要减去1</span></span><br><span class="line">						subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, specificOffset - <span class="number">1</span>);</span><br><span class="line">					&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">						subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET);</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		<span class="keyword">return</span> subscribedPartitionsToStartOffsets;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assign</span><span class="params">(KafkaTopicPartition partition, <span class="keyword">int</span> numParallelSubtasks)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> startIndex = ((partition.getTopic().hashCode() * <span class="number">31</span>) &amp; <span class="number">0x7FFFFFFF</span>) % numParallelSubtasks;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// here, the assumption is that the id of Kafka partitions are always ascending</span></span><br><span class="line">	<span class="comment">// starting from 0, and therefore can be used directly as the offset clockwise from the start index</span></span><br><span class="line">	<span class="comment">// 这里看出：每个Partition只会分配到一个subtask来消费</span></span><br><span class="line">	<span class="comment">// 1. partition &gt; parallel 一个subtask会订阅多个partition</span></span><br><span class="line">	<span class="comment">// 2. partition &lt; parallel 有subtask会是空闲的</span></span><br><span class="line">	<span class="comment">// 3. startIndex由topic名字计算得出</span></span><br><span class="line">	<span class="keyword">return</span> (startIndex + partition.getPartition()) % numParallelSubtasks;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>消费模型kafka10</h3><p>在完成partition订阅之后，就要开始真正的run方法了，<code>FlinkKafkaConsumer</code>也是实现自<code>SouceFunction</code>，因此主要的逻辑也都是在run方法中实现。 主要逻辑：</p><p>kafkaConsumerThread和Kafka10Fetcher通过<code>Handover</code>交互,我觉得这段代码写的很不错，可以好好学习下。可以形象的比作在接力跑：<code>kafkaConsumerThread</code>通过真正的消费线程消费放入一个<code>HandOver</code>，再由kafkaFetcher去poll，完成整个消费过程。</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// we need only do work, if we actually have partitions assigned</span></span><br><span class="line"><span class="keyword">if</span> (!subscribedPartitionsToStartOffsets.isEmpty()) &#123;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// create the fetcher that will communicate with the Kafka brokers</span></span><br><span class="line">	<span class="keyword">final</span> AbstractFetcher&lt;T, ?&gt; fetcher = createFetcher(</span><br><span class="line">			sourceContext,</span><br><span class="line">			subscribedPartitionsToStartOffsets,</span><br><span class="line">			periodicWatermarkAssigner,</span><br><span class="line">			punctuatedWatermarkAssigner,</span><br><span class="line">			(StreamingRuntimeContext) getRuntimeContext(),</span><br><span class="line">			offsetCommitMode);</span><br><span class="line"></span><br><span class="line">	<span class="comment">// publish the reference, for snapshot-, commit-, and cancel calls</span></span><br><span class="line">	<span class="comment">// IMPORTANT: We can only do that now, because only now will calls to</span></span><br><span class="line">	<span class="comment">//            the fetchers 'snapshotCurrentState()' method return at least</span></span><br><span class="line">	<span class="comment">//            the restored offsets</span></span><br><span class="line">	<span class="keyword">this</span>.kafkaFetcher = fetcher;</span><br><span class="line">	<span class="keyword">if</span> (!running) &#123;</span><br><span class="line">		<span class="keyword">return</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// (3) run the fetcher' main work method</span></span><br><span class="line">	<span class="comment">// 主要工作方法</span></span><br><span class="line">	fetcher.runFetchLoop();</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">	<span class="comment">// this source never completes, so emit a Long.MAX_VALUE watermark</span></span><br><span class="line">	<span class="comment">// to not block watermark forwarding</span></span><br><span class="line">	<span class="comment">// 发送最大的watermark就不会block住下游的watermark更新</span></span><br><span class="line">	sourceContext.emitWatermark(<span class="keyword">new</span> Watermark(Long.MAX_VALUE));</span><br><span class="line"></span><br><span class="line">	<span class="comment">// wait until this is canceled</span></span><br><span class="line">	<span class="keyword">final</span> Object waitLock = <span class="keyword">new</span> Object();</span><br><span class="line">	<span class="keyword">while</span> (running) &#123;</span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			<span class="comment">//noinspection SynchronizationOnLocalVariableOrMethodParameter</span></span><br><span class="line">			<span class="keyword">synchronized</span> (waitLock) &#123;</span><br><span class="line">				waitLock.wait();</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">			<span class="keyword">if</span> (!running) &#123;</span><br><span class="line">				<span class="comment">// restore the interrupted state, and fall through the loop</span></span><br><span class="line">				<span class="comment">// 打断当前线程</span></span><br><span class="line">				Thread.currentThread().interrupt();</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Handover的描述， Handover代码可以再好好学习下。</span><br><span class="line">* The Handover is a utility to hand over data (a buffer of records) and exception from a</span><br><span class="line">* &lt;i&gt;producer&lt;/i&gt; thread to a &lt;i&gt;consumer&lt;/i&gt; thread. It effectively behaves like a</span><br><span class="line">* &quot;size one blocking queue&quot;, with some extras around exception reporting, closing, and</span><br><span class="line">* waking up thread without &#123;@link Thread#interrupt() interrupting&#125; threads.</span><br></pre></td></tr></table></figure><p></p><h3>KafkaFetcher</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">runFetchLoop</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		<span class="keyword">final</span> Handover handover = <span class="keyword">this</span>.handover;</span><br><span class="line"></span><br><span class="line">		<span class="comment">// kick off the actual Kafka consumer</span></span><br><span class="line">		<span class="comment">// 启动真正的消费线程</span></span><br><span class="line">		consumerThread.start();</span><br><span class="line"></span><br><span class="line">		<span class="keyword">while</span> (running) &#123;</span><br><span class="line">			<span class="comment">// this blocks until we get the next records</span></span><br><span class="line">			<span class="comment">// it automatically re-throws exceptions encountered in the fetcher thread</span></span><br><span class="line">			<span class="comment">// 在handover中获取真正的数据，并抛出其他线程中的异常</span></span><br><span class="line">			<span class="keyword">final</span> ConsumerRecords&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; records = handover.pollNext();</span><br><span class="line"></span><br><span class="line">			<span class="comment">// get the records for each topic partition</span></span><br><span class="line">			<span class="comment">// subscribedPartitionStates维护的是每个partition的状态（partition，KPH（partition的描述，依据版本可能不同），offset, committedOffset, watermark）</span></span><br><span class="line">			<span class="keyword">for</span> (KafkaTopicPartitionState&lt;TopicPartition&gt; partition : subscribedPartitionStates()) &#123;</span><br><span class="line"></span><br><span class="line">				List&lt;ConsumerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt; partitionRecords =</span><br><span class="line">						records.records(partition.getKafkaPartitionHandle());</span><br><span class="line"></span><br><span class="line">				<span class="keyword">for</span> (ConsumerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; record : partitionRecords) &#123;</span><br><span class="line">					<span class="keyword">final</span> T value = deserializer.deserialize(</span><br><span class="line">							record.key(), record.value(),</span><br><span class="line">							record.topic(), record.partition(), record.offset());</span><br><span class="line"></span><br><span class="line">					<span class="keyword">if</span> (deserializer.isEndOfStream(value)) &#123;</span><br><span class="line">						<span class="comment">// end of stream signaled</span></span><br><span class="line">						running = <span class="keyword">false</span>;</span><br><span class="line">						<span class="keyword">break</span>;</span><br><span class="line">					&#125;</span><br><span class="line"></span><br><span class="line">					<span class="comment">// emit the actual record. this also updates offset state atomically</span></span><br><span class="line">					<span class="comment">// and deals with timestamps and watermark generation</span></span><br><span class="line">					<span class="comment">// 这里会进入真正的通过sourceContext发送数据的代码 如下</span></span><br><span class="line">					emitRecord(value, partition, record.offset(), record);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">finally</span> &#123;</span><br><span class="line">		<span class="comment">// this signals the consumer thread that no more work is to be done</span></span><br><span class="line">		consumerThread.shutdown();</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// on a clean exit, wait for the runner thread</span></span><br><span class="line">	<span class="keyword">try</span> &#123;</span><br><span class="line">		consumerThread.join();</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">		<span class="comment">// may be the result of a wake-up interruption after an exception.</span></span><br><span class="line">		<span class="comment">// we ignore this here and only restore the interruption state</span></span><br><span class="line">		Thread.currentThread().interrupt();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">emitRecordWithTimestamp</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">		T record, KafkaTopicPartitionState&lt;KPH&gt; partitionState, <span class="keyword">long</span> offset, <span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (record != <span class="keyword">null</span>) &#123;</span><br><span class="line">		<span class="keyword">if</span> (timestampWatermarkMode == NO_TIMESTAMPS_WATERMARKS) &#123;</span><br><span class="line">			<span class="comment">// fast path logic, in case there are no watermarks generated in the fetcher</span></span><br><span class="line"></span><br><span class="line">			<span class="comment">// emit the record, using the checkpoint lock to guarantee</span></span><br><span class="line">			<span class="comment">// atomicity of record emission and offset state update</span></span><br><span class="line">			<span class="keyword">synchronized</span> (checkpointLock) &#123;</span><br><span class="line">				sourceContext.collectWithTimestamp(record, timestamp);</span><br><span class="line">				partitionState.setOffset(offset);</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (timestampWatermarkMode == PERIODIC_WATERMARKS) &#123;</span><br><span class="line">		    <span class="comment">// 更新partitionstate中的watermark状态</span></span><br><span class="line">			emitRecordWithTimestampAndPeriodicWatermark(record, partitionState, offset, timestamp);</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			emitRecordWithTimestampAndPunctuatedWatermark(record, partitionState, offset, timestamp);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		<span class="comment">// if the record is null, simply just update the offset state for partition</span></span><br><span class="line">		<span class="keyword">synchronized</span> (checkpointLock) &#123;</span><br><span class="line">			partitionState.setOffset(offset);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>在设置了kafkaTimestampassigner之后就会进行一个定时任务向下游发送watermark，值为所有partition维护的最小值：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">long</span> minAcrossAll = Long.MAX_VALUE;</span><br><span class="line">	<span class="keyword">for</span> (KafkaTopicPartitionStateWithPeriodicWatermarks&lt;?, ?&gt; state : allPartitions) &#123;</span><br><span class="line">		</span><br><span class="line">		<span class="comment">// we access the current watermark for the periodic assigners under the state</span></span><br><span class="line">		<span class="comment">// lock, to prevent concurrent modification to any internal variables</span></span><br><span class="line">		<span class="keyword">final</span> <span class="keyword">long</span> curr;</span><br><span class="line">		<span class="comment">//noinspection SynchronizationOnLocalVariableOrMethodParameter</span></span><br><span class="line">		<span class="comment">// 这个锁是防止别的线程修改其他变量</span></span><br><span class="line">		<span class="keyword">synchronized</span> (state) &#123;</span><br><span class="line">			curr = state.getCurrentWatermarkTimestamp();</span><br><span class="line">		&#125;</span><br><span class="line">		</span><br><span class="line">		minAcrossAll = Math.min(minAcrossAll, curr);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// emit next watermark, if there is one</span></span><br><span class="line">	<span class="keyword">if</span> (minAcrossAll &gt; lastWatermarkTimestamp) &#123;</span><br><span class="line">		lastWatermarkTimestamp = minAcrossAll;</span><br><span class="line">		emitter.emitWatermark(<span class="keyword">new</span> Watermark(minAcrossAll));</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="comment">// schedule the next watermark</span></span><br><span class="line">	timerService.registerTimer(timerService.getCurrentProcessingTime() + interval, <span class="keyword">this</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h3>KafkaConsumerThread</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (records == <span class="keyword">null</span>) &#123;</span><br><span class="line">				<span class="keyword">try</span> &#123;</span><br><span class="line">					records = consumer.poll(pollTimeout);</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">catch</span> (WakeupException we) &#123;</span><br><span class="line">					<span class="keyword">continue</span>;</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="keyword">try</span> &#123;</span><br><span class="line">				handover.produce(records);</span><br><span class="line">				records = <span class="keyword">null</span>;</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">catch</span> (Handover.WakeupException e) &#123;</span><br><span class="line">				<span class="comment">// fall through the loop</span></span><br><span class="line">			&#125;</span><br></pre></td></tr></table></figure><p></p><p>主要做的工作就是从consumer消费数据塞入handover，等待拉取</p><h3>Handover</h3><h3>桥接模式</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumerCallBridge</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">assignPartitions</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, List&lt;TopicPartition&gt; topicPartitions)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">		consumer.assign(topicPartitions);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekPartitionToBeginning</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, TopicPartition partition)</span> </span>&#123;</span><br><span class="line">		consumer.seekToBeginning(partition);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekPartitionToEnd</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, TopicPartition partition)</span> </span>&#123;</span><br><span class="line">		consumer.seekToEnd(partition);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>解决08 09 10 版本的api不兼容问题</p><h2>Kafkaproducer10</h2><p>###initializeState</p><p>什么都不做</p><h3>snapshot</h3><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="comment">// check for asynchronous errors and fail the checkpoint if necessary</span></span><br><span class="line">	checkErroneous();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> (flushOnCheckpoint) &#123;</span><br><span class="line">		<span class="comment">// flushing is activated: We need to wait until pendingRecords is 0</span></span><br><span class="line">		flush();</span><br><span class="line">		<span class="keyword">synchronized</span> (pendingRecordsLock) &#123;</span><br><span class="line">			<span class="keyword">if</span> (pendingRecords != <span class="number">0</span>) &#123;</span><br><span class="line">				<span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Pending record count must be zero at this point: "</span> + pendingRecords);</span><br><span class="line">			&#125;</span><br><span class="line"></span><br><span class="line">			<span class="comment">// if the flushed requests has errors, we should propagate it also and fail the checkpoint</span></span><br><span class="line">			checkErroneous();</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>这里涉及到一个flushOnCheckPoint的问题，再调用<code>producer.flush</code>期间，producer会将所有没写入的，在buffer中的数据刷盘，然后调用commitCallBack，这就保证了ckpt之后数据不会丢的问题。</p><p>主要工作方法：</p><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(IN next)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">	<span class="comment">// propagate asynchronous errors</span></span><br><span class="line">	checkErroneous();</span><br><span class="line"></span><br><span class="line">	<span class="keyword">byte</span>[] serializedKey = schema.serializeKey(next);</span><br><span class="line">	<span class="keyword">byte</span>[] serializedValue = schema.serializeValue(next);</span><br><span class="line">	<span class="comment">// 每条元素可以自己自己要写到的topic</span></span><br><span class="line">	String targetTopic = schema.getTargetTopic(next);</span><br><span class="line">	<span class="keyword">if</span> (targetTopic == <span class="keyword">null</span>) &#123;</span><br><span class="line">		targetTopic = defaultTopicId;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">int</span>[] partitions = <span class="keyword">this</span>.topicPartitionsMap.get(targetTopic);</span><br><span class="line">	<span class="keyword">if</span>(<span class="keyword">null</span> == partitions) &#123;</span><br><span class="line">		partitions = getPartitionsByTopic(targetTopic, producer);</span><br><span class="line">		<span class="keyword">this</span>.topicPartitionsMap.put(targetTopic, partitions);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	ProducerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; record;</span><br><span class="line">	<span class="keyword">if</span> (flinkKafkaPartitioner == <span class="keyword">null</span>) &#123;</span><br><span class="line">		record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(targetTopic, serializedKey, serializedValue);</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(</span><br><span class="line">				targetTopic,</span><br><span class="line">				flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions),</span><br><span class="line">				serializedKey,</span><br><span class="line">				serializedValue);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> (flushOnCheckpoint) &#123;</span><br><span class="line">		<span class="keyword">synchronized</span> (pendingRecordsLock) &#123;</span><br><span class="line">			pendingRecords++;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	producer.send(record, callback);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h1>问题</h1><ol><li><p>kafka恢复状态直接从状态中去获取了之前保存的partition和offset，但是如果是扩容partition的场景就不会从新的Partition消费 issue:<a href="https://issues.apache.org/jira/projects/FLINK/issues/FLINK-8869?filter=reportedbyme" target="_blank" rel="noopener">FLINK-8869</a></p></li><li><p>flink内部维护了offset，为什么向kafka提交的时候还需要在checkpoint之后再提交而不是定时提交就算了？</p><p>因为虽然从checkpoint点恢复的时候不需要从kafka broker获取消费点的位置了，但是如果是应用重启消费上次消费到的点的数据，这个offset就是flink向kafka提交的，放在checkpoint完成后去做的好处就是让应用即使不是从上个点恢复的，也能够从kafka消费正确的offset点。</p></li><li><p>如果在新的checkpoint没打之前任务失败了，重新从上次的offset点消费的话下游数据是不是重复了?</p><p><strong>是的，因为有一部分数据经过处理已经sink出去了，因此才需要0.11的一致性语义</strong></p></li></ol><p><em>以上代码： kafka-connector0.10 来源于release1.3.2 kafka-connector0.11 来源于release1.4.0</em></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;flink kafka-connector0.10版本分析，与1.4版本中kafka11对比&lt;/p&gt;&lt;p&gt;&amp;lt;!-- more --&amp;gt;&lt;/p
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
      <category term="Kafka" scheme="http://www.aitozi.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>flink-cep-paper</title>
    <link href="http://www.aitozi.com/flink-cep-paper.html"/>
    <id>http://www.aitozi.com/flink-cep-paper.html</id>
    <published>2018-02-25T12:19:15.000Z</published>
    <updated>2019-03-14T17:04:45.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --><p>CEP实现论文</p><p>&lt;!-- more --&gt;</p><h3>引言</h3><p>2016年1月29号，社区提了这个patch<a href="https://github.com/apache/flink/pull/1557" target="_blank" rel="noopener">[FLINK-3216]</a>引入flink cep模块，flink cep实现基于论文：<a href="https://people.cs.umass.edu/%7Eyanlei/publications/sase-sigmod08.pdf" target="_blank" rel="noopener">Efficient Pattern Matching over Event Streams</a>。我们来深入阅读以下这篇论文。</p><h3>event pattern查询表达形式</h3><p>先看三个查询 <img src="http://or0igopk2.bkt.clouddn.com/18-2-25/25532292.jpg" alt></p><p><em>Query1</em></p><p>匹配出的是：挑选从货架取出的商品，未经checkout，带离了商店的行为序列</p><ol><li><code>SEQ(Shelf a, ∼(Register b), Exit c)</code></li><li>where条件指定了a，b，c的tagid相同</li><li>within指定窗口大小12小时</li></ol><p><em>Query2</em></p><p>匹配出的是：一个污染报警和相关受污染的发货卸货流转站点序列</p><ol><li>报警类型是污染</li><li>匹配出的装载点和前一个到达点一致</li><li>within指定匹配窗口大小是3小时</li></ol><p><em>Query3</em></p><p>匹配出的是：匹配股票交易拐点</p><ol><li>初始成交量高于1000</li><li>持续增加，最后成交量突然跌为最高点的80%以下的序列</li><li>1小时的时间窗口</li></ol><p>可以看出规则是怎么描述的：</p><ol><li>指定匹配出的序列是什么（structure）</li><li>限制或筛选条件</li></ol><p>还需要匹配策略来进行规制匹配。</p><ol><li>严格的邻近匹配（Strict contiguity）：只有连续两个event能满足匹配，中间不能夹杂其他不符合条件的元素</li><li>分区邻近匹配（Partition contiguity）：这就是说被选出的两个event之间不一定需要完全紧密相连，但是在一个分区中的需要紧密相连 如<em>Query3</em>中的a[]需要连续，a与b不需要连续</li><li>skip_till_next_match: 所有的不相关的元素都会被跳过，不再考虑是不是邻近的元素</li><li>skip_till_any_match: <em>Query2</em>解释了这种用法。比如当前最后识别的shipment到达了X位置。这时候读进来一条，X-&gt;Y的Event，在<code>skip_till_any_match</code>匹配规则下系统会做两件事：1.接收X-&gt;Y事件，更新状态2.在当前状态的另一个实例中不接收他来保留一个状态。这样当下一个比如来了个X-&gt;Z事件那还是可以接受这个实例。这种策略就是返回所有的可能的序列（allowing non-deterministic actions）在接收到同一个事件后可以有不一样的走向</li></ol><hr><p>以上介绍了event pattern的查询形式，接下来我们看其内部实现算法。</p><h3>NFA 非确定性自动状态机</h3><p>非确定性自动状态机由以下结构组成。A = (Q,E,θ,q1,F),</p><pre><code>Q: 一系列状态
E： 边
θ： 计算公式
q1：起始状态
F: 结束状态
</code></pre><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/18248041.jpg" alt></p><p><strong>States</strong></p><blockquote><p>the start state,a[1], is where the matching process begins. It awaits input to start the Kleene plus and to select an event into the a[1] unit of the match buffer. At the next state a[i], it attempts to select another event into the a[i] (i &gt; 1) unit of the buffer. The subsequent state b denotes that the matching process has fulfilled the Kleene plus (for a particular match) and is ready to process the next pattern component. The final state, F, represents the completion of the process, resulting in the creation of a pattern match.</p></blockquote><p><strong>Edges</strong></p><blockquote><p>Each state is associated with a number of edges, representing the actions that can be taken at the state. As Figure 2(a) shows, each state that is a singleton state or the first state,p[1], of a pair has a forward begin edge. Each second state, p[i], of a pair has a forward proceed edge, and a looping take edge. Every state (except the start and final states) has a looping ignore edge. The start state has no edges to it as we are only interested in matches that start with selected events. Each edge at a state, q, is precisely described by a triplet:(1) a formula that specifies the condition on taking it, denoted by θq-edge, (2) an operation on the input stream (i.e.,consume an event or not), and (3) an operation on the match buffer (i.e., write to the buffer or not). Formulas of edges are compiled from pattern queries, which we explain in detail shortly. As shown in Figure 2(a), we use solid lines to denote begin and take edges that consume an event from the input and write it to the buffer, and dashed lines for ignore edges that consume an event but do not write it to the buffer. The proceed edge is a special edge: it does not consume any input event but only evaluates its formula and tries proceeding. We distinguish the proceed edge from ignore edges in the style of arrow, denoting its behavior.</p></blockquote><p><em>非确定性</em> 某些state具有两条边，但是这两条边的行为不一定是完全相反的</p><p>NFA算法过程：</p><ol><li>先根据整个structure构建state和边的行为</li><li>将condition翻译成边上的公式</li><li>将select strategy翻译成边上的公式</li><li>将时间窗口翻译成：最先匹配的元素和最后的元素时长不能超过窗口大小</li></ol><p>优化：</p><ol><li>先做过滤</li><li>将window条件前置</li><li>优化proceed边，进入下一个state的时候要检查是否满足begin的条件判断</li></ol><h3>带版本的内存共享</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/20508653.jpg" alt></p><p>通过带版本的共享内存解决多runs之间的重复事件问题。</p><h3>Computation state</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/48608682.jpg" alt></p><ol><li>version number of a run</li><li>current automaton state the run is in</li><li>a pointer to the most revent event selected into the buffer</li><li>a vector =&gt; 保留做edge判断的所需的最少的元素。例如上图中存储了a[i]的和和count为了计算平均值</li></ol><h3>Merge相同的Runs</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/87062132.jpg" alt></p><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/41465208.jpg" alt></p><p>将Query3中的avg算法改为max算法，他们在接收到e4之后达到了同样的状态，因此其之后的运算可以merge为一个。</p><ol><li>首先探测什么时候两个runs一样了，引入了算子M。每一个状态机都有一个mask，每一个mask对每一个Vector中列都有一个bit位，都相同时那么这两个run就是相等的。</li><li>创建combined的run<ol><li>时间设置为最小的那个时间</li></ol></li></ol><h3>Backtrack算法（回溯）</h3><p>每次只跑一个run，跑失败了再回退到分叉点跑另一条。也就是以广度优先搜索方式 <code>breadth first search manner</code></p><h3>待补充</h3><ol><li>性能评估方案</li><li>有哪些因素会影响计算速度和吞吐</li><li>内存优化管理措施</li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Sun Mar 31 2019 01:19:19 GMT+0800 (中国标准时间) --&gt;&lt;p&gt;CEP实现论文&lt;/p&gt;&lt;p&gt;&amp;lt;!-- more --&amp;gt;&lt;/p&gt;&lt;h3&gt;引言&lt;/h3&gt;&lt;p&gt;2016年1月29号，社区提了这个patch&lt;a
    
    </summary>
    
      <category term="论文" scheme="http://www.aitozi.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
      <category term="CEP" scheme="http://www.aitozi.com/tags/CEP/"/>
    
  </entry>
  
</feed>
