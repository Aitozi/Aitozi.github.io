<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Aitozi</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.aitozi.com/"/>
  <updated>2019-03-14T17:25:37.000Z</updated>
  <id>http://www.aitozi.com/</id>
  
  <author>
    <name>aitozi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>to be or not to be in 2019</title>
    <link href="http://www.aitozi.com/2019-flag.html"/>
    <id>http://www.aitozi.com/2019-flag.html</id>
    <published>2019-03-14T17:25:03.000Z</published>
    <updated>2019-03-14T17:25:37.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>2019年的flag</p><a id="more"></a><p>2019年的Q1快接近尾声了，是时候给今年来一个flag了，今年有以下3个目标：</p><ol><li>跑步500公里</li><li>读书10本+ <em>技术书籍不低于3本</em></li><li>flink，netty，hbase系列的源码分析博客及仓库更新</li><li>机器学习简单入门</li><li>简单学会尤克里里的弹奏</li><li>努力工作，攒钱</li></ol><p>截止时间:</p><p>———————————–2020.01.01———————————————–</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;2019年的flag&lt;/p&gt;
    
    </summary>
    
      <category term="杂七杂八" scheme="http://www.aitozi.com/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"/>
    
    
      <category term="flag" scheme="http://www.aitozi.com/tags/flag/"/>
    
  </entry>
  
  <entry>
    <title>String,StringBuffer,StringBuilder的区别</title>
    <link href="http://www.aitozi.com/string-stringbuilder-stringbuffer.html"/>
    <id>http://www.aitozi.com/string-stringbuilder-stringbuffer.html</id>
    <published>2018-08-27T16:13:48.000Z</published>
    <updated>2019-03-14T17:09:57.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>常用的jdk的组件的源码分析之：<code>String</code>,<code>StringBuffer</code>,<code>StringBuilder</code></p><a id="more"></a><p>String 字符串常量<br>StringBuffer 字符串变量（线程安全）<br>StringBuilder 字符串变量（非线程安全）</p><p>String和StringBuffer的主要性能区别其实在于 String是不可变的对象, 因此在每次对String类型进行改变的时候其实都等同于生成了一个新的 String对象，然后将指针指向新的String对象，所以经常改变内容的字符串最好不要用String，因为每次生成对象都会对系统性能产生影响，特别当内存中无引用对象多了以后，JVM的GC就会开始工作，那速度是一定会相当慢的。</p><p>那么String为什么要是不可变的呢？</p><h3 id="String类不可变的好处"><a href="#String类不可变的好处" class="headerlink" title="String类不可变的好处"></a>String类不可变的好处</h3><ol><li>只有当字符串是不可变的，字符串池才有可能实现。字符串池的实现可以在运行时节约很多heap空间，因为不同的字符串变量都指向池中的同一个字符串。但如果字符串是可变的，那么String interning将不能实现，String interning是指对不同的字符串仅仅只保存一个，即不会保存多个相同的字符串。因为这样的话，如果变量改变了它的值，那么其它指向这个值的变量的值也会一起改变。</li><li>如果字符串是可变的，那么会引起很严重的安全问题。譬如，数据库的用户名、密码都是以字符串的形式传入来获得数据库的连接，或者在socket编程中，主机名和端口都是以字符串的形式传入。因为字符串是不可变的，所以它的值是不可改变的，否则黑客们可以钻到空子，改变字符串指向的对象的值，造成安全漏洞。</li><li>因为字符串是不可变的，所以是多线程安全的，同一个字符串实例可以被多个线程共享。这样便不用因为线程安全问题而使用同步。字符串自己便是线程安全的。</li><li>类加载器要用到字符串，不可变性提供了安全性，以便正确的类被加载。譬如你想加载java.sql.Connection类，而这个值被改成了myhacked.Connection，那么会对你的数据库造成不可知的破坏。</li><li>因为字符串是不可变的，所以在它创建的时候hashcode就被缓存了，不需要重新计算。这就使得字符串很适合作为Map中的键，字符串的处理速度要快过其它的键对象。这就是HashMap中的键往往都使用字符串。也同时指出一个理念，千万不要把可变类型作为HashMap和HashSet的键值</li></ol><h3 id="在java中如何设计不可变"><a href="#在java中如何设计不可变" class="headerlink" title="在java中如何设计不可变"></a>在java中如何设计不可变</h3><ol><li>对于属性不提供设值的方法</li><li>所有的属性定义为private final</li><li>类声明为final不允许继承</li><li>return deep cloned objects with copied content for all mutable fields in class</li></ol><p>翻看string的源码，可以看到string的本质是个char数组，并且使用final关键字修饰。但是char数组用final修饰只能让数组的引用地址不变，array数组还是可变的，主要是SUN的工程师没有暴露内部成员字段，所以String不可变主要在底层实现，而不是在final。</p><h3 id="String的内存存储"><a href="#String的内存存储" class="headerlink" title="String的内存存储"></a>String的内存存储</h3><p>一般而言，Java 对象在虚拟机的结构如下：</p><ul><li>对象头（object header）：8 个字节</li><li>Java 原始类型数据：如 int, float, char 等类型的数据，各类型数据占内存。<ul><li>boolean 1</li><li>byte</li><li>char 2</li><li>short</li><li>int 4</li><li>long 8</li></ul></li><li>引用（reference）：4 个字节</li><li>填充符（padding）</li></ul><p>然而，一个 Java 对象实际还会占用些额外的空间，如：对象的 class 信息、ID、在虚拟机中的状态。在 Oracle JDK 的 Hotspot 虚拟机中，一个普通的对象需要额外 8 个字节。</p><p>String对象的声明</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">private final char value[]; </div><div class="line">private final int offset; </div><div class="line">private final int count; </div><div class="line">private int hash;</div></pre></td></tr></table></figure><p>那么因该如何计算该 String 所占的空间？</p><p>首先计算一个空的 char 数组所占空间，在 Java 里数组也是对象，因而数组也有对象头，故一个数组所占的空间为对象头所占的空间加上数组长度，即 8 + 4 = 12 字节 , 经过填充后为 16 字节。</p><p>那么一个空 String 所占空间为：</p><p>对象头（8 字节）+ char 数组（16 字节）+ 3 个 int（3 × 4 = 12 字节）+1 个 char 数组的引用 (4 字节 ) = 40 字节。</p><p>因此一个实际的 String 所占空间的计算公式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">8*( ( 8+2*n+4+12)+7 ) / 8 = 8*(int) ( ( ( (n) *2 )+43) /8 )</div></pre></td></tr></table></figure><p>在java中对String对象特殊对待，所以在heap上分为两块，一块是String constant pool存储java字符串常量，另一块存储普通对象和字符串对象，主要区别：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">String a = &quot;abc&quot;;</div><div class="line">String b = new String(&quot;acb&quot;)</div></pre></td></tr></table></figure><p>第一种jvm会先去查找constant pool是否存在此常量，不存在就在constant pool上进行创建，第二种是在堆上创建对象，并且不会加入到constant pool上，因此可能会带来字符串重复占用内存的问题。可以调用String.intern()加入到String constant pool中，其实是JVM heap 中 PermGen 相应的区域。</p><p>jdk1.6和1.7还有所不同，jdk1.7的常量池是在堆中的</p><h3 id="StringBuffer"><a href="#StringBuffer" class="headerlink" title="StringBuffer"></a>StringBuffer</h3><p>StringBuffer和String不同，每次修改都会对 StringBuffer 对象本身进行操作，而不是生成新的对象，再改变对象引用。所以在一般情况下我们推荐使用 StringBuffer ，特别是字符串对象经常改变的情况下。而在某些特别情况下， String 对象的字符串拼接其实是被JVM解释成了 StringBuffer 对象的拼接，所以这些时候 String 对象的速度并不会比 StringBuffer 对象慢，而特别是以下的字符串对象生成中， String效率是远要比 StringBuffer 快的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">String S1 = “This is only a” + “ simple” + “ test”;</div><div class="line">StringBuffer Sb = new StringBuilder(“This is only a”).append(“ simple”).append(“ test”);</div></pre></td></tr></table></figure><p>你会很惊讶的发现，生成 String S1 对象的速度简直太快了，而这个时候 StringBuffer 居然速度上根本一点都不占优势。其实这是 JVM 的一个把戏，在 JVM 眼里，这个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">String S1 = “This is only a” + “ simple” + “test”;</div><div class="line">其实就是：</div><div class="line">String S1 = “This is only a simple test”;</div></pre></td></tr></table></figure><p>当然不需要太多的时间了。但大家这里要注意的是，如果你的字符串是来自另外的 String 对象的话，速度就没那么快了，譬如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">String S2 = “This is only a”;</div><div class="line">String S3 = “ simple”;</div><div class="line">String S4 = “ test”;</div><div class="line">String S1 = S2 +S3 + S4;</div></pre></td></tr></table></figure><p>这时候 JVM 会规规矩矩的按照原来的方式去做</p><p><a href="https://www.ibm.com/developerworks/cn/java/j-lo-optmizestring/index.html" target="_blank" rel="external">https://www.ibm.com/developerworks/cn/java/j-lo-optmizestring/index.html</a></p><p><a href="https://blog.csdn.net/qq_36357995/article/details/79985538" target="_blank" rel="external">https://blog.csdn.net/qq_36357995/article/details/79985538</a></p><p><a href="https://segmentfault.com/a/1190000004261063" target="_blank" rel="external">https://segmentfault.com/a/1190000004261063</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;常用的jdk的组件的源码分析之：&lt;code&gt;String&lt;/code&gt;,&lt;code&gt;StringBuffer&lt;/code&gt;,&lt;code&gt;StringBuilder&lt;/code&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="jdk" scheme="http://www.aitozi.com/tags/jdk/"/>
    
  </entry>
  
  <entry>
    <title>rocksdb概念简介</title>
    <link href="http://www.aitozi.com/rocksdb-wiki.html"/>
    <id>http://www.aitozi.com/rocksdb-wiki.html</id>
    <published>2018-08-23T15:20:44.000Z</published>
    <updated>2019-03-14T17:07:11.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>本文翻译自：<a href="https://github.com/facebook/rocksdb/wiki/rocksdb-basics" target="_blank" rel="external">https://github.com/facebook/rocksdb/wiki/rocksdb-basics</a></p><p>主要是rocksdb的一些概念理解和介绍</p><a id="more"></a><p>rocksdb主要组成部分memtable,sstfile,logfile，rocksdb 支持将database切分成多个columnFamily，所有的数据库创建如果没有指定的话会是一个default column<br>他支持批量原子写入，key和value都是纯byte流，key和value的大小都没有做限制</p><p>所有database中的数据都是以一个有序的形式被放置（怎么做的呢？后append的数据怎么有序），应用可以指定key的comparison方法，来指定key的排序方式，Iterator API可以在database做一个RangeScan操作，他会指向一个特定的key，然后进行一个一个遍历。在调用iterator的时候会创建database的即时视图，因此所有查询的key都是一致的。</p><h3 id="Snapshot"><a href="#Snapshot" class="headerlink" title="Snapshot"></a>Snapshot</h3><p>Snapshot Api也支持创建database某一时间点的视图，Get和Iterator Api可以用以读取指定snapshot的数据，从某种意义上说，snapshot和iterator都会提供database的当前视图，但是他们的实现不同。iterator是短期的/前台线程的scan，而长期/后台的scan最好是通过snapshot。iterator会对所有底层与pint-in-time database视图相关的的文件保留一个引用计数，这些文件知道iterator结束之后才会被删除。然而snapshot不会阻碍文件的删除，取而代之的是在compaction的过程会意识到snapshots的存在，直接不会删除在已经存在于snapshot中的key。snapshot在database重启的会丢失，reload rocksdb library会释放所有的snapshot</p><h3 id="Prefix-Iterators"><a href="#Prefix-Iterators" class="headerlink" title="Prefix Iterators"></a>Prefix Iterators</h3><p>大多数基于LSM设计的存储引擎都不太能支持高效的RangeScan API，因为他需要查阅每个文件，但是真正的应用并不是纯粹的随机读取key，一般会以一个key-prefix去查询，rocksdb利用了这个特点做了一些优化。应用可以配置prefix_extractor来指定key-prefix，rocksdb决定，存储的blooms，iterator可以通过ReadOptios指定prefix，然后rocksDB将会使用这些bloom bits来避免查询那些不包含那些key-perfix开始的key的文件。</p><h3 id="Persistence"><a href="#Persistence" class="headerlink" title="Persistence"></a>Persistence</h3><p>rocksdb有一个事务日志，所有的puts操作会被存储在memtables中，同时也会可选的写入事务日志中，在重启的时候，会重新执行事务日志中的记录。事务日志可以配置和sst文件放在不同的目录。这是因为有时你并不想持久化数据文件，同时又可以将事务日志持久化到一个相对较慢的持久化存储中，来确保数据不会丢失。 每一个Put都有一个标志，通过WriteOptios来标志是否需要写入事务日志中，同时也可以配置是否需要同步等到数据已经被写入到事务日志完成之后才将Put操作标记为commit完成。</p><p>在内部实现中，RocksDB会使用batch-commit的机制去批量的将事务操作提交到事务日志中，所以在一次同步调用中会提交多个transactions</p><h3 id="Fault-Tolerance"><a href="#Fault-Tolerance" class="headerlink" title="Fault Tolerance"></a>Fault Tolerance</h3><p>Rocksdb使用checksum去检测存储是否有损坏</p><h3 id="Multi-Threaded-Compactions"><a href="#Multi-Threaded-Compactions" class="headerlink" title="Multi-Threaded Compactions"></a>Multi-Threaded Compactions</h3><p>compaction的存在是为了删除同一个key的多个副本，这种情况发生在用户更新了某个key的值，compaction也负责将要删除的key进行删除。整个database是存储在sstable中的，当memtable满了的时候会写入到Level-0（L0）的文件中，RocksDB在将数据从memtable flush到文件的时候会先将重复的key进行删除。然后一些文件会周期性的读入并形成更大的文件，这就是compaction的过程。</p><p>对于一个LSM的database的写入的吞吐量取决于compaction所能达到的速度，特别是数据存储在ssd或者RAM中。RocksDB可以配置为启用多个并发compaction线程。据观察，与单线程compaction相比，基于ssd的数据库的持续写入速率可能在多线程compaction的情况下增加10倍之多</p><h3 id="compaction-Styles"><a href="#compaction-Styles" class="headerlink" title="compaction Styles"></a>compaction Styles</h3><p>通常的style的compaction是完全基于排序的，运行与L0文件或者L1+. Compaction会挑选一些按时间顺序相邻文件，然后将其合并成一个新的sstable</p><p>level style compaction在数据库存储会分为多个等级，最近的数据存储在L0层，最老的数据存储在Lmax层，只有L0层会存在重叠的key，一次compaction会将Ln的file和Ln+1的file做compaction然后形成新的文件替换Ln+1的文件，Universal Style 和Level style相比通常会有较低的写入放大但是较高的磁盘占用和读放大</p><p>（写入放大）：<br><a href="https://www.zhihu.com/question/31024021" target="_blank" rel="external">https://www.zhihu.com/question/31024021</a><br><a href="https://www.wikiwand.com/zh-hans/%E5%86%99%E5%85%A5%E6%94%BE%E5%A4%A7" target="_blank" rel="external">https://www.wikiwand.com/zh-hans/%E5%86%99%E5%85%A5%E6%94%BE%E5%A4%A7</a></p><p>同时RocksDB也支持用户自定义compaction方式，可以通过<code>Options.disable_auto_compaction</code>关闭原生的compaction算法，同时<code>GetLiveFilesMetaData</code>接口可以让外置组件查看每一个database中的数据文件从而决定哪些数据需要merge和合并。通过调用<code>CompactFiles</code>来进行文件的合并，<code>DeleteFile</code>来进行文件的删除</p><h3 id="metadata-storage"><a href="#metadata-storage" class="headerlink" title="metadata storage"></a>metadata storage</h3><p>数据库中的MANIFEST文件记录了数据库的状态，compaction线程会新增新文件，删除旧文件，这些操作会通过 MANIFEST记录来持久化，同样记录操作也是用了batch-commit的算法来缓冲重复的对MANIFEST文件的同步写入</p><h3 id="Avoiding-Stalls"><a href="#Avoiding-Stalls" class="headerlink" title="Avoiding Stalls"></a>Avoiding Stalls</h3><p>后台的compaction线程会将memtable中的内容flush到文件中，如果所有的后台线程都忙着做长时间的compaction，那么突然一个大流量的写入可能就会把memtable写满，这就会导致新的写入被hung住，这个问题可以通过配置rocksdb保持特定的几个线程专门保留来进行flush操作</p><h3 id="Compaction-Filter"><a href="#Compaction-Filter" class="headerlink" title="Compaction Filter"></a>Compaction Filter</h3><p>一些应用可能在compaction的时候可能期望对key做出一些处理，比如数据库内部实现可能需要支持TTL，来删除过期的key，这可以通过自定义实现compaction filter来实现。他提供了用户在compaction的过程中修改key value以及丢弃这个key的数据的能力</p><h3 id="Read-only-mode"><a href="#Read-only-mode" class="headerlink" title="Read only mode"></a>Read only mode</h3><p>database可以以read only的模式打开这样数据库保证所有的数据都不可修改，同时也会极大的提升读性能，因为完全了避免了锁</p><h3 id="Full-Backups-Incremental-Backups-and-Replication"><a href="#Full-Backups-Incremental-Backups-and-Replication" class="headerlink" title="Full Backups, Incremental Backups and Replication"></a>Full Backups, Incremental Backups and Replication</h3><p>RocksDB支持增量的备份，BackupableDB使得Rocksdb的备份很简单，后续会深入介绍</p><p>增量的复制需要能够找到数据库最近的改变，GetUpdatesSince API支持应用tail最近的事务日志，因此他能够连续的获取事务日志，然后将其应用于远程的复制或者备份</p><p>未完待续</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;本文翻译自：&lt;a href=&quot;https://github.com/facebook/rocksdb/wiki/rocksdb-basics&quot;&gt;https://github.com/facebook/rocksdb/wiki/rocksdb-basics&lt;/a&gt;&lt;/p&gt;&lt;p&gt;主要是rocksdb的一些概念理解和介绍&lt;/p&gt;
    
    </summary>
    
      <category term="刨根问底" scheme="http://www.aitozi.com/categories/%E5%88%A8%E6%A0%B9%E9%97%AE%E5%BA%95/"/>
    
    
      <category term="RocksDB" scheme="http://www.aitozi.com/tags/RocksDB/"/>
    
  </entry>
  
  <entry>
    <title>flink中状态实现的深入理解</title>
    <link href="http://www.aitozi.com/flink-state.html"/>
    <id>http://www.aitozi.com/flink-state.html</id>
    <published>2018-08-04T08:35:29.000Z</published>
    <updated>2018-08-05T04:42:13.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>本文是源于要在内部分享，所以提前整理了一些flink中的状态的一些知识，flink状态所包含的东西很多，在下面列举了一些，还有一<br>些在本文没有体现，后续会单独的挑出来再进行讲解</p><a id="more"></a><ul><li>state的层次结构</li><li>keyedState =&gt; windowState</li><li>OperatorState =&gt; kafkaOffset</li><li>stateBackend</li><li>snapshot/restore</li><li><em>internalTimerService</em></li><li><strong>RocksDB操作的初探</strong></li><li><em>state ttL</em></li><li><em>state local recovery</em></li><li><strong>QueryableState</strong></li><li><strong>increamental checkpoint</strong></li><li>state redistribution</li><li><em>broadcasting state</em></li><li><strong>CheckpointStreamFactory</strong></li></ul><hr><h3 id="内部和外部状态"><a href="#内部和外部状态" class="headerlink" title="内部和外部状态"></a>内部和外部状态</h3><p>flink状态分为了内部和外部使用接口，但是两个层级都是一一对应，内部接口都实现了外部接口，主要是有两个目的</p><ul><li>内部接口提供了更多的方法，包括获取state中的serialize之后的byte，以及Namespace的操作方法。内部状态主要用于内部runtime实现时所需要用到的一些状态比如window中的windowState，CEP中的sharedBuffer,kafkaConsumer中offset管理的ListState,而外部State接口主要是用户自定义使用的一些状态</li><li>考虑到各个版本的兼容性，外部接口要保障跨版本之间的兼容问题，而内部接口就很少受到这个限制，因此也就比较灵活</li></ul><p>层次结构图：</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-8-2/82936981.jpg" alt=""></p><h3 id="状态的使用"><a href="#状态的使用" class="headerlink" title="状态的使用"></a>状态的使用</h3><p>了解了flink 状态的层次结构，那么编程中和flink内部是如何使用这些状态呢？</p><p>flink中使用状态主要是两部分，一部分是函数中使用状态，另一部分是在operator中使用状态</p><p>方式：</p><ul><li>CheckpointedFunction</li><li>ListCheckpointed</li><li>RuntimeContext （DefaultKeyedStateStore）</li><li>StateContext</li></ul><p>StateContext</p><p>StateInitializationContext</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Iterable&lt;StatePartitionStreamProvider&gt; getRawOperatorStateInputs();</div><div class="line"></div><div class="line">Iterable&lt;KeyGroupStatePartitionStreamProvider&gt; getRawKeyedStateInputs();</div></pre></td></tr></table></figure><p>ManagedInitializationContext</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">OperatorStateStore getOperatorStateStore();</div><div class="line">KeyedStateStore getKeyedStateStore();</div></pre></td></tr></table></figure><p>举例：</p><ol><li><p>AbstractStreamOperator封装了这个方法<code>initializeState(StateInitializationContext context)</code>用以在operator中进行raw和managed的状态管理</p></li><li><p>CheckpointedFunction的用法其实也是借助于StateContext进行相关实现</p></li></ol><p><code>CheckpointedFunction#initializeState</code>方法在transformation function的各个并发实例初始化的时候被调用这个方法提供了<code>FunctionInitializationContext</code>的对象，可以通过这个<code>context</code>来获取<code>OperatorStateStore</code>或者<code>KeyedStateStore</code>，也就是说通过这个接口可以注册这两种类型的State，这也是和ListCheckpointed接口不一样的地方，只是说<code>KeyedStateStore</code>只能在keyedstream上才能注册，否则就会报错而已,以下是一个使用这两种类型状态的样例。 可以参见<code>FlinkKafkaConsumerBase</code>通过这个接口来实现offset的管理。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">public class MyFunction&lt;T&gt; implements MapFunction&lt;T, T&gt;, CheckpointedFunction &#123;</div><div class="line"></div><div class="line">     private ReducingState&lt;Long&gt; countPerKey;</div><div class="line">     private ListState&lt;Long&gt; countPerPartition;</div><div class="line"></div><div class="line">     private long localCount;</div><div class="line"></div><div class="line">     public void initializeState(FunctionInitializationContext context) throws Exception &#123;</div><div class="line">         // get the state data structure for the per-key state</div><div class="line">         countPerKey = context.getKeyedStateStore().getReducingState(</div><div class="line">                 new ReducingStateDescriptor&lt;&gt;(&quot;perKeyCount&quot;, new AddFunction&lt;&gt;(), Long.class));</div><div class="line"></div><div class="line">         // get the state data structure for the per-partition state</div><div class="line">         countPerPartition = context.getOperatorStateStore().getOperatorState(</div><div class="line">                 new ListStateDescriptor&lt;&gt;(&quot;perPartitionCount&quot;, Long.class));</div><div class="line"></div><div class="line">         // initialize the &quot;local count variable&quot; based on the operator state</div><div class="line">         for (Long l : countPerPartition.get()) &#123;</div><div class="line">             localCount += l;</div><div class="line">         &#125;</div><div class="line">     &#125;</div><div class="line"></div><div class="line">     public void snapshotState(FunctionSnapshotContext context) throws Exception &#123;</div><div class="line">         // the keyed state is always up to date anyways</div><div class="line">         // just bring the per-partition state in shape</div><div class="line">         countPerPartition.clear();</div><div class="line">         countPerPartition.add(localCount);</div><div class="line">     &#125;</div><div class="line"></div><div class="line">     public T map(T value) throws Exception &#123;</div><div class="line">         // update the states</div><div class="line">         countPerKey.add(1L);</div><div class="line">         localCount++;</div><div class="line"></div><div class="line">         return value;</div><div class="line">     &#125;</div><div class="line"> &#125;</div><div class="line"> &#125;</div></pre></td></tr></table></figure><p>这个Context的继承接口StateSnapshotContext的方法则提供了raw state的存储方法，但是其实没有对用户函数提供相应的接口，只是在引擎中有相关的使用，相比较而言这个接口提供的方法，context比较多，也有一些简单的方法去注册使用operatorstate 和 keyedState。如通过<code>RuntimeContext</code>注册keyedState:</p><p>因此使用简易化程度为:</p><blockquote><p>RuntimeContext &gt; FunctionInitializationContext &gt; StateSnapshotContext</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">keyedStream.map(new RichFlatMapFunction&lt;MyType, List&lt;MyType&gt;&gt;() &#123;</div><div class="line"></div><div class="line">     private ListState&lt;MyType&gt; state;</div><div class="line"></div><div class="line">     public void open(Configuration cfg) &#123;</div><div class="line">         state = getRuntimeContext().getListState(</div><div class="line">                 new ListStateDescriptor&lt;&gt;(&quot;myState&quot;, MyType.class));</div><div class="line">     &#125;</div><div class="line"></div><div class="line">     public void flatMap(MyType value, Collector&lt;MyType&gt; out) &#123;</div><div class="line">         if (value.isDivider()) &#123;</div><div class="line">             for (MyType t : state.get()) &#123;</div><div class="line">                 out.collect(t);</div><div class="line">             &#125;</div><div class="line">         &#125; else &#123;</div><div class="line">             state.add(value);</div><div class="line">         &#125;</div><div class="line">     &#125;</div><div class="line"> &#125;);</div></pre></td></tr></table></figure><p>通过实现<code>ListCheckpointed</code>来注册OperatorState，但是这个有限制：<br>一个function只能注册一个state，因为并不能像其他接口一样指定state的名字.</p><p>example：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">public class CountingFunction&lt;T&gt; implements MapFunction&lt;T, Tuple2&lt;T, Long&gt;&gt;, ListCheckpointed&lt;Long&gt; &#123;</div><div class="line"></div><div class="line">     // this count is the number of elements in the parallel subtask</div><div class="line">     private long count;</div><div class="line"></div><div class="line">     &#123;@literal @&#125;Override</div><div class="line">     public List&lt;Long&gt; snapshotState(long checkpointId, long timestamp) &#123;</div><div class="line">         // return a single element - our count</div><div class="line">         return Collections.singletonList(count);</div><div class="line">     &#125;</div><div class="line"></div><div class="line">     &#123;@literal @&#125;Override</div><div class="line">     public void restoreState(List&lt;Long&gt; state) throws Exception &#123;</div><div class="line">         // in case of scale in, this adds up counters from different original subtasks</div><div class="line">         // in case of scale out, list this may be empty</div><div class="line">         for (Long l : state) &#123;</div><div class="line">             count += l;</div><div class="line">         &#125;</div><div class="line">     &#125;</div><div class="line"></div><div class="line">     &#123;@literal @&#125;Override</div><div class="line">     public Tuple2&lt;T, Long&gt; map(T value) &#123;</div><div class="line">         count++;</div><div class="line">         return new Tuple2&lt;&gt;(value, count);</div><div class="line">     &#125;</div><div class="line"> &#125;</div><div class="line"> &#125;</div></pre></td></tr></table></figure><p>下面比较一下里面的两种stateStore</p><ul><li>KeyedStateStore</li><li>OperatorStateStore</li></ul><p>查看OperatorStateStore接口可以看到OperatorState只提供了ListState一种形式的状态接口,OperatorState和KeyedState主要有以下几个区别：</p><ul><li>keyedState只能应用于KeyedStream，而operatorState都可以</li><li>keyedState可以理解成一个算子为每个subtask的每个key维护了一个状态namespace，而OperatorState是每个subtask共享一个状态</li><li>operatorState只提供了ListState，而keyedState提供了<code>ValueState</code>,<code>ListState</code>,<code>ReducingState</code>,<code>MapState</code></li><li>operatorStateStore的默认实现只有<code>DefaultOperatorStateBackend</code>可以看到他的状态都是存储在堆内存之中，而keyedState根据backend配置的不同，线上都是存储在rocksdb之中</li></ul><h3 id="snapshot"><a href="#snapshot" class="headerlink" title="snapshot"></a>snapshot</h3><p>这个让我们着眼于两个Operator的snapshot，<code>AbstractStreamOperator</code> 和 <code>AbstractUdfStreamOperator</code>,这两个基类几乎涵盖了所有相关operator和function在做snapshot的时候会做的处理。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">if (null != operatorStateBackend) &#123;</div><div class="line">				snapshotInProgress.setOperatorStateManagedFuture(</div><div class="line">					operatorStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions));</div><div class="line">			&#125;</div><div class="line"></div><div class="line">			if (null != keyedStateBackend) &#123;</div><div class="line">				snapshotInProgress.setKeyedStateManagedFuture(</div><div class="line">					keyedStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions));</div><div class="line">&#125;</div></pre></td></tr></table></figure><ol><li>按keyGroup去snapshot各个timerService的状态，包括processingTimer和eventTimer（RawKeyedOperatorState）</li><li>将operatorStateBackend和keyedStateBackend中的状态做snapshot</li><li>如果Operator还包含了userFunction，即是一个<code>UdfStreamOperator</code>,那么可以注意到udfStreamOperator覆写了父类的<code>snapshotState(StateSnapshotContext context)</code>方法，其主要目的就是为了将Function中的状态及时的register到相应的backend中，在第二步的时候统一由<code>CheckpointStreamFactory</code>去做快照</li></ol><h4 id="StreamingFunctionUtils-snapshotFunctionState"><a href="#StreamingFunctionUtils-snapshotFunctionState" class="headerlink" title="StreamingFunctionUtils#snapshotFunctionState"></a>StreamingFunctionUtils#snapshotFunctionState</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">if (userFunction instanceof CheckpointedFunction) &#123;</div><div class="line">			((CheckpointedFunction) userFunction).snapshotState(context);</div><div class="line"></div><div class="line">			return true;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		if (userFunction instanceof ListCheckpointed) &#123;</div><div class="line">			@SuppressWarnings(&quot;unchecked&quot;)</div><div class="line">			List&lt;Serializable&gt; partitionableState = ((ListCheckpointed&lt;Serializable&gt;) userFunction).</div><div class="line">				snapshotState(context.getCheckpointId(), context.getCheckpointTimestamp());</div><div class="line"></div><div class="line">			ListState&lt;Serializable&gt; listState = backend.</div><div class="line">				getSerializableListState(DefaultOperatorStateBackend.DEFAULT_OPERATOR_STATE_NAME);</div><div class="line"></div><div class="line">			listState.clear();</div><div class="line"></div><div class="line">			if (null != partitionableState) &#123;</div><div class="line">				try &#123;</div><div class="line">					for (Serializable statePartition : partitionableState) &#123;</div><div class="line">						listState.add(statePartition);</div><div class="line">					&#125;</div><div class="line">				&#125; catch (Exception e) &#123;</div><div class="line">					listState.clear();</div><div class="line"></div><div class="line">					throw new Exception(&quot;Could not write partitionable state to operator &quot; +</div><div class="line">						&quot;state backend.&quot;, e);</div><div class="line">				&#125;</div><div class="line">			&#125;</div></pre></td></tr></table></figure><p>可以看到这里就只有以上分析的两种类型的checkpoined接口，<code>CheckpointedFunction</code>，只需要执行相应的snapshot方法，相应的函数就已经将要做snapshot的数据打入了相应的state中，而<code>ListCheckpointed</code>接口由于返回的是个List，所以需要手动的通过<code>getSerializableListState</code>注册一个<code>ListState</code>(<em>这也是ListCheckpointed只能注册一个state的原因</em>),然后将List数据挨个存入ListState中。</p><h4 id="operatorStateBackend-snapshot"><a href="#operatorStateBackend-snapshot" class="headerlink" title="operatorStateBackend#snapshot"></a>operatorStateBackend#snapshot</h4><ol><li>针对所有注册的state作deepCopy,为了防止在checkpoint的时候数据结构又被修改，deepcopy其实是通过序列化和反序列化的过程（参见<a href="http://aitozi.com/java-serialization.html" target="_blank" rel="external">http://aitozi.com/java-serialization.html</a>）</li><li>异步将state以及metainfo的数据写入到hdfs中，使用的是flink的asyncIO（这个也可以后续深入了解下），并返回相应的statehandle用作restore的过程</li><li>在StreamTask触发checkpoint的时候会将一个Task中所有的operator触发一次snapshot，触发部分就是上面1，2两个步骤，其中第二步是会返回一个RunnableFuture，在触发之后会提交一个<code>AsyncCheckpointRunnable</code>异步任务，会阻塞一直等到checkpoint的<code>Future</code>，其实就是去调用这个方法<code>AbstractAsyncIOCallable</code>, 直到完成之后OperatorState会返回一个<code>OperatorStateHandle</code>,这个地方和后文的keyedState返回的handle不一样。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">@Override</div><div class="line">	public V call() throws Exception &#123;</div><div class="line"></div><div class="line">		synchronized (this) &#123;</div><div class="line">			if (isStopped()) &#123;</div><div class="line">				throw new IOException(&quot;Task was already stopped. No I/O handle opened.&quot;);</div><div class="line">			&#125;</div><div class="line"></div><div class="line">			ioHandle = openIOHandle();</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		try &#123;</div><div class="line"></div><div class="line">			return performOperation();</div><div class="line"></div><div class="line">		&#125; finally &#123;</div><div class="line">			closeIOHandle();</div><div class="line">		&#125;</div></pre></td></tr></table></figure><p>在managed keyedState、managed operatorState、raw keyedState、和raw operatorState都完成返回相应的Handle之后，会生成一个SubTaskState来ack jobmanager,这个主要是用在restore的过程中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">SubtaskState subtaskState = createSubtaskStateFromSnapshotStateHandles(</div><div class="line">					chainedNonPartitionedOperatorsState,</div><div class="line">					chainedOperatorStateBackend,</div><div class="line">					chainedOperatorStateStream,</div><div class="line">					keyedStateHandleBackend,</div><div class="line">					keyedStateHandleStream);</div><div class="line">					</div><div class="line">owner.getEnvironment().acknowledgeCheckpoint(</div><div class="line">	checkpointMetaData.getCheckpointId(),</div><div class="line">	checkpointMetrics,</div><div class="line">	subtaskState);</div></pre></td></tr></table></figure><p>在jm端，ack的时候又将各个handle封装在<code>pendingCheckpoint =&gt; operatorStates =&gt; operatorState =&gt; operatorSubtaskState</code>中,最后无论是savepoint或者是externalCheckpoint都会将相应的handle序列化存储到hdfs，这也就是所谓的checkpoint元数据。这个可以起个任务观察下zk和hdfs上的文件，补充一下相关的验证。</p><p>至此完成operator state的snapshot/checkpoint阶段</p><h4 id="KeyedStateBackend-snapshot"><a href="#KeyedStateBackend-snapshot" class="headerlink" title="KeyedStateBackend#snapshot"></a>KeyedStateBackend#snapshot</h4><p>和operatorStateBackend一样，snapshot也分为了同步和异步两个部分。</p><ol><li>rocksDB的keyedStateBackend的snapshot提供了增量和全量两种方式</li><li>利用rocksdb自身的snapshot进行<code>this.snapshot = stateBackend.db.getSnapshot();</code> 这个过程是同步的，rocksdb这块是怎么snapshot还不是很了解，待后续学习</li><li>之后也是一样异步将数据写入hdfs，返回相应的keyGroupsStateHandle <code>snapshotOperation.closeCheckpointStream();</code></li></ol><p>不同的地方在于增量返回的是<code>IncrementalKeyedStateHandle</code>,而全量返回的是<code>KeyGroupsStateHandle</code>，</p><h3 id="restore-redistribution"><a href="#restore-redistribution" class="headerlink" title="restore / redistribution"></a>restore / redistribution</h3><h4 id="OperatorState的rescale"><a href="#OperatorState的rescale" class="headerlink" title="OperatorState的rescale"></a>OperatorState的rescale</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">void setInitialState(TaskStateHandles taskStateHandles) throws Exception;</div></pre></td></tr></table></figure><p>一个task在真正的执行任务之前所需要做的事情是把状态inject到task中，如果一个任务是失败之后从上次的checkpoint点恢复的话，他的状态就是非空的。streamTask也就靠是否有这样的一个恢复状态来确认算子是不是在restore来branch他的启动逻辑</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">if (null != taskStateHandles) &#123;</div><div class="line">		if (invokable instanceof StatefulTask) &#123;</div><div class="line">			StatefulTask op = (StatefulTask) invokable;</div><div class="line">			op.setInitialState(taskStateHandles);</div><div class="line">		&#125; else &#123;</div><div class="line">			throw new IllegalStateException(&quot;Found operator state for a non-stateful task invokable&quot;);</div><div class="line">		&#125;</div><div class="line">		// be memory and GC friendly - since the code stays in invoke() for a potentially long time,</div><div class="line">		// we clear the reference to the state handle</div><div class="line">		//noinspection UnusedAssignment</div><div class="line">		taskStateHandles = null;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>那么追根究底一下这个Handle是怎么带入的呢？</p><p><code>FixedDelayRestartStrategy =&gt; triggerFullRecovery =&gt; Execution#restart =&gt; Execution#scheduleForExecution =&gt; Execution#deployToSlot =&gt; ExecutionVertex =&gt; TaskDeploymentDescriptor =&gt; taskmanger =&gt; task</code></p><p>当然还有另一个途径就是通过向jobmanager submitJob的时候带入restore的checkpoint path， 这两种方式最终都会通过<code>checkpointCoordinator#restoreLatestCheckpointedState</code>来恢复hdfs中的状态来获取到snapshot时候存入的StateHandle。</p><p>恢复的过程如何进行redistribution呢？ 也就是大家关心的并发度变了我的状态的行为是怎么样的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">// re-assign the task states</div><div class="line">final Map&lt;OperatorID, OperatorState&gt; operatorStates = latest.getOperatorStates();</div><div class="line"></div><div class="line">StateAssignmentOperation stateAssignmentOperation =</div><div class="line">		new StateAssignmentOperation(tasks, operatorStates, allowNonRestoredState);</div><div class="line"></div><div class="line">stateAssignmentOperation.assignStates();</div></pre></td></tr></table></figure><ol><li>如果并发度没变那么不做重新的assign，除非state的模式是broadcast，会将一个task的state广播给所有的task</li><li>对于operator state会针对每一个name的state计算出每个subtask中的element个数之和（这就要求每个element之间相互独立）进行roundrobin分配</li><li>keyedState的重新分配相对简单，就是根据新的并发度和最大并发度计算新的keygroupRange，然后根据subtaskIndex获取keyGroupRange，然后获取到相应的keyStateHandle完成状态的切分。</li></ol><p>这里补充关于raw state和managed state在rescale上的差别，由于operator state在reassign的时候是根据metaInfo来计算出所有的List<element>来重新分配，operatorbackend中注册的状态是会保存相应的metainfo，最终也会在snapshot的时候存入OperatorHandle，那raw state的metainfo是在哪里呢？</element></p><p>其实会在写入hdfs返回相应的handle的时候构建一个默认的，<code>OperatorStateCheckpointOutputStream#closeAndGetHandle</code>,其中状态各个partition的构建来自<code>startNewPartition</code>方法，引擎中我所看到的rawstate仅有timerservice的raw keyedState</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">OperatorStateHandle closeAndGetHandle() throws IOException &#123;</div><div class="line">		StreamStateHandle streamStateHandle = delegate.closeAndGetHandle();</div><div class="line"></div><div class="line">		if (null == streamStateHandle) &#123;</div><div class="line">			return null;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		if (partitionOffsets.isEmpty() &amp;&amp; delegate.getPos() &gt; initialPosition) &#123;</div><div class="line">			startNewPartition();</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		Map&lt;String, OperatorStateHandle.StateMetaInfo&gt; offsetsMap = new HashMap&lt;&gt;(1);</div><div class="line"></div><div class="line">		OperatorStateHandle.StateMetaInfo metaInfo =</div><div class="line">				new OperatorStateHandle.StateMetaInfo(</div><div class="line">						partitionOffsets.toArray(),</div><div class="line">						OperatorStateHandle.Mode.SPLIT_DISTRIBUTE);</div><div class="line"></div><div class="line">		offsetsMap.put(DefaultOperatorStateBackend.DEFAULT_OPERATOR_STATE_NAME, metaInfo);</div><div class="line"></div><div class="line">		return new OperatorStateHandle(offsetsMap, streamStateHandle);</div><div class="line">	&#125;</div></pre></td></tr></table></figure><h4 id="KeyedState的keyGroup"><a href="#KeyedState的keyGroup" class="headerlink" title="KeyedState的keyGroup"></a>KeyedState的keyGroup</h4><p>keyedState重新分配里引入了一个keyGroup的概念，那么这里为什么要引入keygroup这个概念呢？</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-8-3/53377760.jpg" alt=""></p><p><img src="http://or0igopk2.bkt.clouddn.com/18-8-3/18711916.jpg" alt=""></p><ol><li>hash(key) = key(identity)</li><li>key_group(key) = hash(key) % number_of_key_groups (等于最大并发)，默认flink任务会设置一个max parallel</li><li>subtask(key) = key_greoup(key) * parallel / number_of_key_groups</li></ol><ul><li>避免在恢复的时候带来随机IO</li><li>避免每个subtask需要将所有的状态数据读取出来pick和自己subtask相关的浪费了很多io资源</li><li>减少元数据的量，不再需要保存每次的key，每一个keygroup组只需保留一个range</li></ul><p>实际实现上的keyGroup range和上图有区别，是连续的:</p><p>比如：subtask1: [0-10], subtask2: [11-12] …</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">int start = operatorIndex == 0 ? 0 : ((operatorIndex * maxParallelism - 1) / parallelism) + 1;</div><div class="line">int end = ((operatorIndex + 1) * maxParallelism - 1) / parallelism;</div><div class="line">return new KeyGroupRange(start, end);</div></pre></td></tr></table></figure><ul><li>每一个backend（subtask）上只有一个keygroup range</li><li>每一个subtask在restore的时候就接收到了已经分配好的和重启后当前这个并发相绑定的keyStateHandle</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">subManagedKeyedState = getManagedKeyedStateHandles(operatorState, keyGroupPartitions.get(subTaskIndex));</div><div class="line">subRawKeyedState = getRawKeyedStateHandles(operatorState, keyGroupPartitions.get(subTaskIndex));</div></pre></td></tr></table></figure><p>这里面关键的一步在于，根据新的subtask上的keyGroupRange，从原来的operator的keyGroupsStateHandle中求取本subtask所关心的一部分Handle，可以看到每个KeyGroupsStateHandle都维护了<code>KeyGroupRangeOffsets</code>这样一个变量，来标记这个handle所覆盖的keygrouprange，以及keygrouprange在stream中offset的位置，可以看下再snapshot的时候会记录offset到这个对象中来</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">keyGroupRangeOffsets.setKeyGroupOffset(mergeIterator.keyGroup(), outStream.getPos());</div></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">public KeyGroupRangeOffsets getIntersection(KeyGroupRange keyGroupRange) &#123;</div><div class="line">		Preconditions.checkNotNull(keyGroupRange);</div><div class="line">		KeyGroupRange intersection = this.keyGroupRange.getIntersection(keyGroupRange);</div><div class="line">		long[] subOffsets = new long[intersection.getNumberOfKeyGroups()];</div><div class="line">		if(subOffsets.length &gt; 0) &#123;</div><div class="line">			System.arraycopy(</div><div class="line">					offsets,</div><div class="line">					computeKeyGroupIndex(intersection.getStartKeyGroup()),</div><div class="line">					subOffsets,</div><div class="line">					0,</div><div class="line">					subOffsets.length);</div><div class="line">		&#125;</div><div class="line">		return new KeyGroupRangeOffsets(intersection, subOffsets);</div><div class="line">	&#125;</div></pre></td></tr></table></figure><p>KeyGroupsStateHandle是一个subtask的所有state的一个handle<br>KeyGroupsStateHandle维护一个KeyGroupRangeOffsets，<br>KeyGroupRangeOffsets维护一个KeyGroupRange和offsets<br>KeyGroupRange维护多个KeyGroup<br>KeyGroup维护多个key</p><p>KeyGroupsStateHandle和operatorStateHandle还有一个不同点，operatorStateHandle维护了metainfo中的offset信息用在restore时的reassign，原因在于KeyGroupsStateHandle的reassign不依赖这些信息，当然在restore的时候也需要keygroupOffset中的offset信息来重新构建keyGroupsStateHandle来进行各个task的状态分配。</p><p>参考：</p><p><a href="https://flink.apache.org/features/2017/07/04/flink-rescalable-state.html" target="_blank" rel="external">https://flink.apache.org/features/2017/07/04/flink-rescalable-state.html</a></p><p><a href="http://chenyuzhao.me/2017/12/24/Flink-%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E7%9A%84%E8%AE%BE%E8%AE%A1-%E5%AD%98%E5%82%A8/" target="_blank" rel="external">http://chenyuzhao.me/2017/12/24/Flink-%E5%88%86%E5%B8%83%E5%BC%8F%E5%BF%AB%E7%85%A7%E7%9A%84%E8%AE%BE%E8%AE%A1-%E5%AD%98%E5%82%A8/</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;本文是源于要在内部分享，所以提前整理了一些flink中的状态的一些知识，flink状态所包含的东西很多，在下面列举了一些，还有一&lt;br&gt;些在本文没有体现，后续会单独的挑出来再进行讲解&lt;/p&gt;
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Protobuf深入理解</title>
    <link href="http://www.aitozi.com/dig-protobuf.html"/>
    <id>http://www.aitozi.com/dig-protobuf.html</id>
    <published>2018-07-28T15:28:01.000Z</published>
    <updated>2019-03-14T17:06:49.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>本文带你深入理解和使用protobuf</p><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Protocol Buffers 是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，或者说序列化。它很适合做数据存储或 RPC 数据交换格式。可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式。目前提供了 C++、Java、Python、Go 等语言的 API</p><p>使用protobuf需要这样几个步骤：</p><ul><li>在<code>.proto</code>文件中定义消息的格式</li><li>通过protoBuffer compiler编译生成相应的java类</li><li>通过Java protocol buffer Api来write和read相关的对象</li></ul><p>关于PB的操作方式见： <a href="https://developers.google.com/protocol-buffers/docs/javatutorial" target="_blank" rel="external">https://developers.google.com/protocol-buffers/docs/javatutorial</a></p><hr><p>protobuf的序列化快的原因主要在于其编码实现和封解包的速度</p><h2 id="Protobuf编码"><a href="#Protobuf编码" class="headerlink" title="Protobuf编码"></a>Protobuf编码</h2><h3 id="Base-128-Varints-编码"><a href="#Base-128-Varints-编码" class="headerlink" title="Base 128 Varints 编码"></a>Base 128 Varints 编码</h3><p>数据传输中出于IO的考虑，我们会希望尽可能的对数据进行压缩。<br>Varint就是一种对数字进行编码的方法，编码后二进制数据是不定长的，数值越小的数字使用的字节数越少。例如对于int32_t，采用Varint编码后需要1~5个bytes，小的数字使用1个byte，大的数字使用5个bytes。基于实际场景中小数字的使用远远多于大数字，因此通过Varint编码对于大部分场景都可以起到一个压缩的效果。Varint的主要想法就是以标志位替换掉高字节的若干个0</p><p>下图是数字131415的variant编码,通过3个字节来表示131415<br><img src="http://or0igopk2.bkt.clouddn.com/18-7-28/50478975.jpg" alt=""></p><p>其中第一个字节的高位msb（Most Significant Bit ）为1表示下一个字节还有有效数据，msb为0表示该字节中的后7为是最后一组有效数字。踢掉最高位后的有效位组成真正的数字。注意到最终计算前将两个 byte 的位置相互交换过一次，这是因为 Google Protocol Buffer 字节序采用 little-endian（即低位字节排放在内存的低地址端） 的方式</p><p>从上面可以看出，variant编码存储比较小的整数时很节省空间，小于等于127的数字可以用一个字节存储。但缺点是对于大于</p><p>268,435,455（0xfffffff）的整数需要5个字节来存储。但是一般情况下（尤其在tag编码中）不会存储这么大的整数。</p><p>关于int32的varint编码代码</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">char</span>* <span class="title">EncodeVarint32</span><span class="params">(<span class="keyword">char</span>* dst, <span class="keyword">uint32_t</span> v)</span> </span>&#123;</div><div class="line">  <span class="comment">// Operate on characters as unsigneds</span></div><div class="line">  <span class="keyword">unsigned</span> <span class="keyword">char</span>* ptr = <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">unsigned</span> <span class="keyword">char</span>*&gt;(dst);</div><div class="line">  <span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> B = <span class="number">128</span>;</div><div class="line">  <span class="keyword">if</span> (v &lt; (<span class="number">1</span>&lt;&lt;<span class="number">7</span>)) &#123;</div><div class="line">    *(ptr++) = v;</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span>&lt;&lt;<span class="number">14</span>)) &#123;</div><div class="line">    *(ptr++) = v | B;</div><div class="line">    *(ptr++) = v&gt;&gt;<span class="number">7</span>;</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span>&lt;&lt;<span class="number">21</span>)) &#123;</div><div class="line">    *(ptr++) = v | B;</div><div class="line">    *(ptr++) = (v&gt;&gt;<span class="number">7</span>) | B;</div><div class="line">    *(ptr++) = v&gt;&gt;<span class="number">14</span>;</div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (v &lt; (<span class="number">1</span>&lt;&lt;<span class="number">28</span>)) &#123;</div><div class="line">    *(ptr++) = v | B;</div><div class="line">    *(ptr++) = (v&gt;&gt;<span class="number">7</span>) | B;</div><div class="line">    *(ptr++) = (v&gt;&gt;<span class="number">14</span>) | B;</div><div class="line">    *(ptr++) = v&gt;&gt;<span class="number">21</span>;</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    *(ptr++) = v | B;</div><div class="line">    *(ptr++) = (v&gt;&gt;<span class="number">7</span>) | B;</div><div class="line">    *(ptr++) = (v&gt;&gt;<span class="number">14</span>) | B;</div><div class="line">    *(ptr++) = (v&gt;&gt;<span class="number">21</span>) | B;</div><div class="line">    *(ptr++) = v&gt;&gt;<span class="number">28</span>;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> <span class="keyword">reinterpret_cast</span>&lt;<span class="keyword">char</span>*&gt;(ptr);</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="Message-Structure编码"><a href="#Message-Structure编码" class="headerlink" title="Message Structure编码"></a>Message Structure编码</h3><p>protocol buffer 中 message 是一系列键值对。message 的二进制版本只是使用字段号(field’s number 和 wire_type)作为 key。每个字段的名称和声明类型只能在解码端通过引用消息类型的定义（即 .proto 文件）来确定。这一点也是人们常常说的 protocol buffer 比 JSON，XML 安全一点的原因，如果没有数据结构描述 .proto 文件，拿到数据以后是无法解释成正常的数据的。</p><p>当消息编码时，键和值被连接成一个字节流。当消息被解码时，解析器需要能够跳过它无法识别的字段。这样，可以将新字段添加到消息中，而不会破坏不知道它们的旧程序。这就是所谓的 “向后”兼容性。</p><p>为此，线性的格式消息中每对的“key”实际上是两个值，其中一个是来自.proto文件的字段编号，加上提供正好足够的信息来查找下一个值的长度。在大多数语言实现中，这个 key 被称为 tag</p><p>wireType</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-7-28/87891770.jpg" alt=""></p><p>key 的计算方法是 (field_number &lt;&lt; 3) | wire_type，换句话说，key 的最后 3 位表示的就是 wire_type。因此这里也涉及到前面proto文件定义的时候的宗旨，尽量将频繁使用的字段的字段号设置成1-15之间的数值，避免位数开销。这里的key的存储也是用了varint的方式</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-7-28/94576622.jpg" alt=""></p><p>举例，一般 message 的字段号都是 1 开始的，所以对应的 tag 可能是这样的：</p><p><code>000 1000</code></p><p>末尾3位表示的是value的类型，这里是000，即0，代表的是varint值。右移3位，即0001，这代表的就是字段号(field number)。tag的例子就举这么多，接下来举一个 value的例子，还是用varint来举例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">96 01 = 1001 0110  0000 0001</div><div class="line">       → 000 0001  ++  001 0110 (drop the msb and reverse the groups of 7 bits)</div><div class="line">       → 10010110</div><div class="line">       → 128 + 16 + 4 + 2 = 150</div></pre></td></tr></table></figure><p>所以 96 01 代表的数据就是 150 。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">message Test1 &#123;</div><div class="line">  required int32 a = 1;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>如果存在上面这样的一个 message 的结构，如果存入 150，在 Protocol Buffer 中显示的二进制应该为 08 96 01 <code>varint(1 &lt;&lt; 3 | 0) = 0x08</code>.</p><p>注意到varint编码也应用在了key的计算上，使用非常频繁，或许是基于这个原因，pb里实现了一种性能更高的方案（coded_stream.cc）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">inline uint8* CodedOutputStream::WriteVarint32FallbackToArrayInline(</div><div class="line">    uint32 value, uint8* target) &#123;</div><div class="line">  target[0] = static_cast&lt;uint8&gt;(value | 0x80);</div><div class="line">  if (value &gt;= (1 &lt;&lt; 7)) &#123;</div><div class="line">    target[1] = static_cast&lt;uint8&gt;((value &gt;&gt;  7) | 0x80);</div><div class="line">    if (value &gt;= (1 &lt;&lt; 14)) &#123;</div><div class="line">      target[2] = static_cast&lt;uint8&gt;((value &gt;&gt; 14) | 0x80);</div><div class="line">      if (value &gt;= (1 &lt;&lt; 21)) &#123;</div><div class="line">        target[3] = static_cast&lt;uint8&gt;((value &gt;&gt; 21) | 0x80);</div><div class="line">        if (value &gt;= (1 &lt;&lt; 28)) &#123;</div><div class="line">          target[4] = static_cast&lt;uint8&gt;(value &gt;&gt; 28);</div><div class="line">          return target + 5;</div><div class="line">        &#125; else &#123;</div><div class="line">          target[3] &amp;= 0x7F;</div><div class="line">          return target + 4;</div><div class="line">        &#125;</div><div class="line">      &#125; else &#123;</div><div class="line">        target[2] &amp;= 0x7F;</div><div class="line">        return target + 3;</div><div class="line">      &#125;</div><div class="line">    &#125; else &#123;</div><div class="line">      target[1] &amp;= 0x7F;</div><div class="line">      return target + 2;</div><div class="line">    &#125;</div><div class="line">  &#125; else &#123;</div><div class="line">    target[0] &amp;= 0x7F;</div><div class="line">    return target + 1;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>测试了1kw条数据，两种方案的时间对比为 196742us vs 269806us，在pb序列化反序列化大量使用varint的前提下，这个性能提升就很有必要了(这是原作者做的测试)</p><p>type 需要注意的是 type = 2 的情况，tag 里面除了包含 field number 和 wire_type ，还需要再包含一个 length，决定 value 从那一段取出来</p><h3 id="负数使用varint编码的问题"><a href="#负数使用varint编码的问题" class="headerlink" title="负数使用varint编码的问题"></a>负数使用varint编码的问题</h3><p>varint编码希望以标志位能够节省掉高字节的0，但是负数的最高位一定是1， 所以varint在处理32位负数时会固定的占用5个字节。比如我们修改下之前的程序test.set_a(-1)，序列化之后的数据为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">08ff ffff ffff ffff ffff 01</div></pre></td></tr></table></figure><p>有11个字节之多！除了key=0x08占用的1个字节，value=-1占用了10个字节。</p><p>对应的代码（coded_stream.h）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">inline void CodedOutputStream::WriteVarint32SignExtended(int32 value) &#123;</div><div class="line">  if (value &lt; 0) &#123;</div><div class="line">    WriteVarint64(static_cast&lt;uint64&gt;(value));</div><div class="line">  &#125; else &#123;</div><div class="line">    WriteVarint32(static_cast&lt;uint32&gt;(value));</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>int32被转换成了uint64(为什么？)原作者这里问为什么== 原因在文档中有提及</p><blockquote><p>If you use int32 or int64 as the type for a negative number, the resulting varint is always ten bytes long – it is, effectively, treated like a very large unsigned integer,即uint64，也就是上面代码写的那样。但是为什么生成是10个字节呢? 因为uint64是10个?</p></blockquote><p>再经过varint编码。这就是10个字节的原因了。当然如果你使用了signed types那么产出的varint编码结果使用了Zigzag编码就会相当的高效。</p><h3 id="Zigzag编码"><a href="#Zigzag编码" class="headerlink" title="Zigzag编码"></a>Zigzag编码</h3><p>ZigZag是将有符号数统一映射到无符号数的一种编码方案，对于无符号数0 1 2 3 4，映射前的有符号数分别为0 -1 1 -2 2，负数以及对应的正数来回映射到从0变大的数字序列里，这也是”zig-zag”的名字来源。将所有整数映射成无符号整数，然后再采用 varint 编码方式编码，这样，绝对值小的整数，编码后也会有一个较小的 varint 编码值。</p><p>Zigzag 映射函数为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Zigzag(n) = (n &lt;&lt; 1) ^ (n &gt;&gt; 31), n 为 sint32 时</div><div class="line">Zigzag(n) = (n &lt;&lt; 1) ^ (n &gt;&gt; 63), n 为 sint64 时</div></pre></td></tr></table></figure><p>按照这种方法，-1 将会被编码成 1，1 将会被编码成 2，-2 会被编码成 3，如下表所示：</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-7-28/17159197.jpg" alt=""></p><p>存疑？</p><p>目前仍有一个地方不大清楚，就是对于int32类型的负数，protobuf强制编码成10个字节，理论上5个字节就够了。 （来自别人的问题，我也没懂，确实想了下int32的负数5个就够了，int64的负数才需要10个？）</p><h3 id="负数及大整数的解决方案"><a href="#负数及大整数的解决方案" class="headerlink" title="负数及大整数的解决方案"></a>负数及大整数的解决方案</h3><p>protobuf里提供了一种sint32/sint64来使用ZigZag编码。</p><p>修改proto:optional sint32 a = 1，这样在test.set_a(-1)并序列化后只有两个字节08 01</p><p>同理对于大整数，optional int32 a = 1;，test.set_a(1 &lt;&lt; 28)序列化后可以看到占用了6个字节0880 8080 8001，解决方案也是使用不同的类型定义optional <strong>fixed32</strong> a = 1来解决，使用这种方案后int32固定的占用4个字节。这种其实就是官网中的<code>Non-varint Numbers</code></p><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>wire_type 类型为 2 的数据，是一种指定长度的编码方式：key + length + content，key 的编码方式是统一的，length 采用 varints 编码方式，content 就是由 length 指定长度的 Bytes</p><p>举例，假设定义如下的 message 格式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">message Test2 &#123;</div><div class="line">  optional string b = 2;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>设置该值为”testing”，二进制格式查看：<code>12 07 74 65 73 74 69 6e 67</code>, <code>74 65 73 74 69 6e 67</code> 是“testing”的 UTF8 代码。</p><p>12 -&gt; 0001 0010，后三位 010 为 wire type = 2，0001 0010 右移三位为 0000 0010，即 tag = 2。</p><p>length 此处为 7，后边跟着 7 个bytes，即我们的字符串”testing”。</p><p>所以 wire_type 类型为 2 的数据，编码的时候会默认转换为 T-L-V (Tag - Length - Value)的形式. TLV的模式减少了分隔符的使用，数据存储更加紧凑。需要转变为 T - L - V 形式的还有 string, bytes, embedded messages, packed repeated fields。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ul><li>Protocol Buffer 利用 varint 原理压缩数据以后，二进制数据非常紧凑，option 也算是压缩体积的一个举措。所以 pb 体积更小，如果选用它作为网络数据传输，势必相同数据，消耗的网络流量更少。但是并没有压缩到极限，float、double 浮点型都没有压缩。</li><li>Protocol Buffer 比 JSON 和 XML 少了 {、}、: 这些符号，体积也减少一些。再加上 varint 压缩，gzip 压缩以后体积更小！</li><li>Protocol Buffer 是 Tag - Value (Tag - Length - Value)的编码方式的实现，减少了分隔符的使用，数据存储更加紧凑。</li><li>Protocol Buffer 另外一个核心价值在于提供了一套工具，一个编译工具，自动化生成 get/set 代码。简化了多语言交互的复杂度，使得编码解码工作有了生产力。</li><li>Protocol Buffer 不是自我描述的，离开了数据描述 .proto 文件，就无法理解二进制数据流。这点即是优点，使数据具有一定的“加密性”，也是缺点，数据可读性极差。所以 Protocol Buffer 非常适合内部服务之间 RPC 调用和传递数据。</li><li>Protocol Buffer 具有向后兼容的特性，更新数据结构以后，老版本依旧可以兼容，这也是 Protocol Buffer 诞生之初被寄予解决的问题。因为编译器对不识别的新增字段会跳过不处理</li></ul><h2 id="Protobuf反序列化"><a href="#Protobuf反序列化" class="headerlink" title="Protobuf反序列化"></a>Protobuf反序列化</h2><p><a href="https://halfrost.com/protobuf_encode/" target="_blank" rel="external">https://halfrost.com/protobuf_encode/</a></p><p>整个解析过程需要 Protobuf 本身的框架代码和由 Protobuf 编译器生成的代码共同完成。Protobuf 提供了基类 Message 以及 Message_lite 作为通用的 Framework，，CodedInputStream 类，WireFormatLite 类等提供了对二进制数据的 decode 功能，从 5.1 节的分析来看，Protobuf 的解码可以通过几个简单的数学运算完成，无需复杂的词法语法分析，因此 ReadTag() 等方法都非常快。 在这个调用路径上的其他类和方法都非常简单，感兴趣的读者可以自行阅读。 相对于 XML 的解析过程，以上的流程图实在是非常简单吧？这也就是 Protobuf 效率高的第二个原因了</p><p><img src="http://or0igopk2.bkt.clouddn.com/18-7-28/17159197.jpg" alt=""></p><h2 id="与json-thift的性能比较"><a href="#与json-thift的性能比较" class="headerlink" title="与json thift的性能比较"></a>与json thift的性能比较</h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzA3NDcyMTQyNQ==&amp;mid=2649257430&amp;idx=1&amp;sn=975b6123d8256221f6bac3b99e52af9a&amp;chksm=8767a428b0102d3e6ab7abdf797c481da570cb29e274aa4ff6ecd931f535166b776e6548941d&amp;scene=0&amp;key=399a205ce674169cbedcc1c459650908e22d6a2b81674195c3b251114acdf821dbde7bb49102c6b47f61b26a7a404d74e0e8440cea3675a7ea8f49eafd8639bfb733183a1bfb4603232d6cb8ecd230e5&amp;ascene=0&amp;uin=NTkxMDk2NjU=&amp;devicetype=iMac+MacBookPro12,1+OSX+OSX+10.12.4+build(16E195)&amp;version=12020510&amp;nettype=WIFI&amp;fontScale=100&amp;pass_ticket=wHPj0w18CV8zHl6HCfd9t9LQfs3I0ZULhUILuOHgL0E=" target="_blank" rel="external">Protobuf有没有比JSON快5倍？用代码来击破pb性能神话</a></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>protobuf的性能来源于对存储的压缩，避免一切不必要的字节开销。flink中如过将大多数需要存储到state中的对象先转成PB格式会得到很大的性能提升。</p><p>和protobuf通常被同时提及的有Apache Thrift / Avro 本文已经没有空间介绍，待后续深入了解。</p><p>这篇文章介绍了3中数据结构如何做到对消息体格式演变的透明<br><a href="https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html" target="_blank" rel="external">https://martin.kleppmann.com/2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html</a></p><p>参考：</p><p><a href="https://developers.google.com/protocol-buffers/docs/javatutorial" target="_blank" rel="external">https://developers.google.com/protocol-buffers/docs/javatutorial</a><br><a href="https://developers.google.com/protocol-buffers/docs/encoding#structure" target="_blank" rel="external">https://developers.google.com/protocol-buffers/docs/encoding#structure</a><br><a href="https://halfrost.com/protobuf_encode/" target="_blank" rel="external">https://halfrost.com/protobuf_encode/</a><br><a href="https://izualzhy.cn/protobuf-encode-varint-and-zigzag" target="_blank" rel="external">https://izualzhy.cn/protobuf-encode-varint-and-zigzag</a><br><a href="https://izualzhy.cn/protobuf-encoding" target="_blank" rel="external">https://izualzhy.cn/protobuf-encoding</a><br><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-gpb/index.html" target="_blank" rel="external">https://www.ibm.com/developerworks/cn/linux/l-cn-gpb/index.html</a><br><a href="https://segmentfault.com/a/1190000004891020" target="_blank" rel="external">https://segmentfault.com/a/1190000004891020</a> protobufstuff</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;本文带你深入理解和使用protobuf&lt;/p&gt;
    
    </summary>
    
      <category term="刨根问底" scheme="http://www.aitozi.com/categories/%E5%88%A8%E6%A0%B9%E9%97%AE%E5%BA%95/"/>
    
    
      <category term="protobuf" scheme="http://www.aitozi.com/tags/protobuf/"/>
    
  </entry>
  
  <entry>
    <title>Java序列化拾掇</title>
    <link href="http://www.aitozi.com/java-serialization.html"/>
    <id>http://www.aitozi.com/java-serialization.html</id>
    <published>2018-07-27T16:51:18.000Z</published>
    <updated>2019-03-14T17:06:29.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>Java序列化拾掇</p><a id="more"></a><blockquote><p>本来想总结一下对google protobuf的用法总结，然而搜资料的过程中发现对很多java序列化的知识不足，故做了一些拾掇，在flink中关于类型序列化的地方其实也涉及很多，待以后看到，如有新的思考再来补充。</p></blockquote><h2 id="java序列化"><a href="#java序列化" class="headerlink" title="java序列化"></a>java序列化</h2><ol><li>在Java中，只要一个类实现了java.io.Serializable接口，那么它就可以被序列化</li><li>若父类未实现Serializable,而子类序列化了，父类属性值不会被保存，反序列化后父类属性值丢失</li><li>通过ObjectOutputStream和ObjectInputStream对对象进行序列化及反序列化</li><li>在deserialized的时候类的构造器是不会被调用的，只会调用没有实现Serializabe接口的父类的无参构造方法，如果其父类不可序列化，并且没有无参构造函数就会导致<code>InvalidClassException</code></li><li>只有non-static的成员并且没有标记为transient才会被序列化</li><li>类所包含的成员变量也必须是可序列化的</li><li>transient 关键字的作用是控制变量的序列化，在变量声明前加上该关键字，可以阻止该变量被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 型的是 0，对象型的是 null</li><li>java的serialVersionUID用来表明类的不同版本间的兼容性，必须被定义成final static long才能生效，否则会报错</li><li>在使用Externalizable进行序列化的时候，在读取对象时，会调用被序列化类的无参构造器去创建一个新的对象，然后再将被保存对象的字段的值分别填充到新对象中。所以，实现Externalizable接口的类必须要提供一个public的无参的构造器，如果一个Java类没有定义任何构造函数，编译器会帮我们自动添加一个无参的构造方法，可是，如果我们在类中定义了一个有参数的构造方法了，编译器便不会再帮我们创建无参构造方法，这点需要注意</li><li>用户自定义的 writeObject 和 readObject 方法可以允许用户控制序列化的过程</li></ol><h3 id="serialVersionUID"><a href="#serialVersionUID" class="headerlink" title="serialVersionUID"></a>serialVersionUID</h3><p>Java的序列化机制是通过在运行时判断类的serialVersionUID来验证版本一致性的。在进行反序列化时，JVM会把传来 的字节流中的serialVersionUID与本地相应实体（类）的serialVersionUID进行比较，如果相同就认为是一致的，可以进行反序 列化，否则就会出现序列化版本不一致的异常。</p><p>当实现java.io.Serializable接口的实体（类）没有显式地定义一个名为serialVersionUID，类型为long的变 量时，Java序列化机制会根据编译的class自动生成一个serialVersionUID作序列化版本比较用，这种情况下，只有同一次编译生成的 class才会生成相同的serialVersionUID 。</p><p>如果我们不希望通过编译来强制划分软件版本，即实现序列化接口的实体能够兼容先前版本，未作更改的类，就需要显式地定义一个名为serialVersionUID，类型为long的变量，不修改这个变量值的序列化实体都可以相互进行串行化和反串行化。</p><h3 id="在内部类使用中带来的困扰"><a href="#在内部类使用中带来的困扰" class="headerlink" title="在内部类使用中带来的困扰"></a>在内部类使用中带来的困扰</h3><blockquote><p>A class that is serializable with an enclosing class that is not serializable causes serialization to fail.<br>Non-static nested classes that implement Serializable must be defined in an enclosing class that is also serializable. Non-static nested classes retain an implicit reference to an instance of their enclosing class. If the enclosing class is not serializable, the Java serialization mechanism fails with a java.io.NotSerializableException.</p></blockquote><p>一个非静态的内部类实现了Serializable接口，要求其外部类也同样实现Serializable接口。这是因为一个非静态内部类包含有一个隐式的指向外部包装类实例对象的一个指针，如上面指出的规则，序列化的时候要求类的非静态成员也需要是可序列化的，如果外部类没有声明Serializable，java序列化机制就会报错，解法通常是</p><ul><li>将内部类声明为static，这样就不包含隐式指针了</li><li>将外部类声明为Serializable</li></ul><p>在flink中采用了另一种解法，用户通过匿名内部类来定义一个userFuntion，通常userFunction需要被序列化来分发到各个task节点来执行，定义成static不如匿名类方便，外部主类定义成Serializable的代价又比较大，因此采用另一种解法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">for (Field f: cls.getDeclaredFields()) &#123;</div><div class="line">	if (f.getName().startsWith(&quot;this$&quot;)) &#123;</div><div class="line">		// found a closure referencing field - now try to clean</div><div class="line">		closureAccessed |= cleanThis0(func, cls, f.getName());</div><div class="line">	&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">private static boolean cleanThis0(Object func, Class&lt;?&gt; cls, String this0Name) &#123;</div><div class="line"></div><div class="line">	This0AccessFinder this0Finder = new This0AccessFinder(this0Name);</div><div class="line">	getClassReader(cls).accept(this0Finder, 0);</div><div class="line"></div><div class="line">	final boolean accessesClosure = this0Finder.isThis0Accessed();</div><div class="line"></div><div class="line">	if (LOG.isDebugEnabled()) &#123;</div><div class="line">		LOG.debug(this0Name + &quot; is accessed: &quot; + accessesClosure);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	if (!accessesClosure) &#123;</div><div class="line">		Field this0;</div><div class="line">		try &#123;</div><div class="line">			this0 = func.getClass().getDeclaredField(this0Name);</div><div class="line">		&#125; catch (NoSuchFieldException e) &#123;</div><div class="line">			// has no this$0, just return</div><div class="line">			throw new RuntimeException(&quot;Could not set &quot; + this0Name + &quot;: &quot; + e);</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		try &#123;</div><div class="line">			this0.setAccessible(true);</div><div class="line">			this0.set(func, null);</div><div class="line">		&#125;</div><div class="line">		catch (Exception e) &#123;</div><div class="line">			// should not happen, since we use setAccessible</div><div class="line">			throw new RuntimeException(&quot;Could not set &quot; + this0Name + &quot; to null. &quot; + e.getMessage(), e);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	return accessesClosure;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>对每一个userFunction(可能实现自一个匿名内部类)有一个clean的机制。</p><ul><li>检查其声明字段有没有<code>this$</code>开始的，即指向外部类的引用</li><li>如果有将对应的字段通过反射置成null,这样就不会受第三条规则的困扰了</li></ul><h3 id="自定义序列化"><a href="#自定义序列化" class="headerlink" title="自定义序列化"></a>自定义序列化</h3><p>在序列化过程中，如果被序列化的类中定义了writeObject 和 readObject 方法，虚拟机会试图调用对象类里的 writeObject 和 readObject 方法，进行用户自定义的序列化和反序列化。并且这两个方法的signature必须是以下这样才会生效，否则就是默认的序列化方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">private void readObject(java.io.ObjectInputStream in)</div><div class="line">     throws IOException, ClassNotFoundException;</div><div class="line">private void writeObject(java.io.ObjectOutputStream out)</div><div class="line">     throws IOException;</div></pre></td></tr></table></figure><p>如果没有这样的方法，则默认调用是 ObjectOutputStream 的 defaultWriteObject 方法以及 ObjectInputStream 的 defaultReadObject 方法。这两个方法没有覆写，也没有被显式调用，为什么会生效呢？ 具体可以参见<a href="https://mp.weixin.qq.com/s/ABtxdNpr4bLpXtFiOK47hA" target="_blank" rel="external">https://mp.weixin.qq.com/s/ABtxdNpr4bLpXtFiOK47hA</a></p><blockquote><p>在使用ObjectOutputStream的writeObject方法和ObjectInputStream的readObject方法时，会通过反射的方式调用</p></blockquote><p>关于序列化的困惑可以在这个源码中得到解答</p><blockquote><p>ObjectOutputStream#writeObject</p><p>writeObject =&gt; writeObject0 =&gt; writeOrdinaryObject =&gt; writeSerialData =&gt; invokeWriteObject</p></blockquote><p>可以参见ArrayList使用了这种自定义序列化的方法，ArrayList实际上是动态数组，每次在放满以后自动增长设定的长度值，如果数组自动增长长度设为100，而实际只放了一个元素，那就会序列化99个null元素。为了保证在序列化的时候不会将这么多null同时进行序列化，ArrayList把元素数组设置为transient。</p><p><em>Ps：在两个方法的开始处，你会发现调用了defaultWriteObject()和defaultReadObject()。它们做的是默认的序列化进程，就像写/读所有的non-transient和 non-static字段(但他们不会去做serialVersionUID的检查).通常说来，所有我们想要自己处理的字段都应该声明为transient。这样的话，defaultWriteObject/defaultReadObject便可以专注于其余字段，而我们则可为这些特定的字段(译者：指transient)定制序列化。使用那两个默认的方法并不是强制的，而是给予了处理复杂应用时更多的灵活性</em></p><h3 id="关于反序列化的时候构造方法是否会被调用（反序列化是怎么做的）"><a href="#关于反序列化的时候构造方法是否会被调用（反序列化是怎么做的）" class="headerlink" title="关于反序列化的时候构造方法是否会被调用（反序列化是怎么做的）"></a>关于反序列化的时候构造方法是否会被调用（反序列化是怎么做的）</h3><blockquote><p>A non-serializable, immediate superclass of a serializable class that does not itself declare an accessible, no-argument constructor causes deserialization to fail<br>To allow subtypes of non-serializable classes to be serialized, the subtype may assume responsibility for saving and restoring the state of the supertype’s public, protected, and (if accessible) package fields. The subtype may assume this responsibility only if the class it extends has an accessible no-arg constructor to initialize the class’s state. It is an error to declare a class Serializable if this is not the case. The error will be detected at runtime.</p></blockquote><p>反序列化其实是将先前序列化生成的byte流重新构建成一个对象，byte流包含了所有重构对象的信息，包括class的元数据，实例的变量的类型信息，以及相应的值。然后在反序列化的时候它要求<strong>all the parent classes of instance should be Serializable; and if any super class in hirarchy is not Serializable then it must have a default constructor</strong>。在反序列化的时候会一直搜寻其父类，直到找到第一个不可序列化的类，就尝试调用其无参构造函数创建对象，如果所有的父类都是可序列化的，最终找到的就是Object类，然后首先创建一个Object对象。接着JVM就继续读取byte流，设置相关的类型信息，一个空对象创建完成后，jvm就设置相关的static字段，并调用<code>readObject</code>方法进行赋值</p><p>由于本类的构造方法不会被调用，所以你期望某个变量的初始化在构造方法中完成得到的只会是null。</p><p>在构造函数的调用上<code>Externalizable</code>和<code>Serializable</code>的表现不同，Externalizable依赖于本身类的无参构造函数</p><h3 id="利用序列化来做deepCopy"><a href="#利用序列化来做deepCopy" class="headerlink" title="利用序列化来做deepCopy"></a>利用序列化来做deepCopy</h3><p>主要用到了<code>ByteArrayOutputStream</code>,存储在内存中，不做持久化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">public SerializableClass deepCopy() throws Exception&#123;</div><div class="line">    //Serialization of object</div><div class="line">    ByteArrayOutputStream bos = new ByteArrayOutputStream();</div><div class="line">    ObjectOutputStream out = new ObjectOutputStream(bos);</div><div class="line">    out.writeObject(this);</div><div class="line"></div><div class="line">    //De-serialization of object</div><div class="line">    ByteArrayInputStream bis = new   ByteArrayInputStream(bos.toByteArray());</div><div class="line">    ObjectInputStream in = new ObjectInputStream(bis);</div><div class="line">    SerializableClass copied = (SerializableClass) in.readObject();</div><div class="line">    return copied;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>更多关于java clone 可参考:</p><p><a href="https://howtodoinjava.com/core-java/cloning/a-guide-to-object-cloning-in-java/" target="_blank" rel="external">https://howtodoinjava.com/core-java/cloning/a-guide-to-object-cloning-in-java/</a></p><h3 id="如果对象状态需要同步，则对象序列化也需要同步"><a href="#如果对象状态需要同步，则对象序列化也需要同步" class="headerlink" title="如果对象状态需要同步，则对象序列化也需要同步"></a>如果对象状态需要同步，则对象序列化也需要同步</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">private synchronized void writeObject(ObjectOutputStream s) throws IOException &#123;</div><div class="line">        s.defaultWriteObject();</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="单例模式序列化"><a href="#单例模式序列化" class="headerlink" title="单例模式序列化"></a>单例模式序列化</h3><p>参考：</p><ul><li><a href="http://www.hollischuang.com/archives/1144" target="_blank" rel="external">http://www.hollischuang.com/archives/1144</a></li><li>枚举实现可序列化单例 <a href="http://www.cnblogs.com/cielosun/p/6596475.html" target="_blank" rel="external">http://www.cnblogs.com/cielosun/p/6596475.html</a></li><li><a href="https://leokongwq.github.io/2017/08/21/why-enum-singleton-are-serialization-safe.html" target="_blank" rel="external">https://leokongwq.github.io/2017/08/21/why-enum-singleton-are-serialization-safe.html</a></li></ul><h3 id="在不同的classloader之间进行对象的序列化和反序列化"><a href="#在不同的classloader之间进行对象的序列化和反序列化" class="headerlink" title="在不同的classloader之间进行对象的序列化和反序列化"></a>在不同的classloader之间进行对象的序列化和反序列化</h3><p>如上所说，在同一个classloader中，利用如下的方法serializabale和deserializable对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ByteArrayOutputStream bo=new ByteArrayOutputStream();</div><div class="line">ObjectOutputStream oo=new ObjectOutputStream(bo);</div><div class="line">oo.writeObject(outObject);</div><div class="line">ByteArrayInputStream bi=new ByteArrayInputStream(bo.toByteArray());</div><div class="line">ObjectInputStream oi=new ObjectInputStream(bi);</div><div class="line">Object inObject = oi.readObject();</div></pre></td></tr></table></figure><p>当序列化的对象和反序列化的对象不在同一个classloader中时，以上的代码执行时，就会报无法把属性付给对象的错误，此时应当，通过设置反序列化得classloader，来解决这个问题。</p><p>首先，从ObjectInputStream继承一个自己的ObjectInputStream</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">public class CustomObjectInputStream extends ObjectInputStream &#123;</div><div class="line"></div><div class="line">    protected ClassLoader classLoader = this.getClass().getClassLoader();</div><div class="line"></div><div class="line">    public CustomObjectInputStream(InputStream in) throws IOException &#123;</div><div class="line">        super(in);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    public CustomObjectInputStream(InputStream in, ClassLoader cl)</div><div class="line">            throws IOException &#123;</div><div class="line">        super(in);</div><div class="line">        this.classLoader = cl;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    @Override</div><div class="line">    protected Class&lt;?&gt; resolveClass(ObjectStreamClass desc) throws IOException,</div><div class="line">            ClassNotFoundException &#123;</div><div class="line">        // TODO Auto-generated method stub</div><div class="line">        String name = desc.getName();</div><div class="line">        try &#123;</div><div class="line">            return Class.forName(name, false, this.classLoader);</div><div class="line">        &#125; catch (ClassNotFoundException ex) &#123;</div><div class="line">            return super.resolveClass(desc);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>比较重要的是这里的resolveClass方法传入classloader,反序列化时将classloader传入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"> ByteArrayOutputStream bo=new ByteArrayOutputStream();</div><div class="line">ObjectOutputStream oo=new ObjectOutputStream(bo);</div><div class="line">oo.writeObject(outObject);</div><div class="line">ByteArrayInputStream bi=new ByteArrayInputStream(bo.toByteArray());</div><div class="line">CustomObjectInputStream oi=new CustomObjectInputStream(bi, outObject.getClass().getClassLoader());</div><div class="line">Object = oi.readObject();</div><div class="line">// flink源码中也有多次类似的使用userClassloader和FlinkClassLoader的切换</div><div class="line">https://issues.apache.org/jira/browse/FLINK-9122 这个bug曾经就是因为这个原因引起的</div></pre></td></tr></table></figure><h3 id="序列化相关方法"><a href="#序列化相关方法" class="headerlink" title="序列化相关方法"></a>序列化相关方法</h3><p>writeObject、readObject、readObjectNoData、writeReplace和readResolve 待补充</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://help.semmle.com/wiki/display/JAVA/Serializable+inner+class+of+non-serializable+class," title="Serializable inner class of non-serializable class" target="_blank" rel="external">Serializable inner class of non-serializable class</a></li><li><a href="https://mp.weixin.qq.com/s/-D9N9_9IDqSbuIjuADJ7ZA" target="_blank" rel="external">https://mp.weixin.qq.com/s/-D9N9_9IDqSbuIjuADJ7ZA</a></li><li><a href="https://howtodoinjava.com/core-java/serialization/how-deserialization-process-happen-in-java/" target="_blank" rel="external">https://howtodoinjava.com/core-java/serialization/how-deserialization-process-happen-in-java/</a></li><li><a href="https://www.quora.com/Why-are-enum-singleton-serialization-safe" target="_blank" rel="external">https://www.quora.com/Why-are-enum-singleton-serialization-safe</a></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;Java序列化拾掇&lt;/p&gt;
    
    </summary>
    
      <category term="刨根问底" scheme="http://www.aitozi.com/categories/%E5%88%A8%E6%A0%B9%E9%97%AE%E5%BA%95/"/>
    
    
      <category term="serialization" scheme="http://www.aitozi.com/tags/serialization/"/>
    
  </entry>
  
  <entry>
    <title>flink中jobgraph的生成逻辑</title>
    <link href="http://www.aitozi.com/flink-jobgraph-generate.html"/>
    <id>http://www.aitozi.com/flink-jobgraph-generate.html</id>
    <published>2018-05-26T10:43:23.000Z</published>
    <updated>2018-05-27T12:15:46.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>flink中jobgraph的生成逻辑，接前面的文章<a href="http://aitozi.com/2018/04/11/flink-streamGraph/" target="_blank" rel="external">flink图流转之StreamGraph</a></p><a id="more"></a><blockquote><p>今天想了一下源码分析类的文章应该是直接在源码上做注释来的直接，然后再抛开细节概括总体流程和关键点，今天这篇分析jobgraph生成的文章就以这个形式展开</p></blockquote><ol><li>先生成各个节点streamnode的hash值 主体代码在：<code>StreamGraphHasherV2.java</code></li><li>设置chaining<ul><li>找到节点中能chain和不能chain的边</li><li>生成相应的JobVertex节点，并设置StreamConfig（资源，名称，chain的节点），这个streamConfig是在部署期间比较重要的一个配置项，并拼接物理执行顺序，主要在connect函数</li></ul></li><li>设置inEdges配置项</li><li>设置slotsharingGroup</li><li>配置checkpoint，这里主要设置需要发送barrier的节点即source节点</li></ol><p><em>其实总结就是分两步：</em></p><ol><li>在createChain过程中创建JobVertex</li><li>设置各个StreamConfig需要的信息用作生成物理执行图的时候使用</li></ol><p>主要涉及的代码为两块，如下：</p><script src="//gist.github.com/2e021e923ed394155b191853e0975a71.js?file=StreamGraphHasherV2.java"></script><script src="//gist.github.com/2e021e923ed394155b191853e0975a71.js?file=StreamingJobGraphGenerator.java"></script><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;flink中jobgraph的生成逻辑，接前面的文章&lt;a href=&quot;http://aitozi.com/2018/04/11/flink-streamGraph/&quot;&gt;flink图流转之StreamGraph&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink cep源码分析</title>
    <link href="http://www.aitozi.com/flink-cep-code.html"/>
    <id>http://www.aitozi.com/flink-cep-code.html</id>
    <published>2018-05-25T13:54:23.000Z</published>
    <updated>2019-03-14T17:03:52.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>关于Flink中cep实现原理的分析</p><a id="more"></a><blockquote><p>最近一直在搞动态cep的事情，有点焦头烂额很久没有更新博客了。其实有时候也在思考写博客的意义，因为写博客也是时间成本很高的一件事，如果得不到相应的收益其实是划不来的。那么写博客的收益到底是什么呢？两点：笔记记录和传播知识的作用，整理记录的功能一个web博客不会强于一个终端笔记例如：为知笔记。所以博客的真正意义在于传播知识观点。有时候你遇到一个百思不得其解的问题的时候，google一下找到一篇博客，竟然能够解答心中所惑的时候你是不是心中会很感谢博主呢，我认为这样的一篇文章就是有价值的文章，所以我希望我也能做好这样一件有价值的事情。</p></blockquote><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>好了，进入本文的主题flink cep原理的深入理解，很多人可能还不知道flink cep是什么，flink cep其实实现自一篇论文，具体论文细节见我之前的一篇文章的分享<a href="http://aitozi.com/2018/02/25/flink-cep-paper/" target="_blank" rel="external">flink-cep-paper</a>. flink cep的全称是Complex Event Processing，在我看来它主要能做的是在一个连续不断的事件中提取出用户所关心的事件序列，他和flink的filter算子的区别在于filter只能去实现单个元素的过滤，而cep是能完成先后顺序事件的过滤。下面让我们来走进他的源码实现原理吧。以下代码基于社区1.4.2分支分析。</p><p>我们的文章以一系列问题来展开：</p><ol><li>用户定义的Pattern最后会以什么形式工作</li><li>当CEP Operator获取到上游一个算子的时候会做什么事情？</li><li>在ProcessingTime和Eventtime的语义下处理逻辑有什么不同点？</li><li>匹配成功的元素如何存储，状态机转化流程是怎么样的？</li><li>超时未匹配成功的元素会做什么？</li></ol><h2 id="问题一"><a href="#问题一" class="headerlink" title="问题一"></a>问题一</h2><p>用户在定义Pattern和condition之后，会通过NFAcompiler将Pattern翻译成一个一个相关联的State，表明了这一组规则的状态机的走向流程。</p><p>State包括<code>Start、Normal、Final、Stop</code>，start表示一个起始状态例如<code>begin(&#39;A&#39;).followedBy(&#39;B&#39;)</code> 这里面A就是一个Start状态。Final状态表示整个序列已经匹配完成可以向下游发送了，Stop状态是用来处理否定类型的规则，一旦到达Stop状态即意味着整个匹配过程失败。各个状态之间通过<code>StateTransition</code>来连接，连接方式有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ignore: 忽略此次匹配的元素</div><div class="line">proceed: 相当于forward的意思，到达下一个状态，不存储元素，继续做下一个状态的condition判断</div><div class="line">take： 存储本次的元素</div></pre></td></tr></table></figure><p>这是一段创建中间状态的代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div></pre></td><td class="code"><pre><div class="line">private State&lt;T&gt; createMiddleStates(final State&lt;T&gt; sinkState) &#123;</div><div class="line">			State&lt;T&gt; lastSink = sinkState;</div><div class="line">			// 不断往上遍历pattern进行state的生成</div><div class="line">			while (currentPattern.getPrevious() != null) &#123;</div><div class="line"></div><div class="line">				if (currentPattern.getQuantifier().getConsumingStrategy() == Quantifier.ConsumingStrategy.NOT_FOLLOW) &#123;</div><div class="line">					//skip notFollow patterns, they are converted into edge conditions</div><div class="line">				&#125; else if (currentPattern.getQuantifier().getConsumingStrategy() == Quantifier.ConsumingStrategy.NOT_NEXT) &#123;</div><div class="line">					final State&lt;T&gt; notNext = createState(currentPattern.getName(), State.StateType.Normal);</div><div class="line">					final IterativeCondition&lt;T&gt; notCondition = getTakeCondition(currentPattern);</div><div class="line">					// 否定类型的pattern需要创建一个stop state</div><div class="line">					final State&lt;T&gt; stopState = createStopState(notCondition, currentPattern.getName());</div><div class="line"></div><div class="line">					if (lastSink.isFinal()) &#123;</div><div class="line">						//so that the proceed to final is not fired</div><div class="line">						结尾状态不用proceed过去做下一次计算了，可以直接ignore到Final，然后输出结果</div><div class="line">						notNext.addIgnore(lastSink, new NotCondition&lt;&gt;(notCondition));</div><div class="line">					&#125; else &#123;</div><div class="line">						notNext.addProceed(lastSink, new NotCondition&lt;&gt;(notCondition));</div><div class="line">					&#125;</div><div class="line">					// 在满足Not_NEXT的条件的时候就转化成stop状态即匹配失败</div><div class="line">					notNext.addProceed(stopState, notCondition);</div><div class="line">					lastSink = notNext;</div><div class="line">				&#125; else &#123;</div><div class="line">					// 非否定类型的状态的处理逻辑都在这个方法中</div><div class="line">					lastSink = convertPattern(lastSink);</div><div class="line">				&#125;</div><div class="line"></div><div class="line">				// we traverse the pattern graph backwards</div><div class="line">				followingPattern = currentPattern;</div><div class="line">				currentPattern = currentPattern.getPrevious();</div><div class="line"></div><div class="line">				final Time currentWindowTime = currentPattern.getWindowTime();</div><div class="line">				if (currentWindowTime != null &amp;&amp; currentWindowTime.toMilliseconds() &lt; windowTime) &#123;</div><div class="line">					// the window time is the global minimum of all window times of each state</div><div class="line">					windowTime = currentWindowTime.toMilliseconds();</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">			return lastSink;</div><div class="line">		&#125;</div></pre></td></tr></table></figure><p>生成这样的state列表之后，最终会创建一个NFA，一个NFA中包含了两个重要组件：<br>一个是SharedBuffer用于存储中间匹配命中的数据，这是一个基于论文实现的带版本的内存共享，主要解决的事情是在同一个元素触发多个分支的时候避免存储多次。<br>另一个是ComputationState队列表示的是一系列当前匹配到的计算状态，每一个状态在拿到下一个元素的时候都会根据condition判断自己是能够继续往下匹配生成下一个computation state还是匹配失败。</p><h2 id="问题二、三"><a href="#问题二、三" class="headerlink" title="问题二、三"></a>问题二、三</h2><p>问题二和问题三一起解释，在消费到上游一个元素之后会判断时间语义，这里主要是为了处理乱序问题，如果是processingtime的话就会直接经由nfa#process进行处理，因为processing time不需要考虑事件是否乱序，他给每个事件都打上了当前的时间戳。而event语义下，会先将该数据buffer到rocksdb中，并且注册一个比当前时间戳大1的eventimer，用以触发真正的计算，也就是说，eventtime其实是每毫秒获取过去存储的数据做一次匹配计算。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">protected void saveRegisterWatermarkTimer() &#123;</div><div class="line">	long currentWatermark = timerService.currentWatermark();</div><div class="line">	// protect against overflow</div><div class="line">	if (currentWatermark + 1 &gt; currentWatermark) &#123;</div><div class="line">		timerService.registerEventTimeTimer(VoidNamespace.INSTANCE, currentWatermark + 1);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="问题四、五"><a href="#问题四、五" class="headerlink" title="问题四、五"></a>问题四、五</h2><p>nfa#process做了什么？取出前面说到的nfa中所有的当前computationState去做计算，当然计算之前会先判断时间和computation的starttime比较匹配是否超出时间，即within算子所做的时间，如果设置了超时处理的方式，就会将超时未匹配完成，已匹配到的部分元素向下游发送，并做sharebuffer的清理工作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">if (!computationState.isStartState() &amp;&amp;</div><div class="line">	windowTime &gt; 0L &amp;&amp;</div><div class="line">	timestamp - computationState.getStartTimestamp() &gt;= windowTime) &#123;</div><div class="line"></div><div class="line">	if (handleTimeout) &#123;</div><div class="line">		// extract the timed out event pattern</div><div class="line">		Map&lt;String, List&lt;T&gt;&gt; timedOutPattern = extractCurrentMatches(computationState);</div><div class="line">		timeoutResult.add(Tuple2.of(timedOutPattern, timestamp));</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	eventSharedBuffer.release(</div><div class="line">			NFAStateNameHandler.getOriginalNameFromInternal(computationState.getPreviousState().getName()),</div><div class="line">			computationState.getEvent(),</div><div class="line">			computationState.getTimestamp(),</div><div class="line">			computationState.getCounter());</div><div class="line"></div><div class="line">	newComputationStates = Collections.emptyList();</div><div class="line">	nfaChanged = true;</div><div class="line">&#125; else if (event != null) &#123;</div><div class="line">   // 在computeNextState的时候判断成功的take条件会将元素put到eventSharedBuffer中</div><div class="line">	newComputationStates = computeNextStates(computationState, event, timestamp);</div><div class="line"></div><div class="line">	if (newComputationStates.size() != 1) &#123;</div><div class="line">		nfaChanged = true;</div><div class="line">	&#125; else if (!newComputationStates.iterator().next().equals(computationState)) &#123;</div><div class="line">		nfaChanged = true;</div><div class="line">	&#125;</div><div class="line">&#125; else &#123;</div><div class="line">	newComputationStates = Collections.singleton(computationState);</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>在完成匹配之后达到final状态将数据提取出来向下游发送完成匹配。</p><p>以上便是cep的大致原理，说白了其实这个就是基于flink runntime开发出来的一个衍生lib，flink runtime其实是一个分布式的阻塞队列，通过这个概念可以在上面开发出很多有意思的产品，cep就是其中一个。 分析结束，欢迎拍砖~</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;关于Flink中cep实现原理的分析&lt;/p&gt;
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
      <category term="CEP" scheme="http://www.aitozi.com/tags/CEP/"/>
    
  </entry>
  
  <entry>
    <title>flink图流转之StreamGraph</title>
    <link href="http://www.aitozi.com/flink-streamGraph.html"/>
    <id>http://www.aitozi.com/flink-streamGraph.html</id>
    <published>2018-04-10T23:13:58.000Z</published>
    <updated>2018-04-10T23:19:36.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>flink DAG图流转分析<br><a id="more"></a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>每次我们编写完flink作业，跑任务的时候都会在flink-ui上展示一个作业的DAG图，那么这个图是如何形成的呢？本文就和你一起来揭开flink执行图生成的神秘面纱~</p><p>##总览<br>在flink中的执行图可以分为4层StreamGraph -&gt; JobGraph -&gt; ExecutionGraph -&gt; 物理执行图。</p><ul><li>StreamGraph：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。</li><li>JobGraph：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。</li><li>ExecutionGraph：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。</li><li>物理执行图：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</li></ul><p>今天我们就来看下streamGraph的生成</p><h2 id="StreamGraph的生成"><a href="#StreamGraph的生成" class="headerlink" title="StreamGraph的生成"></a>StreamGraph的生成</h2><h3 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">StreamGraph：根据用户通过 Stream API 编写的代码生成的最初的图。</div><div class="line">StreamNode：用来代表 operator 的类，并具有所有相关的属性，如并发度、入边和出边等。</div><div class="line">StreamEdge：表示连接两个StreamNode的边。</div></pre></td></tr></table></figure><p>flink任务从定义一个运行环境开始<code>streamExecutionEnvironment</code>，流计算任务起始于addSource,我们来看这个函数，addSource之后生成了一个<code>DataStream</code>. <code>DataStream</code>的构造函数参数接收一个<code>StreamTransformation</code>类型的对象，这个对象反映了流之间的转换操作。但是这个transformation和<code>operation</code>不是一一对应的。一些分区操作：union，split/select，partition只是逻辑概念，并不会在最后的dag图上显示出来。<br>在生成datastream之后，经历<code>DataStream.java</code>中定义的一些api算子，完成业务逻辑的定义，在这之中可能包含以下的转化：</p><p>假设一个场景：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">addsource -&gt; map -&gt; filter -&gt; connect -&gt; flatmap </div><div class="line">-&gt; keyby -&gt; window -&gt; apply -&gt; addSink -&gt; excute</div></pre></td></tr></table></figure><ol><li>addSource创建生成一个<code>SingleOutputStreamOperator</code> 本质上是一个带有transformation=”SourceTransformation”的datastream</li><li>map创建生成一个<code>OneInputTransformation</code> 并调用getExecutionEnvironment().addOperator(resultTransform)将其添加入env的<code>List&lt;StreamTransformation&lt;?&gt;&gt;</code>中</li><li>filter 通过 map相同操作</li><li>connect 直接返回一个<code>ConnectedStreams</code>不是<code>Datastream</code>的子类</li><li>flatmap 生成一个<code>TwoInputTransformation</code>将其添加入env的<code>List&lt;StreamTransformation&lt;?&gt;&gt;</code>中，并返回一个<code>SingleOutputStreamOperator</code>,并且返回的Datastream中包含的是当前这个transformation</li><li>keyby 生成一个keyedStream，这里直接生成一个<code>PartitionTransformation</code> 替代了父类<code>DataStream</code>中的transformation</li><li>window 生成windowStream</li><li>apply调用将生成一个<code>OneInputTransformation</code>,增加至<code>List&lt;StreamTransformation&lt;?&gt;&gt;</code></li><li>addSink 获取 <code>SinkTransformation</code></li></ol><p>其中每一次创建<code>OneInputTransformation</code>都是基于Datastream的当前的transformation来创建的，也就是说keyby之后的PartitionTransformation信息也加入了.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">new OneInputTransformation&lt;&gt;(</div><div class="line">			this.transformation,</div><div class="line">			operatorName,</div><div class="line">			operator,</div><div class="line">			outTypeInfo,</div><div class="line">			environment.getParallelism());</div></pre></td></tr></table></figure><p>好了到这里已经获取了各个流程的streamtransformation,最后调用execute方法，截取了流式环境下的实现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">public JobExecutionResult execute(String jobName) throws Exception &#123;</div><div class="line">	Preconditions.checkNotNull(&quot;Streaming Job name should not be null.&quot;);</div><div class="line"></div><div class="line">	StreamGraph streamGraph = this.getStreamGraph();</div><div class="line">	streamGraph.setJobName(jobName);</div><div class="line"></div><div class="line">	transformations.clear();</div><div class="line"></div><div class="line">	// execute the programs</div><div class="line">	if (ctx instanceof DetachedEnvironment) &#123;</div><div class="line">		LOG.warn(&quot;Job was executed in detached mode, the results will be available on completion.&quot;);</div><div class="line">		((DetachedEnvironment) ctx).setDetachedPlan(streamGraph);</div><div class="line">		return DetachedEnvironment.DetachedJobExecutionResult.INSTANCE;</div><div class="line">	&#125; else &#123;</div><div class="line">		return ctx</div><div class="line">			.getClient()</div><div class="line">			.run(streamGraph, ctx.getJars(), ctx.getClasspaths(), ctx.getUserCodeClassLoader(), ctx.getSavepointRestoreSettings())</div><div class="line">			.getJobExecutionResult();</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>其实主要调用的就是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">StreamGraphGenerator.generate(this, transformations);</div></pre></td></tr></table></figure><p>每一个<code>OneInputTransformation</code>都会记录他的上游的input的transformation，在<code>StreamGraphGenerator.generate</code>主要针对不同的transformation进行不同的转化</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> Collection&lt;Integer&gt; <span class="title">transform</span><span class="params">(StreamTransformation&lt;?&gt; transform)</span> </span>&#123;</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (alreadyTransformed.containsKey(transform)) &#123;</div><div class="line">			<span class="keyword">return</span> alreadyTransformed.get(transform);</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		LOG.debug(<span class="string">"Transforming "</span> + transform);</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (transform.getMaxParallelism() &lt;= <span class="number">0</span>) &#123;</div><div class="line"></div><div class="line">			<span class="comment">// if the max parallelism hasn't been set, then first use the job wide max parallelism</span></div><div class="line">			<span class="comment">// from theExecutionConfig.</span></div><div class="line">			<span class="keyword">int</span> globalMaxParallelismFromConfig = env.getConfig().getMaxParallelism();</div><div class="line">			<span class="keyword">if</span> (globalMaxParallelismFromConfig &gt; <span class="number">0</span>) &#123;</div><div class="line">				transform.setMaxParallelism(globalMaxParallelismFromConfig);</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="comment">// call at least once to trigger exceptions about MissingTypeInfo</span></div><div class="line">		transform.getOutputType();</div><div class="line"></div><div class="line">		Collection&lt;Integer&gt; transformedIds;</div><div class="line">		<span class="keyword">if</span> (transform <span class="keyword">instanceof</span> OneInputTransformation&lt;?, ?&gt;) &#123;</div><div class="line">			transformedIds = transformOneInputTransform((OneInputTransformation&lt;?, ?&gt;) transform);</div><div class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> TwoInputTransformation&lt;?, ?, ?&gt;) &#123;</div><div class="line">			transformedIds = transformTwoInputTransform((TwoInputTransformation&lt;?, ?, ?&gt;) transform);</div><div class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SourceTransformation&lt;?&gt;) &#123;</div><div class="line">			transformedIds = transformSource((SourceTransformation&lt;?&gt;) transform);</div><div class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SinkTransformation&lt;?&gt;) &#123;</div><div class="line">			transformedIds = transformSink((SinkTransformation&lt;?&gt;) transform);</div><div class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> UnionTransformation&lt;?&gt;) &#123;</div><div class="line">			transformedIds = transformUnion((UnionTransformation&lt;?&gt;) transform);</div><div class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SplitTransformation&lt;?&gt;) &#123;</div><div class="line">			transformedIds = transformSplit((SplitTransformation&lt;?&gt;) transform);</div><div class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SelectTransformation&lt;?&gt;) &#123;</div><div class="line">			transformedIds = transformSelect((SelectTransformation&lt;?&gt;) transform);</div><div class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> FeedbackTransformation&lt;?&gt;) &#123;</div><div class="line">			transformedIds = transformFeedback((FeedbackTransformation&lt;?&gt;) transform);</div><div class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> CoFeedbackTransformation&lt;?&gt;) &#123;</div><div class="line">			transformedIds = transformCoFeedback((CoFeedbackTransformation&lt;?&gt;) transform);</div><div class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> PartitionTransformation&lt;?&gt;) &#123;</div><div class="line">			transformedIds = transformPartition((PartitionTransformation&lt;?&gt;) transform);</div><div class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (transform <span class="keyword">instanceof</span> SideOutputTransformation&lt;?&gt;) &#123;</div><div class="line">			transformedIds = transformSideOutput((SideOutputTransformation&lt;?&gt;) transform);</div><div class="line">		&#125; <span class="keyword">else</span> &#123;</div><div class="line">			<span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Unknown transformation: "</span> + transform);</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="comment">// need this check because the iterate transformation adds itself before</span></div><div class="line">		<span class="comment">// transforming the feedback edges</span></div><div class="line">		<span class="keyword">if</span> (!alreadyTransformed.containsKey(transform)) &#123;</div><div class="line">			alreadyTransformed.put(transform, transformedIds);</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (transform.getBufferTimeout() &gt; <span class="number">0</span>) &#123;</div><div class="line">			streamGraph.setBufferTimeout(transform.getId(), transform.getBufferTimeout());</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">if</span> (transform.getUid() != <span class="keyword">null</span>) &#123;</div><div class="line">			streamGraph.setTransformationUID(transform.getId(), transform.getUid());</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">if</span> (transform.getUserProvidedNodeHash() != <span class="keyword">null</span>) &#123;</div><div class="line">			streamGraph.setTransformationUserHash(transform.getId(), transform.getUserProvidedNodeHash());</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (transform.getMinResources() != <span class="keyword">null</span> &amp;&amp; transform.getPreferredResources() != <span class="keyword">null</span>) &#123;</div><div class="line">			streamGraph.setResources(transform.getId(), transform.getMinResources(), transform.getPreferredResources());</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="keyword">return</span> transformedIds;</div><div class="line">	&#125;</div></pre></td></tr></table></figure><p>可以看到他里面的方法都是递归调用<code>transform(input)</code>方法，然后通过<code>alreadyTransformed</code>数据结构，避免重复计算，所以我们最终看的时候最先是从source处进行的，也就是从上游到下游进行转化</p><ol><li>如果已经在<code>alreadyTransformed</code>数据结构中那么就直接返回transformation的id</li><li>分别有addSource，addOperator，addSink，addCoOperator，addEdge的不同操作来生成streamGraph中的不同节点</li><li>addEdge建立每一个transformation和他所有上游输入节点的连线</li></ol><p>在streamgraph中还建立了几个虚拟的节点，这几个节点主要针对的是partition，split/select，sideoutput的操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">private Map&lt;Integer, Tuple2&lt;Integer, List&lt;String&gt;&gt;&gt; virtualSelectNodes;</div><div class="line">private Map&lt;Integer, Tuple2&lt;Integer, OutputTag&gt;&gt; virtualSideOutputNodes;</div><div class="line">private Map&lt;Integer, Tuple2&lt;Integer, StreamPartitioner&lt;?&gt;&gt;&gt; virtualPartitionNodes;</div></pre></td></tr></table></figure><p>在进行这些操作时，会添加一个唯一的虚拟节点</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//记录了上游某个transformId到下游的partition方式</span></div><div class="line">virtualPartitionNodes.put(virtualId, <span class="keyword">new</span> Tuple2&lt;Integer, StreamPartitioner&lt;?&gt;&gt;(originalId, partitioner));  </div><div class="line"><span class="comment">//记录上游的不同outputTag，用以将部分数据从该tag输出</span></div><div class="line">virtualSideOutputNodes.put(virtualId, <span class="keyword">new</span> Tuple2&lt;&gt;(originalId, outputTag));</div><div class="line"><span class="comment">//记录一个上游的select虚拟节点</span></div><div class="line">virtualSelectNodes.put(virtualId, <span class="keyword">new</span> Tuple2&lt;Integer, List&lt;String&gt;&gt;(originalId, selectedNames));</div></pre></td></tr></table></figure><p>经过一些列的<code>addNode</code>以及<code>addEdge</code>之后，streamGraph已经生成。关于其他几个graph的生成请听下回的分解</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;flink DAG图流转分析&lt;br&gt;
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink-ui中的反压采样</title>
    <link href="http://www.aitozi.com/flink-backpressure-sample.html"/>
    <id>http://www.aitozi.com/flink-backpressure-sample.html</id>
    <published>2018-04-10T23:02:51.000Z</published>
    <updated>2018-04-10T23:08:08.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>flink反压值采样计算原理</p><a id="more"></a><p>我们在点击flink ui的<code>operator-&gt;backpressure</code>之后，会触发Backpressure采样：每隔<code>BACK_PRESSURE_REFRESH_INTERVAL</code>的间隔进行一次采样。</p><h3 id="BackPressureStatsTracker-triggerStackTraceSample"><a href="#BackPressureStatsTracker-triggerStackTraceSample" class="headerlink" title="BackPressureStatsTracker#triggerStackTraceSample"></a>BackPressureStatsTracker#triggerStackTraceSample</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"> <span class="comment">/**</span></div><div class="line"> * Triggers a stack trace sample for a operator to gather the back pressure</div><div class="line"> * statistics. If there is a sample in progress for the operator, the call</div><div class="line"> * is ignored.</div><div class="line"> *</div><div class="line"> * <span class="doctag">@param</span> vertex Operator to get the stats for. 代表要采样的Operator节点</div><div class="line"> * <span class="doctag">@return</span> Flag indicating whether a sample with triggered.</div><div class="line"> */</div><div class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">triggerStackTraceSample</span><span class="params">(ExecutionJobVertex vertex)</span> </span>&#123;</div><div class="line">	<span class="comment">// 保证没有并行triggerSimple</span></div><div class="line">	<span class="keyword">synchronized</span> (lock) &#123;</div><div class="line">		<span class="keyword">if</span> (shutDown) &#123;</div><div class="line">			<span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">		&#125;</div><div class="line">		<span class="comment">//排除掉已经pending在采样和结束的Operator</span></div><div class="line">		<span class="keyword">if</span> (!pendingStats.contains(vertex) &amp;&amp;</div><div class="line">				!vertex.getGraph().getState().isGloballyTerminalState()) &#123;</div><div class="line">				</div><div class="line">			<span class="comment">// 拿到对应ExecutionGraph的FutureExecutor，这是用以在相应的ExecutionGraph发起任务的</span></div><div class="line">			Executor executor = vertex.getGraph().getFutureExecutor();</div><div class="line"></div><div class="line">			<span class="comment">// Only trigger if still active job</span></div><div class="line">			<span class="keyword">if</span> (executor != <span class="keyword">null</span>) &#123;</div><div class="line">				pendingStats.add(vertex);</div><div class="line"></div><div class="line">				<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</div><div class="line">					LOG.debug(<span class="string">"Triggering stack trace sample for tasks: "</span> + Arrays.toString(vertex.getTaskVertices()));</div><div class="line">				&#125;</div><div class="line">				<span class="comment">// 核心方法 通过StackTraceSampleCoordinator 去发起相应的trigger流程, 这里的Future是Flink内部自己定义的异步结果接口 具体可查看FlinkFuture.java(可以深入了解下)</span></div><div class="line">				Future&lt;StackTraceSample&gt; sample = coordinator.triggerStackTraceSample(</div><div class="line">						vertex.getTaskVertices(),</div><div class="line">						numSamples,</div><div class="line">						delayBetweenSamples,</div><div class="line">						MAX_STACK_TRACE_DEPTH);</div><div class="line">				<span class="comment">// 指定异步回调函数（采样结果分析的函数）</span></div><div class="line">				sample.handleAsync(<span class="keyword">new</span> StackTraceSampleCompletionCallback(vertex), executor);</div><div class="line"></div><div class="line">				<span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="StackTraceSampleCoordinator-triggerStackTraceSample"><a href="#StackTraceSampleCoordinator-triggerStackTraceSample" class="headerlink" title="StackTraceSampleCoordinator#triggerStackTraceSample"></a>StackTraceSampleCoordinator#triggerStackTraceSample</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line">	 * Triggers a stack trace sample to all tasks.</div><div class="line">	 *</div><div class="line">	 * @param tasksToSample       Tasks to sample.</div><div class="line">	 * @param numSamples          Number of stack trace samples to collect.</div><div class="line">	 * @param delayBetweenSamples Delay between consecutive samples.</div><div class="line">	 * @param maxStackTraceDepth  Maximum depth of the stack trace. 0 indicates</div><div class="line">	 *                            no maximum and keeps the complete stack trace.</div><div class="line">	 * @return A future of the completed stack trace sample</div><div class="line">	 */</div><div class="line">	@SuppressWarnings(&quot;unchecked&quot;)</div><div class="line">	public Future&lt;StackTraceSample&gt; triggerStackTraceSample(</div><div class="line">			ExecutionVertex[] tasksToSample,</div><div class="line">			int numSamples,</div><div class="line">			Time delayBetweenSamples,</div><div class="line">			int maxStackTraceDepth) &#123;</div><div class="line"></div><div class="line">		checkNotNull(tasksToSample, &quot;Tasks to sample&quot;);</div><div class="line">		checkArgument(tasksToSample.length &gt;= 1, &quot;No tasks to sample&quot;);</div><div class="line">		checkArgument(numSamples &gt;= 1, &quot;No number of samples&quot;);</div><div class="line">		checkArgument(maxStackTraceDepth &gt;= 0, &quot;Negative maximum stack trace depth&quot;);</div><div class="line"></div><div class="line">		// 通过ExecutionVertex获取ExecutionAttemptID和Execution，并最后做存活判断</div><div class="line">		// Execution IDs of running tasks</div><div class="line">		ExecutionAttemptID[] triggerIds = new ExecutionAttemptID[tasksToSample.length];</div><div class="line">		Execution[] executions = new Execution[tasksToSample.length];</div><div class="line"></div><div class="line">		// Check that all tasks are RUNNING before triggering anything. The</div><div class="line">		// triggering can still fail.</div><div class="line">		for (int i = 0; i &lt; triggerIds.length; i++) &#123;</div><div class="line">			Execution execution = tasksToSample[i].getCurrentExecutionAttempt();</div><div class="line">			if (execution != null &amp;&amp; execution.getState() == ExecutionState.RUNNING) &#123;</div><div class="line">				executions[i] = execution;</div><div class="line">				triggerIds[i] = execution.getAttemptId();</div><div class="line">			&#125; else &#123;</div><div class="line">				return FlinkCompletableFuture.completedExceptionally(</div><div class="line">					new IllegalStateException(&quot;Task &quot; + tasksToSample[i]</div><div class="line">					.getTaskNameWithSubtaskIndex() + &quot; is not running.&quot;));</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		synchronized (lock) &#123;</div><div class="line">			if (isShutDown) &#123;</div><div class="line">				return FlinkCompletableFuture.completedExceptionally(new IllegalStateException(&quot;Shut down&quot;));</div><div class="line">			&#125;</div><div class="line"></div><div class="line">			final int sampleId = sampleIdCounter++;</div><div class="line"></div><div class="line">			LOG.debug(&quot;Triggering stack trace sample &#123;&#125;&quot;, sampleId);</div><div class="line">			</div><div class="line">			// 包含采样id和ExecutionAttemptID</div><div class="line">			final PendingStackTraceSample pending = new PendingStackTraceSample(</div><div class="line">					sampleId, triggerIds);</div><div class="line"></div><div class="line">			// Discard the sample if it takes too long. We don&apos;t send cancel</div><div class="line">			// messages to the task managers, but only wait for the responses</div><div class="line">			// and then ignore them.</div><div class="line">			long expectedDuration = numSamples * delayBetweenSamples.toMilliseconds();</div><div class="line">			Time timeout = Time.milliseconds(expectedDuration + sampleTimeout);</div><div class="line"></div><div class="line">			// Add the pending sample before scheduling the discard task to</div><div class="line">			// prevent races with removing it again.</div><div class="line">			pendingSamples.put(sampleId, pending);</div><div class="line"></div><div class="line">			// Trigger all samples</div><div class="line">			// execution是executionVertex的多次执行（recovery...）</div><div class="line">			for (Execution execution: executions) &#123;</div><div class="line">			    // 对相应的execution进行多次numSamples采样，但是都是同一个sampleId</div><div class="line">				final Future&lt;StackTraceSampleResponse&gt; stackTraceSampleFuture = execution.requestStackTraceSample(</div><div class="line">					sampleId,</div><div class="line">					numSamples,</div><div class="line">					delayBetweenSamples,</div><div class="line">					maxStackTraceDepth,</div><div class="line">					timeout);</div><div class="line"></div><div class="line">				stackTraceSampleFuture.handleAsync(new BiFunction&lt;StackTraceSampleResponse, Throwable, Void&gt;() &#123;</div><div class="line">					@Override</div><div class="line">					public Void apply(StackTraceSampleResponse stackTraceSampleResponse, Throwable throwable) &#123;</div><div class="line">						if (stackTraceSampleResponse != null) &#123;</div><div class="line">						    // 收集返回的List&lt;StackTraceElement[]&gt; 到PendingStackTraceSample</div><div class="line">							collectStackTraces(</div><div class="line">								stackTraceSampleResponse.getSampleId(),</div><div class="line">								stackTraceSampleResponse.getExecutionAttemptID(),</div><div class="line">								// 返回的堆栈信息包含所有的采样结果</div><div class="line">								stackTraceSampleResponse.getSamples());</div><div class="line">						&#125; else &#123;</div><div class="line">							cancelStackTraceSample(sampleId, throwable);</div><div class="line">						&#125;</div><div class="line"></div><div class="line">						return null;</div><div class="line">					&#125;</div><div class="line">				&#125;, executor);</div><div class="line">			&#125;</div><div class="line"></div><div class="line">			return pending.getStackTraceSampleFuture();</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure><h3 id="BackPressureStatsTracker-StackTraceSampleCompletionCallback"><a href="#BackPressureStatsTracker-StackTraceSampleCompletionCallback" class="headerlink" title="BackPressureStatsTracker#StackTraceSampleCompletionCallback"></a>BackPressureStatsTracker#StackTraceSampleCompletionCallback</h3><p>采样的结果就是<code>StackTraceSample</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">* java.lang.Object.wait(Native Method)</div><div class="line">* o.a.f.[...].LocalBufferPool.requestBuffer(LocalBufferPool.java:163)</div><div class="line">* o.a.f.[...].LocalBufferPool.requestBufferBlocking(LocalBufferPool.java:133) &lt;--- BLOCKING</div><div class="line">* request</div><div class="line">* [...]</div></pre></td></tr></table></figure><p>利用这样的线程堆栈类型来判断是否block住了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line">		 * Creates the back pressure stats from a stack trace sample.</div><div class="line">		 *</div><div class="line">		 * @param sample Stack trace sample to base stats on.</div><div class="line">		 *</div><div class="line">		 * @return Back pressure stats</div><div class="line">		 */</div><div class="line">		private OperatorBackPressureStats createStatsFromSample(StackTraceSample sample) &#123;</div><div class="line">			Map&lt;ExecutionAttemptID, List&lt;StackTraceElement[]&gt;&gt; traces = sample.getStackTraces();</div><div class="line"></div><div class="line">			// Map task ID to subtask index, because the web interface expects</div><div class="line">			// it like that.</div><div class="line">			// 方便下面根据executionId查询相应的并发度</div><div class="line">			Map&lt;ExecutionAttemptID, Integer&gt; subtaskIndexMap = Maps</div><div class="line">					.newHashMapWithExpectedSize(traces.size());</div><div class="line"></div><div class="line">			Set&lt;ExecutionAttemptID&gt; sampledTasks = sample.getStackTraces().keySet();</div><div class="line"></div><div class="line">			for (ExecutionVertex task : vertex.getTaskVertices()) &#123;</div><div class="line">				ExecutionAttemptID taskId = task.getCurrentExecutionAttempt().getAttemptId();</div><div class="line">				if (sampledTasks.contains(taskId)) &#123;</div><div class="line">					subtaskIndexMap.put(taskId, task.getParallelSubtaskIndex());</div><div class="line">				&#125; else &#123;</div><div class="line">					LOG.debug(&quot;Outdated sample. A task, which is part of the &quot; +</div><div class="line">							&quot;sample has been reset.&quot;);</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line"></div><div class="line">			// Ratio of blocked samples to total samples per sub task. Array</div><div class="line">			// position corresponds to sub task index.</div><div class="line">			// 数组的index和task的并发度相绑定</div><div class="line">			double[] backPressureRatio = new double[traces.size()];</div><div class="line"></div><div class="line">			for (Entry&lt;ExecutionAttemptID, List&lt;StackTraceElement[]&gt;&gt; entry : traces.entrySet()) &#123;</div><div class="line">				int backPressureSamples = 0;</div><div class="line"></div><div class="line">				List&lt;StackTraceElement[]&gt; taskTraces = entry.getValue();</div><div class="line"></div><div class="line">				for (StackTraceElement[] trace : taskTraces) &#123;</div><div class="line">					for (int i = trace.length - 1; i &gt;= 0; i--) &#123;</div><div class="line">						StackTraceElement elem = trace[i];</div><div class="line"></div><div class="line">						if (elem.getClassName().equals(EXPECTED_CLASS_NAME) &amp;&amp;</div><div class="line">								elem.getMethodName().equals(EXPECTED_METHOD_NAME)) &#123;</div><div class="line"></div><div class="line">							backPressureSamples++;</div><div class="line">							break; // Continue with next stack trace</div><div class="line">						&#125;</div><div class="line">					&#125;</div><div class="line">				&#125;</div><div class="line"></div><div class="line">				int subtaskIndex = subtaskIndexMap.get(entry.getKey());</div><div class="line"></div><div class="line">				int size = taskTraces.size();</div><div class="line">				double ratio = (size &gt; 0)</div><div class="line">						? ((double) backPressureSamples) / size</div><div class="line">						: 0;</div><div class="line"></div><div class="line">				backPressureRatio[subtaskIndex] = ratio;</div><div class="line">			&#125;</div><div class="line"></div><div class="line">			return new OperatorBackPressureStats(</div><div class="line">					sample.getSampleId(),</div><div class="line">					sample.getEndTime(),</div><div class="line">					backPressureRatio);</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure><p>至此完成采样</p><p>待学习</p><ol><li>Flink反压原理</li><li>理解清里面很多的Future使用方法</li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;flink反压值采样计算原理&lt;/p&gt;
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>maven-shade-plugin插件高级用法</title>
    <link href="http://www.aitozi.com/advance-maven-shade-plugin.html"/>
    <id>http://www.aitozi.com/advance-maven-shade-plugin.html</id>
    <published>2018-04-10T14:06:03.000Z</published>
    <updated>2019-03-14T16:58:55.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>工作中的maven-shade-plugin插件高级用法小记</p><a id="more"></a><h3 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h3><p>flink-table 库中使用了org.apache.calcite 1.12版本，我在开发flink-cep的过程中引入了org.apache-calcite 1.6版本，服务端要同时引入这两个版本的包，如何解决这个版本冲突问题呢？</p><h3 id="maven-shade-plugin"><a href="#maven-shade-plugin" class="headerlink" title="maven-shade-plugin"></a>maven-shade-plugin</h3><p>使用maven-shade-plugin的relocate功能可以将包名打包成一个别名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">&lt;plugin&gt;</div><div class="line">    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</div><div class="line">    &lt;executions&gt;</div><div class="line">        &lt;execution&gt;</div><div class="line">            &lt;goals&gt;</div><div class="line">                &lt;goal&gt;shade&lt;/goal&gt;</div><div class="line">            &lt;/goals&gt;</div><div class="line">			&lt;configuration&gt;</div><div class="line">				&lt;filters&gt;</div><div class="line">					&lt;filter&gt;</div><div class="line">						&lt;artifact&gt;*:*&lt;/artifact&gt;</div><div class="line">						&lt;excludes&gt;</div><div class="line">							&lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt;</div><div class="line">							&lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt;</div><div class="line">							&lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt;</div><div class="line">						&lt;/excludes&gt;</div><div class="line">					&lt;/filter&gt;</div><div class="line">				&lt;/filters&gt;</div><div class="line">				&lt;relocations&gt;</div><div class="line">				   &lt;!--通过relocation配置将包名进行了替换--&gt;</div><div class="line">					&lt;relocation&gt;</div><div class="line">						&lt;pattern&gt;org.apache.calcite&lt;/pattern&gt;</div><div class="line">						&lt;shadedPattern&gt;org.apache.flink.shaded.cep.calcite&lt;/shadedPattern&gt;</div><div class="line">					&lt;/relocation&gt;</div><div class="line">				&lt;/relocations&gt;</div><div class="line">				&lt;transformers&gt;</div><div class="line">					&lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer&quot;/&gt;</div><div class="line">				&lt;/transformers&gt;</div><div class="line">				&lt;!-- Additional configuration. --&gt;</div><div class="line">			&lt;/configuration&gt;</div><div class="line">        &lt;/execution&gt;</div><div class="line">    &lt;/executions&gt;</div><div class="line">&lt;/plugin&gt;</div></pre></td></tr></table></figure><p>看起来一切都ok了，不过思考一个问题，maven-shade-plugin插件修改包名后项目中引用的包名怎么找到的呢？Class.forName(“xxxx.xxx.xxx”)去生成类的方式还奏不奏效呢？Stack Overflow上关于shade插件的描述也提到了对这种类加载的方式是否奏效的疑问</p><p>然而当应用一跑起来之后发现，并不奏效！！！报出异常 <code>No suitable driver found for jdbc:calcite</code> 也就是<code>DriverManager</code>生成相应的driver的时候没找到相应的类。</p><p><strong>相关的issue</strong></p><p><a href="https://github.com/xerial/sqlite-jdbc/issues/145" target="_blank" rel="external">https://github.com/xerial/sqlite-jdbc/issues/145</a></p><p>在里面我们看到sqlite数据库有这样的配置文件指定了相应driver的类名</p><p><a href="https://github.com/xerial/sqlite-jdbc/blob/master/src/main/resources/java.sql.Driver" target="_blank" rel="external">https://github.com/xerial/sqlite-jdbc/blob/master/src/main/resources/java.sql.Driver</a></p><p>查看calcite也有相应的配置文件指定</p><p><a href="https://github.com/apache/calcite/blob/master/core/src/main/resources/META-INF/services/java.sql.Driver" target="_blank" rel="external">https://github.com/apache/calcite/blob/master/core/src/main/resources/META-INF/services/java.sql.Driver</a></p><p>那是不是在shade的过程中将这个文件内容也进行相应的变更就可以了呢? shade也确实有这样的transformation</p><p><a href="http://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#ServicesResourceTransformer" target="_blank" rel="external">http://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#ServicesResourceTransformer</a></p><blockquote><p>JAR files providing implementations of some interfaces often ship with a META-INF/services/ directory that maps interfaces to their implementation classes for lookup by the service locator. To relocate the class names of these implementation classes, and to merge multiple implementations of the same interface into one service entry, the ServicesResourceTransformer can be used</p></blockquote><p>最后一个问题: maven-shade-plugin 要在3.0以上才能生效</p><p><a href="https://stackoverflow.com/questions/47476402/maven-shade-plugin-relocation-not-updating-a-entry-in-resource-file" target="_blank" rel="external">https://stackoverflow.com/questions/47476402/maven-shade-plugin-relocation-not-updating-a-entry-in-resource-file</a></p><p>效果:<br><img src="http://or0igopk2.bkt.clouddn.com/18-4-10/84724380.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;工作中的maven-shade-plugin插件高级用法小记&lt;/p&gt;
    
    </summary>
    
      <category term="编程工具" scheme="http://www.aitozi.com/categories/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Maven" scheme="http://www.aitozi.com/tags/Maven/"/>
    
  </entry>
  
  <entry>
    <title>上篇·flink 1.4利用kafka0.11实现完整的一致性语义</title>
    <link href="http://www.aitozi.com/flink-kafka-exactly-once.html"/>
    <id>http://www.aitozi.com/flink-kafka-exactly-once.html</id>
    <published>2018-03-11T14:29:44.000Z</published>
    <updated>2019-03-14T17:04:58.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>flink kafka-connector0.10版本分析，与1.4版本中kafka11对比</p><a id="more"></a><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>官方文档在出了1.4之后特意发表了一篇blog，通过以下这两个条件实现了真正意义上的exactly once语义</p><ol><li>kafka producer0.11的事务性</li><li><a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol" target="_blank" rel="external">two phase commit protocol</a></li></ol><p>我们先看0.10版本的kafka-connector的行为逻辑.</p><h1 id="Kafka-Connector10"><a href="#Kafka-Connector10" class="headerlink" title="Kafka-Connector10"></a>Kafka-Connector10</h1><h2 id="kafkaConsumer10"><a href="#kafkaConsumer10" class="headerlink" title="kafkaConsumer10"></a>kafkaConsumer10</h2><h3 id="FlinkKafkaConsumerBase"><a href="#FlinkKafkaConsumerBase" class="headerlink" title="FlinkKafkaConsumerBase"></a>FlinkKafkaConsumerBase</h3><p>这个抽象类实现了<code>CheckpointedFunction</code>, 这个接口的描述：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">* This is the core <span class="class"><span class="keyword">interface</span> <span class="title">for</span> &lt;<span class="title">i</span>&gt;<span class="title">stateful</span> <span class="title">transformation</span> <span class="title">functions</span>&lt;/<span class="title">i</span>&gt;, <span class="title">meaning</span> <span class="title">functions</span></span></div><div class="line">* <span class="title">that</span> <span class="title">maintain</span> <span class="title">state</span> <span class="title">across</span> <span class="title">individual</span> <span class="title">stream</span> <span class="title">records</span>.</div><div class="line">* <span class="title">While</span> <span class="title">more</span> <span class="title">lightweight</span> <span class="title">interfaces</span> <span class="title">exist</span> <span class="title">as</span> <span class="title">shortcuts</span> <span class="title">for</span> <span class="title">various</span> <span class="title">types</span> <span class="title">of</span> <span class="title">state</span>, <span class="title">this</span> <span class="title">interface</span> <span class="title">offer</span> <span class="title">the</span></div><div class="line">* <span class="title">greatest</span> <span class="title">flexibility</span> <span class="title">in</span> <span class="title">managing</span> <span class="title">both</span> &lt;<span class="title">i</span>&gt;<span class="title">keyed</span> <span class="title">state</span>&lt;/<span class="title">i</span>&gt; <span class="title">and</span> &lt;<span class="title">i</span>&gt;<span class="title">operator</span> <span class="title">state</span>&lt;/<span class="title">i</span>&gt;.</div></pre></td></tr></table></figure><p>这个接口中主要要去做两件事情：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//每一次做checkpoint的时候被调用</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception</span>;</div><div class="line"></div><div class="line"><span class="comment">//初始化每一个并发的实例的时候被调用</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception</span>;</div></pre></td></tr></table></figure><p>初始化函数的调用时机是在<code>open</code>之前的：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//StreamTask.java</span></div><div class="line">initializeState();</div><div class="line">openAllOperators();</div></pre></td></tr></table></figure><p>在初始化的函数中提供了一个<code>FunctionSnapshotContext</code><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">void snapshotState(FunctionSnapshotContext context) throws Exception;</div></pre></td></tr></table></figure><p></p><p>让你既可以注册一个KeyedStateStore，也可以注册一个OperatorStateStore</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ManagedInitializationContext</span> </span>&#123;</div><div class="line"></div><div class="line">	<span class="comment">/**</span></div><div class="line">	 * Returns true, if state was restored from the snapshot of a previous execution. This returns always false for</div><div class="line">	 * stateless tasks.</div><div class="line">	 */</div><div class="line">	<span class="function"><span class="keyword">boolean</span> <span class="title">isRestored</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line">	<span class="comment">/**</span></div><div class="line">	 * Returns an interface that allows for registering operator state with the backend.</div><div class="line">	 */</div><div class="line">	<span class="function">OperatorStateStore <span class="title">getOperatorStateStore</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line">	<span class="comment">/**</span></div><div class="line">	 * Returns an interface that allows for registering keyed state with the backend.</div><div class="line">	 */</div><div class="line">	<span class="function">KeyedStateStore <span class="title">getKeyedStateStore</span><span class="params">()</span></span>;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>我们可以看到kafka10是怎么利用这个<code>CheckpointedFunction</code>来管理记录内部offset的呢？</p><h3 id="initializeState"><a href="#initializeState" class="headerlink" title="initializeState"></a>initializeState</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//初始化过程</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">initializeState</span><span class="params">(FunctionInitializationContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line"></div><div class="line">		<span class="comment">// we might have been restored via restoreState() which restores from legacy operator state</span></div><div class="line">		<span class="keyword">if</span> (!restored) &#123;</div><div class="line">			restored = context.isRestored();</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		OperatorStateStore stateStore = context.getOperatorStateStore();</div><div class="line">		offsetsStateForCheckpoint = stateStore.getSerializableListState(DefaultOperatorStateBackend.DEFAULT_OPERATOR_STATE_NAME);</div><div class="line">		<span class="comment">//如果是在重启恢复的过程中</span></div><div class="line">		<span class="keyword">if</span> (context.isRestored()) &#123;</div><div class="line">			<span class="keyword">if</span> (restoredState == <span class="keyword">null</span>) &#123;</div><div class="line">				restoredState = <span class="keyword">new</span> HashMap&lt;&gt;();</div><div class="line">				<span class="keyword">for</span> (Tuple2&lt;KafkaTopicPartition, Long&gt; kafkaOffset : offsetsStateForCheckpoint.get()) &#123;</div><div class="line">				<span class="comment">//partition和相应的offset数</span></div><div class="line">					restoredState.put(kafkaOffset.f0, kafkaOffset.f1);</div><div class="line">				&#125;</div><div class="line"></div><div class="line">				LOG.info(<span class="string">"Setting restore state in the FlinkKafkaConsumer."</span>);</div><div class="line">				<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</div><div class="line">					LOG.debug(<span class="string">"Using the following offsets: &#123;&#125;"</span>, restoredState);</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125; <span class="keyword">else</span> &#123;</div><div class="line">			LOG.info(<span class="string">"No restore state for FlinkKafkaConsumer."</span>);</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure><p>那么我们看到恢复或者初始化的时候将<partition ,offset="">信息保存到了一组HashMap中，但是这个Map怎么作用于消费阶段呢？</partition></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//将从状态中获取到的列表赋予给消费的列表</span></div><div class="line">subscribedPartitionsToStartOffsets = restoredState;</div></pre></td></tr></table></figure><h3 id="snapshot"><a href="#snapshot" class="headerlink" title="snapshot"></a>snapshot</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext context)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">	<span class="keyword">if</span> (!running) &#123;</div><div class="line">		LOG.debug(<span class="string">"snapshotState() called on closed source"</span>);</div><div class="line">	&#125; <span class="keyword">else</span> &#123;</div><div class="line"></div><div class="line">		offsetsStateForCheckpoint.clear();</div><div class="line"></div><div class="line">		<span class="keyword">final</span> AbstractFetcher&lt;?, ?&gt; fetcher = <span class="keyword">this</span>.kafkaFetcher;</div><div class="line">		<span class="keyword">if</span> (fetcher == <span class="keyword">null</span>) &#123;</div><div class="line">			<span class="comment">// fetcher还没有初始化，返回上一次恢复的partition和offset，也就是这里会有个bug。我提了个issue:[FLINK-8869][2]</span></div><div class="line">			<span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; subscribedPartition : subscribedPartitionsToStartOffsets.entrySet()) &#123;</div><div class="line">				offsetsStateForCheckpoint.add(Tuple2.of(subscribedPartition.getKey(), subscribedPartition.getValue()));</div><div class="line">			&#125;</div><div class="line">			<span class="comment">//</span></div><div class="line">			<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</div><div class="line">				<span class="comment">// the map cannot be asynchronously updated, because only one checkpoint call can happen</span></div><div class="line">				<span class="comment">// on this function at a time: either snapshotState() or notifyCheckpointComplete()</span></div><div class="line">				pendingOffsetsToCommit.put(context.getCheckpointId(), restoredState);</div><div class="line">			&#125;</div><div class="line">		&#125; <span class="keyword">else</span> &#123;</div><div class="line">			HashMap&lt;KafkaTopicPartition, Long&gt; currentOffsets = fetcher.snapshotCurrentState();</div><div class="line"></div><div class="line">			<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</div><div class="line">				<span class="comment">// the map cannot be asynchronously updated, because only one checkpoint call can happen</span></div><div class="line">				<span class="comment">// on this function at a time: either snapshotState() or notifyCheckpointComplete()</span></div><div class="line">				pendingOffsetsToCommit.put(context.getCheckpointId(), currentOffsets);</div><div class="line">			&#125;</div><div class="line"></div><div class="line">			<span class="keyword">for</span> (Map.Entry&lt;KafkaTopicPartition, Long&gt; kafkaTopicPartitionLongEntry : currentOffsets.entrySet()) &#123;</div><div class="line">				offsetsStateForCheckpoint.add(</div><div class="line">						Tuple2.of(kafkaTopicPartitionLongEntry.getKey(), kafkaTopicPartitionLongEntry.getValue()));</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</div><div class="line">			<span class="comment">// truncate the map of pending offsets to commit, to prevent infinite growth</span></div><div class="line">			<span class="keyword">while</span> (pendingOffsetsToCommit.size() &gt; MAX_NUM_PENDING_CHECKPOINTS) &#123;</div><div class="line">				pendingOffsetsToCommit.remove(<span class="number">0</span>);</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="offsetCommitMode"><a href="#offsetCommitMode" class="headerlink" title="offsetCommitMode"></a>offsetCommitMode</h3><p>如上述代码中的<code>offsetCommitMode</code>,主要有以下几种</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">DISABLED,</div><div class="line">ON_CHECKPOINTS, <span class="comment">// 完成一次checkpoint后向kafka提交消费offset，只有这种模式下才需要我们手动去提交offset到kafka</span></div><div class="line">KAFKA_PERIODIC;</div></pre></td></tr></table></figure><p>这个配置主要改变了commitOffset回kafka的时机. 首先在snapshot的时候会将对应的checkpointId和相应的offset的列表放入<code>pendingOffsetsToCommit</code>, 在checkpoint完成后回调<code>notifyCheckpointComplete</code>，这里面主要完成了offset的commit工作。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS) &#123;</div><div class="line">			<span class="comment">// only one commit operation must be in progress</span></div><div class="line">			<span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</div><div class="line">				LOG.debug(<span class="string">"Committing offsets to Kafka/ZooKeeper for checkpoint "</span> + checkpointId);</div><div class="line">			&#125;</div><div class="line"></div><div class="line">			<span class="keyword">try</span> &#123;</div><div class="line">				<span class="keyword">final</span> <span class="keyword">int</span> posInMap = pendingOffsetsToCommit.indexOf(checkpointId);</div><div class="line">				<span class="keyword">if</span> (posInMap == -<span class="number">1</span>) &#123;</div><div class="line">					LOG.warn(<span class="string">"Received confirmation for unknown checkpoint id &#123;&#125;"</span>, checkpointId);</div><div class="line">					<span class="keyword">return</span>;</div><div class="line">				&#125;</div><div class="line"></div><div class="line">				<span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</div><div class="line">				HashMap&lt;KafkaTopicPartition, Long&gt; offsets =</div><div class="line">					(HashMap&lt;KafkaTopicPartition, Long&gt;) pendingOffsetsToCommit.remove(posInMap);</div><div class="line"></div><div class="line">				<span class="comment">// remove older checkpoints in map</span></div><div class="line">				<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; posInMap; i++) &#123;</div><div class="line">					pendingOffsetsToCommit.remove(<span class="number">0</span>);</div><div class="line">				&#125;</div><div class="line"></div><div class="line">				<span class="keyword">if</span> (offsets == <span class="keyword">null</span> || offsets.size() == <span class="number">0</span>) &#123;</div><div class="line">					LOG.debug(<span class="string">"Checkpoint state was empty."</span>);</div><div class="line">					<span class="keyword">return</span>;</div><div class="line">				&#125;</div><div class="line"></div><div class="line">				fetcher.commitInternalOffsetsToKafka(offsets, offsetCommitCallback);</div><div class="line">			&#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">				<span class="keyword">if</span> (running) &#123;</div><div class="line">					<span class="keyword">throw</span> e;</div><div class="line">				&#125;</div><div class="line">				<span class="comment">// else ignore exception if we are no longer running</span></div><div class="line">			&#125;</div></pre></td></tr></table></figure><h3 id="消费partition分配问题"><a href="#消费partition分配问题" class="headerlink" title="消费partition分配问题"></a>消费partition分配问题</h3><ul><li><p>在从上次点恢复的情况下是直接从state中获取应该读取哪一个partition，offset。如果并发度改变了会做出什么样的反馈呢?会正确做出rescale吗</p></li><li><p>第一次进行读取的时候会初始化处当前task（并发度）所需要订阅的partition</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">static</span> Map&lt;KafkaTopicPartition, Long&gt; <span class="title">initializeSubscribedPartitionsToStartOffsets</span><span class="params">(</span></span></div><div class="line">			List&lt;KafkaTopicPartition&gt; kafkaTopicPartitions, //topic的所有partition</div><div class="line">			<span class="keyword">int</span> indexOfThisSubtask, // 当前task的维度</div><div class="line">			<span class="keyword">int</span> numParallelSubtasks, // 总的并发度</div><div class="line">			StartupMode startupMode, // 从哪个offset消费的模式（最新，最老，指定offset）</div><div class="line">			Map&lt;KafkaTopicPartition, Long&gt; specificStartupOffsets) &#123;</div><div class="line"></div><div class="line">		Map&lt;KafkaTopicPartition, Long&gt; subscribedPartitionsToStartOffsets = <span class="keyword">new</span> HashMap&lt;&gt;(kafkaTopicPartitions.size());</div><div class="line"></div><div class="line">		<span class="keyword">for</span> (KafkaTopicPartition kafkaTopicPartition : kafkaTopicPartitions) &#123;</div><div class="line">			<span class="comment">// only handle partitions that this subtask should subscribe to（选取当前subtask所需要订阅的partition）</span></div><div class="line">			<span class="keyword">if</span> (KafkaTopicPartitionAssigner.assign(kafkaTopicPartition, numParallelSubtasks) == indexOfThisSubtask) &#123;</div><div class="line">				<span class="keyword">if</span> (startupMode != StartupMode.SPECIFIC_OFFSETS) &#123;</div><div class="line">					<span class="comment">//StateSentinel都是一串随机的负数占位符(都是一个标记在KafkaConsumerThread中进行判断)</span></div><div class="line">					subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, startupMode.getStateSentinel());</div><div class="line">				&#125; <span class="keyword">else</span> &#123;</div><div class="line">					<span class="keyword">if</span> (specificStartupOffsets == <span class="keyword">null</span>) &#123;</div><div class="line">						<span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(</div><div class="line">							<span class="string">"Startup mode for the consumer set to "</span> + StartupMode.SPECIFIC_OFFSETS +</div><div class="line">								<span class="string">", but no specific offsets were specified"</span>);</div><div class="line">					&#125;</div><div class="line"></div><div class="line">					Long specificOffset = specificStartupOffsets.get(kafkaTopicPartition);</div><div class="line">					<span class="keyword">if</span> (specificOffset != <span class="keyword">null</span>) &#123;</div><div class="line">						<span class="comment">// since the specified offsets represent the next record to read, we subtract</span></div><div class="line">						<span class="comment">// it by one so that the initial state of the consumer will be correct</span></div><div class="line">						<span class="comment">// 这里需要减去1</span></div><div class="line">						subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, specificOffset - <span class="number">1</span>);</div><div class="line">					&#125; <span class="keyword">else</span> &#123;</div><div class="line">						subscribedPartitionsToStartOffsets.put(kafkaTopicPartition, KafkaTopicPartitionStateSentinel.GROUP_OFFSET);</div><div class="line">					&#125;</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		<span class="keyword">return</span> subscribedPartitionsToStartOffsets;</div><div class="line">&#125;</div></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">assign</span><span class="params">(KafkaTopicPartition partition, <span class="keyword">int</span> numParallelSubtasks)</span> </span>&#123;</div><div class="line">	<span class="keyword">int</span> startIndex = ((partition.getTopic().hashCode() * <span class="number">31</span>) &amp; <span class="number">0x7FFFFFFF</span>) % numParallelSubtasks;</div><div class="line"></div><div class="line">	<span class="comment">// here, the assumption is that the id of Kafka partitions are always ascending</span></div><div class="line">	<span class="comment">// starting from 0, and therefore can be used directly as the offset clockwise from the start index</span></div><div class="line">	<span class="comment">// 这里看出：每个Partition只会分配到一个subtask来消费</span></div><div class="line">	<span class="comment">// 1. partition &gt; parallel 一个subtask会订阅多个partition</span></div><div class="line">	<span class="comment">// 2. partition &lt; parallel 有subtask会是空闲的</span></div><div class="line">	<span class="comment">// 3. startIndex由topic名字计算得出</span></div><div class="line">	<span class="keyword">return</span> (startIndex + partition.getPartition()) % numParallelSubtasks;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="消费模型kafka10"><a href="#消费模型kafka10" class="headerlink" title="消费模型kafka10"></a>消费模型kafka10</h3><p>在完成partition订阅之后，就要开始真正的run方法了，<code>FlinkKafkaConsumer</code>也是实现自<code>SouceFunction</code>，因此主要的逻辑也都是在run方法中实现。 主要逻辑：</p><p>kafkaConsumerThread和Kafka10Fetcher通过<code>Handover</code>交互,我觉得这段代码写的很不错，可以好好学习下。可以形象的比作在接力跑：<code>kafkaConsumerThread</code>通过真正的消费线程消费放入一个<code>HandOver</code>，再由kafkaFetcher去poll，完成整个消费过程。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// we need only do work, if we actually have partitions assigned</span></div><div class="line"><span class="keyword">if</span> (!subscribedPartitionsToStartOffsets.isEmpty()) &#123;</div><div class="line"></div><div class="line">	<span class="comment">// create the fetcher that will communicate with the Kafka brokers</span></div><div class="line">	<span class="keyword">final</span> AbstractFetcher&lt;T, ?&gt; fetcher = createFetcher(</div><div class="line">			sourceContext,</div><div class="line">			subscribedPartitionsToStartOffsets,</div><div class="line">			periodicWatermarkAssigner,</div><div class="line">			punctuatedWatermarkAssigner,</div><div class="line">			(StreamingRuntimeContext) getRuntimeContext(),</div><div class="line">			offsetCommitMode);</div><div class="line"></div><div class="line">	<span class="comment">// publish the reference, for snapshot-, commit-, and cancel calls</span></div><div class="line">	<span class="comment">// IMPORTANT: We can only do that now, because only now will calls to</span></div><div class="line">	<span class="comment">//            the fetchers 'snapshotCurrentState()' method return at least</span></div><div class="line">	<span class="comment">//            the restored offsets</span></div><div class="line">	<span class="keyword">this</span>.kafkaFetcher = fetcher;</div><div class="line">	<span class="keyword">if</span> (!running) &#123;</div><div class="line">		<span class="keyword">return</span>;</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="comment">// (3) run the fetcher' main work method</span></div><div class="line">	<span class="comment">// 主要工作方法</span></div><div class="line">	fetcher.runFetchLoop();</div><div class="line">&#125;</div><div class="line"><span class="keyword">else</span> &#123;</div><div class="line">	<span class="comment">// this source never completes, so emit a Long.MAX_VALUE watermark</span></div><div class="line">	<span class="comment">// to not block watermark forwarding</span></div><div class="line">	<span class="comment">// 发送最大的watermark就不会block住下游的watermark更新</span></div><div class="line">	sourceContext.emitWatermark(<span class="keyword">new</span> Watermark(Long.MAX_VALUE));</div><div class="line"></div><div class="line">	<span class="comment">// wait until this is canceled</span></div><div class="line">	<span class="keyword">final</span> Object waitLock = <span class="keyword">new</span> Object();</div><div class="line">	<span class="keyword">while</span> (running) &#123;</div><div class="line">		<span class="keyword">try</span> &#123;</div><div class="line">			<span class="comment">//noinspection SynchronizationOnLocalVariableOrMethodParameter</span></div><div class="line">			<span class="keyword">synchronized</span> (waitLock) &#123;</div><div class="line">				waitLock.wait();</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">			<span class="keyword">if</span> (!running) &#123;</div><div class="line">				<span class="comment">// restore the interrupted state, and fall through the loop</span></div><div class="line">				<span class="comment">// 打断当前线程</span></div><div class="line">				Thread.currentThread().interrupt();</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">// Handover的描述， Handover代码可以再好好学习下。</div><div class="line">* The Handover is a utility to hand over data (a buffer of records) and exception from a</div><div class="line">* &lt;i&gt;producer&lt;/i&gt; thread to a &lt;i&gt;consumer&lt;/i&gt; thread. It effectively behaves like a</div><div class="line">* &quot;size one blocking queue&quot;, with some extras around exception reporting, closing, and</div><div class="line">* waking up thread without &#123;@link Thread#interrupt() interrupting&#125; threads.</div></pre></td></tr></table></figure><h3 id="KafkaFetcher"><a href="#KafkaFetcher" class="headerlink" title="KafkaFetcher"></a>KafkaFetcher</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">runFetchLoop</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">	<span class="keyword">try</span> &#123;</div><div class="line">		<span class="keyword">final</span> Handover handover = <span class="keyword">this</span>.handover;</div><div class="line"></div><div class="line">		<span class="comment">// kick off the actual Kafka consumer</span></div><div class="line">		<span class="comment">// 启动真正的消费线程</span></div><div class="line">		consumerThread.start();</div><div class="line"></div><div class="line">		<span class="keyword">while</span> (running) &#123;</div><div class="line">			<span class="comment">// this blocks until we get the next records</span></div><div class="line">			<span class="comment">// it automatically re-throws exceptions encountered in the fetcher thread</span></div><div class="line">			<span class="comment">// 在handover中获取真正的数据，并抛出其他线程中的异常</span></div><div class="line">			<span class="keyword">final</span> ConsumerRecords&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; records = handover.pollNext();</div><div class="line"></div><div class="line">			<span class="comment">// get the records for each topic partition</span></div><div class="line">			<span class="comment">// subscribedPartitionStates维护的是每个partition的状态（partition，KPH（partition的描述，依据版本可能不同），offset, committedOffset, watermark）</span></div><div class="line">			<span class="keyword">for</span> (KafkaTopicPartitionState&lt;TopicPartition&gt; partition : subscribedPartitionStates()) &#123;</div><div class="line"></div><div class="line">				List&lt;ConsumerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt; partitionRecords =</div><div class="line">						records.records(partition.getKafkaPartitionHandle());</div><div class="line"></div><div class="line">				<span class="keyword">for</span> (ConsumerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; record : partitionRecords) &#123;</div><div class="line">					<span class="keyword">final</span> T value = deserializer.deserialize(</div><div class="line">							record.key(), record.value(),</div><div class="line">							record.topic(), record.partition(), record.offset());</div><div class="line"></div><div class="line">					<span class="keyword">if</span> (deserializer.isEndOfStream(value)) &#123;</div><div class="line">						<span class="comment">// end of stream signaled</span></div><div class="line">						running = <span class="keyword">false</span>;</div><div class="line">						<span class="keyword">break</span>;</div><div class="line">					&#125;</div><div class="line"></div><div class="line">					<span class="comment">// emit the actual record. this also updates offset state atomically</span></div><div class="line">					<span class="comment">// and deals with timestamps and watermark generation</span></div><div class="line">					<span class="comment">// 这里会进入真正的通过sourceContext发送数据的代码 如下</span></div><div class="line">					emitRecord(value, partition, record.offset(), record);</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">finally</span> &#123;</div><div class="line">		<span class="comment">// this signals the consumer thread that no more work is to be done</span></div><div class="line">		consumerThread.shutdown();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="comment">// on a clean exit, wait for the runner thread</span></div><div class="line">	<span class="keyword">try</span> &#123;</div><div class="line">		consumerThread.join();</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">		<span class="comment">// may be the result of a wake-up interruption after an exception.</span></div><div class="line">		<span class="comment">// we ignore this here and only restore the interruption state</span></div><div class="line">		Thread.currentThread().interrupt();</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">emitRecordWithTimestamp</span><span class="params">(</span></span></div><div class="line">		T record, KafkaTopicPartitionState&lt;KPH&gt; partitionState, <span class="keyword">long</span> offset, <span class="keyword">long</span> timestamp) <span class="keyword">throws</span> Exception &#123;</div><div class="line"></div><div class="line">	<span class="keyword">if</span> (record != <span class="keyword">null</span>) &#123;</div><div class="line">		<span class="keyword">if</span> (timestampWatermarkMode == NO_TIMESTAMPS_WATERMARKS) &#123;</div><div class="line">			<span class="comment">// fast path logic, in case there are no watermarks generated in the fetcher</span></div><div class="line"></div><div class="line">			<span class="comment">// emit the record, using the checkpoint lock to guarantee</span></div><div class="line">			<span class="comment">// atomicity of record emission and offset state update</span></div><div class="line">			<span class="keyword">synchronized</span> (checkpointLock) &#123;</div><div class="line">				sourceContext.collectWithTimestamp(record, timestamp);</div><div class="line">				partitionState.setOffset(offset);</div><div class="line">			&#125;</div><div class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> (timestampWatermarkMode == PERIODIC_WATERMARKS) &#123;</div><div class="line">		    <span class="comment">// 更新partitionstate中的watermark状态</span></div><div class="line">			emitRecordWithTimestampAndPeriodicWatermark(record, partitionState, offset, timestamp);</div><div class="line">		&#125; <span class="keyword">else</span> &#123;</div><div class="line">			emitRecordWithTimestampAndPunctuatedWatermark(record, partitionState, offset, timestamp);</div><div class="line">		&#125;</div><div class="line">	&#125; <span class="keyword">else</span> &#123;</div><div class="line">		<span class="comment">// if the record is null, simply just update the offset state for partition</span></div><div class="line">		<span class="keyword">synchronized</span> (checkpointLock) &#123;</div><div class="line">			partitionState.setOffset(offset);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>在设置了kafkaTimestampassigner之后就会进行一个定时任务向下游发送watermark，值为所有partition维护的最小值：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line"></div><div class="line">	<span class="keyword">long</span> minAcrossAll = Long.MAX_VALUE;</div><div class="line">	<span class="keyword">for</span> (KafkaTopicPartitionStateWithPeriodicWatermarks&lt;?, ?&gt; state : allPartitions) &#123;</div><div class="line">		</div><div class="line">		<span class="comment">// we access the current watermark for the periodic assigners under the state</span></div><div class="line">		<span class="comment">// lock, to prevent concurrent modification to any internal variables</span></div><div class="line">		<span class="keyword">final</span> <span class="keyword">long</span> curr;</div><div class="line">		<span class="comment">//noinspection SynchronizationOnLocalVariableOrMethodParameter</span></div><div class="line">		<span class="comment">// 这个锁是防止别的线程修改其他变量</span></div><div class="line">		<span class="keyword">synchronized</span> (state) &#123;</div><div class="line">			curr = state.getCurrentWatermarkTimestamp();</div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		minAcrossAll = Math.min(minAcrossAll, curr);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="comment">// emit next watermark, if there is one</span></div><div class="line">	<span class="keyword">if</span> (minAcrossAll &gt; lastWatermarkTimestamp) &#123;</div><div class="line">		lastWatermarkTimestamp = minAcrossAll;</div><div class="line">		emitter.emitWatermark(<span class="keyword">new</span> Watermark(minAcrossAll));</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	<span class="comment">// schedule the next watermark</span></div><div class="line">	timerService.registerTimer(timerService.getCurrentProcessingTime() + interval, <span class="keyword">this</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="KafkaConsumerThread"><a href="#KafkaConsumerThread" class="headerlink" title="KafkaConsumerThread"></a>KafkaConsumerThread</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (records == <span class="keyword">null</span>) &#123;</div><div class="line">				<span class="keyword">try</span> &#123;</div><div class="line">					records = consumer.poll(pollTimeout);</div><div class="line">				&#125;</div><div class="line">				<span class="keyword">catch</span> (WakeupException we) &#123;</div><div class="line">					<span class="keyword">continue</span>;</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line"></div><div class="line">			<span class="keyword">try</span> &#123;</div><div class="line">				handover.produce(records);</div><div class="line">				records = <span class="keyword">null</span>;</div><div class="line">			&#125;</div><div class="line">			<span class="keyword">catch</span> (Handover.WakeupException e) &#123;</div><div class="line">				<span class="comment">// fall through the loop</span></div><div class="line">			&#125;</div></pre></td></tr></table></figure><p>主要做的工作就是从consumer消费数据塞入handover，等待拉取</p><h3 id="Handover"><a href="#Handover" class="headerlink" title="Handover"></a>Handover</h3><h3 id="桥接模式"><a href="#桥接模式" class="headerlink" title="桥接模式"></a>桥接模式</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumerCallBridge</span> </span>&#123;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">assignPartitions</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, List&lt;TopicPartition&gt; topicPartitions)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">		consumer.assign(topicPartitions);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekPartitionToBeginning</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, TopicPartition partition)</span> </span>&#123;</div><div class="line">		consumer.seekToBeginning(partition);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekPartitionToEnd</span><span class="params">(KafkaConsumer&lt;?, ?&gt; consumer, TopicPartition partition)</span> </span>&#123;</div><div class="line">		consumer.seekToEnd(partition);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>解决08 09 10 版本的api不兼容问题</p><h2 id="Kafkaproducer10"><a href="#Kafkaproducer10" class="headerlink" title="Kafkaproducer10"></a>Kafkaproducer10</h2><p>###initializeState</p><p>什么都不做</p><h3 id="snapshot-1"><a href="#snapshot-1" class="headerlink" title="snapshot"></a>snapshot</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">snapshotState</span><span class="params">(FunctionSnapshotContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">	<span class="comment">// check for asynchronous errors and fail the checkpoint if necessary</span></div><div class="line">	checkErroneous();</div><div class="line"></div><div class="line">	<span class="keyword">if</span> (flushOnCheckpoint) &#123;</div><div class="line">		<span class="comment">// flushing is activated: We need to wait until pendingRecords is 0</span></div><div class="line">		flush();</div><div class="line">		<span class="keyword">synchronized</span> (pendingRecordsLock) &#123;</div><div class="line">			<span class="keyword">if</span> (pendingRecords != <span class="number">0</span>) &#123;</div><div class="line">				<span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Pending record count must be zero at this point: "</span> + pendingRecords);</div><div class="line">			&#125;</div><div class="line"></div><div class="line">			<span class="comment">// if the flushed requests has errors, we should propagate it also and fail the checkpoint</span></div><div class="line">			checkErroneous();</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>这里涉及到一个flushOnCheckPoint的问题，再调用<code>producer.flush</code>期间，producer会将所有没写入的，在buffer中的数据刷盘，然后调用commitCallBack，这就保证了ckpt之后数据不会丢的问题。</p><p>主要工作方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(IN next)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">	<span class="comment">// propagate asynchronous errors</span></div><div class="line">	checkErroneous();</div><div class="line"></div><div class="line">	<span class="keyword">byte</span>[] serializedKey = schema.serializeKey(next);</div><div class="line">	<span class="keyword">byte</span>[] serializedValue = schema.serializeValue(next);</div><div class="line">	<span class="comment">// 每条元素可以自己自己要写到的topic</span></div><div class="line">	String targetTopic = schema.getTargetTopic(next);</div><div class="line">	<span class="keyword">if</span> (targetTopic == <span class="keyword">null</span>) &#123;</div><div class="line">		targetTopic = defaultTopicId;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="keyword">int</span>[] partitions = <span class="keyword">this</span>.topicPartitionsMap.get(targetTopic);</div><div class="line">	<span class="keyword">if</span>(<span class="keyword">null</span> == partitions) &#123;</div><div class="line">		partitions = getPartitionsByTopic(targetTopic, producer);</div><div class="line">		<span class="keyword">this</span>.topicPartitionsMap.put(targetTopic, partitions);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	ProducerRecord&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; record;</div><div class="line">	<span class="keyword">if</span> (flinkKafkaPartitioner == <span class="keyword">null</span>) &#123;</div><div class="line">		record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(targetTopic, serializedKey, serializedValue);</div><div class="line">	&#125; <span class="keyword">else</span> &#123;</div><div class="line">		record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(</div><div class="line">				targetTopic,</div><div class="line">				flinkKafkaPartitioner.partition(next, serializedKey, serializedValue, targetTopic, partitions),</div><div class="line">				serializedKey,</div><div class="line">				serializedValue);</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">if</span> (flushOnCheckpoint) &#123;</div><div class="line">		<span class="keyword">synchronized</span> (pendingRecordsLock) &#123;</div><div class="line">			pendingRecords++;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	producer.send(record, callback);</div><div class="line">&#125;</div></pre></td></tr></table></figure><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><ol><li><p>kafka恢复状态直接从状态中去获取了之前保存的partition和offset，但是如果是扩容partition的场景就不会从新的Partition消费 issue:<a href="https://issues.apache.org/jira/projects/FLINK/issues/FLINK-8869?filter=reportedbyme" target="_blank" rel="external">FLINK-8869</a></p></li><li><p>flink内部维护了offset，为什么向kafka提交的时候还需要在checkpoint之后再提交而不是定时提交就算了？</p><p>因为虽然从checkpoint点恢复的时候不需要从kafka broker获取消费点的位置了，但是如果是应用重启消费上次消费到的点的数据，这个offset就是flink向kafka提交的，放在checkpoint完成后去做的好处就是让应用即使不是从上个点恢复的，也能够从kafka消费正确的offset点。</p></li><li><p>如果在新的checkpoint没打之前任务失败了，重新从上次的offset点消费的话下游数据是不是重复了?</p><p><strong>是的，因为有一部分数据经过处理已经sink出去了，因此才需要0.11的一致性语义</strong></p></li></ol><p><em>以上代码：<br>kafka-connector0.10 来源于release1.3.2<br>kafka-connector0.11 来源于release1.4.0</em></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;flink kafka-connector0.10版本分析，与1.4版本中kafka11对比&lt;/p&gt;
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
      <category term="Kafka" scheme="http://www.aitozi.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>flink-cep-paper</title>
    <link href="http://www.aitozi.com/flink-cep-paper.html"/>
    <id>http://www.aitozi.com/flink-cep-paper.html</id>
    <published>2018-02-25T12:19:15.000Z</published>
    <updated>2019-03-14T17:04:45.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>CEP实现论文</p><a id="more"></a><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>2016年1月29号，社区提了这个patch<a href="https://github.com/apache/flink/pull/1557" target="_blank" rel="external">[FLINK-3216]</a>引入flink cep模块，flink cep实现基于论文：<a href="https://people.cs.umass.edu/%7Eyanlei/publications/sase-sigmod08.pdf" target="_blank" rel="external">Efficient Pattern Matching over Event Streams</a>。我们来深入阅读以下这篇论文。</p><h3 id="event-pattern查询表达形式"><a href="#event-pattern查询表达形式" class="headerlink" title="event pattern查询表达形式"></a>event pattern查询表达形式</h3><p>先看三个查询<br><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/25532292.jpg" alt=""></p><p><em>Query1</em></p><p>匹配出的是：挑选从货架取出的商品，未经checkout，带离了商店的行为序列</p><ol><li><code>SEQ(Shelf a, ∼(Register b), Exit c)</code></li><li>where条件指定了a，b，c的tagid相同</li><li>within指定窗口大小12小时</li></ol><p><em>Query2</em></p><p>匹配出的是：一个污染报警和相关受污染的发货卸货流转站点序列</p><ol><li>报警类型是污染</li><li>匹配出的装载点和前一个到达点一致</li><li>within指定匹配窗口大小是3小时</li></ol><p><em>Query3</em></p><p>匹配出的是：匹配股票交易拐点</p><ol><li>初始成交量高于1000</li><li>持续增加，最后成交量突然跌为最高点的80%以下的序列</li><li>1小时的时间窗口</li></ol><p>可以看出规则是怎么描述的：</p><ol><li>指定匹配出的序列是什么（structure）</li><li>限制或筛选条件</li></ol><p>还需要匹配策略来进行规制匹配。</p><ol><li>严格的邻近匹配（Strict contiguity）：只有连续两个event能满足匹配，中间不能夹杂其他不符合条件的元素</li><li>分区邻近匹配（Partition contiguity）：这就是说被选出的两个event之间不一定需要完全紧密相连，但是在一个分区中的需要紧密相连 如<em>Query3</em>中的a[]需要连续，a与b不需要连续</li><li>skip_till_next_match: 所有的不相关的元素都会被跳过，不再考虑是不是邻近的元素</li><li>skip_till_any_match: <em>Query2</em>解释了这种用法。比如当前最后识别的shipment到达了X位置。这时候读进来一条，X-&gt;Y的Event，在<code>skip_till_any_match</code>匹配规则下系统会做两件事：1.接收X-&gt;Y事件，更新状态2.在当前状态的另一个实例中不接收他来保留一个状态。这样当下一个比如来了个X-&gt;Z事件那还是可以接受这个实例。这种策略就是返回所有的可能的序列（allowing non-deterministic actions）在接收到同一个事件后可以有不一样的走向</li></ol><hr><p>以上介绍了event pattern的查询形式，接下来我们看其内部实现算法。</p><h3 id="NFA-非确定性自动状态机"><a href="#NFA-非确定性自动状态机" class="headerlink" title="NFA 非确定性自动状态机"></a>NFA 非确定性自动状态机</h3><p>非确定性自动状态机由以下结构组成。A = (Q,E,θ,q1,F),</p><pre><code>Q: 一系列状态
E： 边
θ： 计算公式
q1：起始状态
F: 结束状态
</code></pre><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/18248041.jpg" alt=""></p><p><strong>States</strong></p><blockquote><p>the start state,a[1], is where the matching process begins. It awaits input to start the Kleene plus and to select an event into the a[1] unit of the match<br>buffer. At the next state a[i], it attempts to select another event into the a[i] (i &gt; 1) unit of the buffer. The subsequent state b denotes that the matching process has fulfilled the Kleene plus (for a particular match) and is ready to process the next pattern component. The final state, F, represents<br>the completion of the process, resulting in the creation of a pattern match.</p></blockquote><p><strong>Edges</strong></p><blockquote><p>Each state is associated with a number of edges,<br>representing the actions that can be taken at the state. As Figure 2(a) shows, each state that is a singleton state or the first state,p[1], of a pair has a forward begin edge. Each second state, p[i], of a pair has a forward proceed edge, and a looping take edge. Every state (except the start and final states) has a looping ignore edge. The start state has no edges to it as we are only interested in matches that start with selected events.<br>Each edge at a state, q, is precisely described by a triplet:(1) a formula that specifies the condition on taking it, denoted by θq-edge, (2) an operation on the input stream (i.e.,consume an event or not), and (3) an operation on the match buffer (i.e., write to the buffer or not). Formulas of edges are compiled from pattern queries, which we explain in detail shortly. As shown in Figure 2(a), we use solid lines to denote begin and take edges that consume an event from the input and write it to the buffer, and dashed lines for ignore edges that consume an event but do not write it to the buffer. The proceed edge is a special edge: it does not consume any input event but only evaluates its formula and tries proceeding. We distinguish the proceed edge from ignore edges in the style of arrow, denoting its behavior.</p></blockquote><p><em>非确定性</em><br>某些state具有两条边，但是这两条边的行为不一定是完全相反的</p><p>NFA算法过程：</p><ol><li>先根据整个structure构建state和边的行为</li><li>将condition翻译成边上的公式</li><li>将select strategy翻译成边上的公式</li><li>将时间窗口翻译成：最先匹配的元素和最后的元素时长不能超过窗口大小</li></ol><p>优化：</p><ol><li>先做过滤</li><li>将window条件前置</li><li>优化proceed边，进入下一个state的时候要检查是否满足begin的条件判断</li></ol><h3 id="带版本的内存共享"><a href="#带版本的内存共享" class="headerlink" title="带版本的内存共享"></a>带版本的内存共享</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/20508653.jpg" alt=""></p><p>通过带版本的共享内存解决多runs之间的重复事件问题。</p><h3 id="Computation-state"><a href="#Computation-state" class="headerlink" title="Computation state"></a>Computation state</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/48608682.jpg" alt=""></p><ol><li>version number of a run</li><li>current automaton state the run is in</li><li>a pointer to the most revent event selected into the buffer</li><li>a vector =&gt; 保留做edge判断的所需的最少的元素。例如上图中存储了a[i]的和和count为了计算平均值</li></ol><h3 id="Merge相同的Runs"><a href="#Merge相同的Runs" class="headerlink" title="Merge相同的Runs"></a>Merge相同的Runs</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/87062132.jpg" alt=""></p><p><img src="http://or0igopk2.bkt.clouddn.com/18-2-25/41465208.jpg" alt=""></p><p>将Query3中的avg算法改为max算法，他们在接收到e4之后达到了同样的状态，因此其之后的运算可以merge为一个。</p><ol><li>首先探测什么时候两个runs一样了，引入了算子M。每一个状态机都有一个mask，每一个mask对每一个Vector中列都有一个bit位，都相同时那么这两个run就是相等的。</li><li>创建combined的run<ol><li>时间设置为最小的那个时间</li></ol></li></ol><h3 id="Backtrack算法（回溯）"><a href="#Backtrack算法（回溯）" class="headerlink" title="Backtrack算法（回溯）"></a>Backtrack算法（回溯）</h3><p>每次只跑一个run，跑失败了再回退到分叉点跑另一条。也就是以广度优先搜索方式 <code>breadth first search manner</code></p><h3 id="待补充"><a href="#待补充" class="headerlink" title="待补充"></a>待补充</h3><ol><li>性能评估方案</li><li>有哪些因素会影响计算速度和吞吐</li><li>内存优化管理措施</li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;CEP实现论文&lt;/p&gt;
    
    </summary>
    
      <category term="论文" scheme="http://www.aitozi.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
      <category term="CEP" scheme="http://www.aitozi.com/tags/CEP/"/>
    
  </entry>
  
  <entry>
    <title>常阅读博客</title>
    <link href="http://www.aitozi.com/blog-collection.html"/>
    <id>http://www.aitozi.com/blog-collection.html</id>
    <published>2018-01-23T01:58:30.000Z</published>
    <updated>2019-03-14T16:59:53.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>优秀可以常阅读汲取的博客</p><a id="more"></a><h2 id="java学习"><a href="#java学习" class="headerlink" title="java学习"></a>java学习</h2><p>java开发: <a href="http://www.cnblogs.com/xrq730/p/5260294.html" target="_blank" rel="external">http://www.cnblogs.com/xrq730/p/5260294.html</a><br>阿里工程师: <a href="http://www.hollischuang.com/" target="_blank" rel="external">http://www.hollischuang.com/</a><br>阿里中间件博客: <a href="http://jm.taobao.org/" target="_blank" rel="external">http://jm.taobao.org/</a><br>Jvm 你假笨: <a href="http://lovestblog.cn" target="_blank" rel="external">http://lovestblog.cn</a><br>RednaxelaFX: <a href="http://rednaxelafx.iteye.com/" target="_blank" rel="external">http://rednaxelafx.iteye.com/</a><br>死磕java: <a href="http://cmsblogs.com/" target="_blank" rel="external">http://cmsblogs.com/</a><br><a href="http://www.cnblogs.com/rollenholt/" target="_blank" rel="external">http://www.cnblogs.com/rollenholt/</a><br><a href="https://wenchao.ren/archives/171" target="_blank" rel="external">https://wenchao.ren/archives/171</a><br><a href="https://halfrost.com" target="_blank" rel="external">https://halfrost.com</a><br><a href="http://www.hollischuang.com/" target="_blank" rel="external">http://www.hollischuang.com/</a></p><h2 id="jvm"><a href="#jvm" class="headerlink" title="jvm"></a>jvm</h2><p><a href="http://greenteajug.cn/" target="_blank" rel="external">http://greenteajug.cn/</a> （查找perfj工具无意翻到的一篇博客，来自阿里坤谷莫大神</p><h2 id="Go"><a href="#Go" class="headerlink" title="Go"></a>Go</h2><p><a href="http://colobu.com/" target="_blank" rel="external">http://colobu.com/</a></p><h2 id="scala学习"><a href="#scala学习" class="headerlink" title="scala学习"></a>scala学习</h2><p><a href="http://hongjiang.info/scala/" target="_blank" rel="external">http://hongjiang.info/scala/</a></p><h2 id="流计算Flink"><a href="#流计算Flink" class="headerlink" title="流计算Flink"></a>流计算Flink</h2><p>阿里流计算工程师: <a href="http://www.cnblogs.com/fxjwind/" target="_blank" rel="external">http://www.cnblogs.com/fxjwind/</a><br>wuchong: <a href="http://wuchong.me/" target="_blank" rel="external">http://wuchong.me/</a><br>yuzhao: <a href="http://chenyuzhao.me/" target="_blank" rel="external">http://chenyuzhao.me/</a><br>vinoyang: <a href="http://blog.csdn.net/yanghua_kobe?viewmode=contents" target="_blank" rel="external">http://blog.csdn.net/yanghua_kobe?viewmode=contents</a><br>Imalds: <a href="http://blog.csdn.net/lmalds?viewmode=contents" target="_blank" rel="external">http://blog.csdn.net/lmalds?viewmode=contents</a></p><h2 id="大数据架构"><a href="#大数据架构" class="headerlink" title="大数据架构"></a>大数据架构</h2><p><a href="http://www.jasongj.com/" target="_blank" rel="external">http://www.jasongj.com/</a></p><h2 id="大数据hadoop"><a href="#大数据hadoop" class="headerlink" title="大数据hadoop"></a>大数据hadoop</h2><p><a href="http://blog.csdn.net/androidlushangderen" target="_blank" rel="external">http://blog.csdn.net/androidlushangderen</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;优秀可以常阅读汲取的博客&lt;/p&gt;
    
    </summary>
    
      <category term="杂七杂八" scheme="http://www.aitozi.com/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>flink中window实现的源码分析</title>
    <link href="http://www.aitozi.com/flink-window-source-code-analyse.html"/>
    <id>http://www.aitozi.com/flink-window-source-code-analyse.html</id>
    <published>2018-01-06T18:05:43.000Z</published>
    <updated>2018-01-07T03:17:05.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>关于Flink中window的实现分析</p><a id="more"></a><h2 id="window"><a href="#window" class="headerlink" title="window"></a>window</h2><p>window提供了一种处理无界数据的一种手段</p><h2 id="window的组件"><a href="#window的组件" class="headerlink" title="window的组件"></a>window的组件</h2><p>首先我们看window包含了哪些组件：触发器<code>trigger</code>，触发器上下文<code>triggerContext</code>，内部状态<code>windowState</code>，窗口分配器<code>windowassigner</code>, 内部时间服务器<code>internalTimerService</code>,初看到这么多的组件可能会有点懵，下面的分析会一点一点介绍这些组件的作用。</p><p>今天我们从flink接收流元素进行处理的角度来分析其实现，在flink的DAG中流动的有这么几种元素：<code>StreamRecord</code>,<code>LatencyRecord</code>,<code>WaterMark</code>,<code>StreamStatus</code>,我们这里只考虑这样两种元素<code>StreamRecord</code>和<code>WaterMark</code></p><h2 id="streamRecord"><a href="#streamRecord" class="headerlink" title="streamRecord"></a>streamRecord</h2><p>windowOperator实现了<code>KeyContext</code>,其实就是代表每一个windowOperator处理每一个元素就会在一个Key的上下文的环境中去做处理。</p><p>当windowOperator接收到一条<code>StreamRecord</code>,windowOperator会做什么呢?</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">synchronized</span> (lock) &#123;</div><div class="line">	numRecordsIn.inc();</div><div class="line">	streamOperator.setKeyContextElement1(record);</div><div class="line">	streamOperator.processElement(record);</div><div class="line">&#125;</div></pre></td></tr></table></figure><ol><li>首先设置该operator的key为当前元素</li><li>根据element所携带的时间戳（processing time或者event time）分配元素所应该属于的窗口，一个元素可能会隶属于多个窗口，比如slideWindowAssigner。</li><li>如果这个窗口是一个可merge窗口，例如session窗口，那么就会进行和原有窗口的合并和状态的更新</li></ol><h3 id="窗口merge原理"><a href="#窗口merge原理" class="headerlink" title="窗口merge原理"></a>窗口merge原理</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-1-7/74574325.jpg" alt=""></p><ol><li>首先取出之前的所有未清除的窗口，和新分配到的窗口做一次merge，有重叠部分则新生成大的窗口</li><li>将各个小窗口的真实数据merge到合并后的大窗口</li><li>注册大窗口的清理时间触发器，清理原先子窗口的清理时间触发器</li></ol><p>窗口merge完之后，则会通过<code>triggerContext#OnElement</code>方法去进行判断是否能触发计算，触发方式如前文分析wartermark的那样，就是通过wartermark来判断这个窗口是否可以进行计算</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onElement</span><span class="params">(Object element, <span class="keyword">long</span> timestamp, TimeWindow window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">	<span class="keyword">if</span> (window.maxTimestamp() &lt;= ctx.getCurrentWatermark()) &#123;</div><div class="line">		<span class="comment">// if the watermark is already past the window fire immediately</span></div><div class="line">		<span class="keyword">return</span> TriggerResult.FIRE;</div><div class="line">	&#125; <span class="keyword">else</span> &#123;</div><div class="line">		ctx.registerEventTimeTimer(window.maxTimestamp());</div><div class="line">		<span class="keyword">return</span> TriggerResult.CONTINUE;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>如果不能触发计算，我们看到其实他是将<code>window.maxTimestamp</code>注册到了<code>eventTimeQueue</code>中, 这里我们先记一下，后文会提到他的作用。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>到这里windowOperator接收到一个<code>StreamRecord</code>元素的处理逻辑已经结束了，如果窗口不是可merge类型的除了不做窗口merge，其他的操作也是大同小异，到这里也许你会有几个疑问(其实是我自己看代码的一些疑问：)</p><ol><li>如果我设置了allowLateness会对我们的计算结果产生什么样的影响呢？</li><li>我是用<code>apply()</code>,<code>process()</code>函数或者<code>reduce()</code>这种聚合函数对于window的<code>cost</code>代价有多大的差别？</li></ol><h3 id="问题一"><a href="#问题一" class="headerlink" title="问题一"></a>问题一</h3><p>上文中提到的在处理元素的最后会注册一个窗口cleanupTimer，那么这个时间是多少呢? <code>window.maxTimestamp() + allowedLateness;</code> 所以我们看到一个窗口存在时长是水位线经过<code>window的最大时间+allowLateness</code>的时间，因此当水位线大于窗口最大时间后就会触发计算，而计算之后状态不会清空，会保留<code>allowedLateness</code>的时长，而此时窗口状态还在保留，所以<strong>上游有晚到的数据来一条就会触发一次该窗口的计算</strong>，而且每次计算的数据都是该窗口的全量数据，所以业务方要慎用，或者下游要做好相应的去重或更新措施，否则可能会造成结果的不准确</p><h3 id="问题二"><a href="#问题二" class="headerlink" title="问题二"></a>问题二</h3><p>不同的函数最终影响的其实是我们最终window保存数据的state的形式有<code>ListState</code>，也有<code>reducingState</code>…,最终影响了rocksdb和checkpoint的大小，不过肯定是能用聚合还是用聚合函数比较好</p><h2 id="waterMark"><a href="#waterMark" class="headerlink" title="waterMark"></a>waterMark</h2><p>当元素来的是一个watermark，window Operator又会以怎样的逻辑去处理呢?</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handleWatermark</span><span class="params">(Watermark watermark)</span> </span>&#123;</div><div class="line">	<span class="keyword">try</span> &#123;</div><div class="line">		<span class="keyword">synchronized</span> (lock) &#123;</div><div class="line">			lastEmittedWatermark = watermark.getTimestamp();</div><div class="line">			operator.processWatermark(watermark);</div><div class="line">		&#125;</div><div class="line">	&#125; <span class="keyword">catch</span> (Exception e) &#123;</div><div class="line">		<span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"Exception occurred while processing valve output watermark: "</span>, e);		&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>这里我们将关注到开篇提到的<code>internalTimeService</code>,</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">advanceWatermark</span><span class="params">(<span class="keyword">long</span> time)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">	currentWatermark = time;</div><div class="line"></div><div class="line">	InternalTimer&lt;K, N&gt; timer;</div><div class="line"></div><div class="line">	<span class="keyword">while</span> ((timer = eventTimeTimersQueue.peek()) != <span class="keyword">null</span> &amp;&amp; timer.getTimestamp() &lt;= time) &#123;</div><div class="line"></div><div class="line">		Set&lt;InternalTimer&lt;K, N&gt;&gt; timerSet = getEventTimeTimerSetForTimer(timer);</div><div class="line">		timerSet.remove(timer);</div><div class="line">		eventTimeTimersQueue.remove();</div><div class="line"></div><div class="line">		keyContext.setCurrentKey(timer.getKey());</div><div class="line">		triggerTarget.onEventTime(timer);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="watermark元素处理逻辑"><a href="#watermark元素处理逻辑" class="headerlink" title="watermark元素处理逻辑"></a>watermark元素处理逻辑</h3><p><img src="http://or0igopk2.bkt.clouddn.com/18-1-7/87202276.jpg" alt=""></p><ol><li>当流元素是watermark时主要处理逻辑集中在<code>internaltimerservice</code>上</li><li>如果<code>eventTimeTimersQueue</code>这个优先级队列中最早的时间低于了水位线，那么就会取出同一时刻的所有key的timer进行计算</li><li>处理可能意味着窗口的计算触发或者某些窗口的清理</li></ol><p>以上便是flink对于窗口的实现逻辑。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;关于Flink中window的实现分析&lt;/p&gt;
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>为什么现代系统架构需要一个新的编程模型 actor编程模型</title>
    <link href="http://www.aitozi.com/why-actor-programming-model.html"/>
    <id>http://www.aitozi.com/why-actor-programming-model.html</id>
    <published>2017-12-17T13:32:20.000Z</published>
    <updated>2019-03-14T17:07:29.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>actor编程模型很早就由<code>Carl Hewitt</code>提出了, 最先是设计来解决在高性能的网络环境中的并发处理场景。只是当时没有这样的环境。如今，网络，硬件的基础设施的能力已经远远超越了那个时代，因此，当时提出的actor模型也有了用武之地。这篇文章讨论了关于传统编程的一些假设和现代实际的多线程，多核cpu架构不匹配的地方.</p><a id="more"></a><h1 id="封装所面临的挑战"><a href="#封装所面临的挑战" class="headerlink" title="封装所面临的挑战"></a>封装所面临的挑战</h1><p>OOP思想的支柱是封装，封装意味着对象中的数据是不能从外界直接访问的，他仅能够通过调用内部的方法被修改。因此对象就承担起暴露<strong>正确</strong>，<strong>安全</strong>的方法来保护其封装变量的不变性。举个例子，对一个顺序二叉树来说一定不允许违反一个二叉树的排序不变性。调用者期望二叉树的顺序在查询某段数据的时候能够保持不变，从而他们能够依赖于这种约束。<br>当我们分析OOP应用的运行行为时，我们通常会会画一个消息序列图标来展示方法调用之间的交互，例如：<br><img src="http://or0igopk2.bkt.clouddn.com/17-12-14/20018675.jpg" alt=""><br>不幸的是，上图没有精确的表达出，在执行的过程中这些对象的生命周期。实际上一个线程完成了所有这些调用。对变量不变性的要求是发生在方法调用的线程中：<br><img src="http://or0igopk2.bkt.clouddn.com/17-12-14/3331951.jpg" alt=""><br>这样展示的意义在于你对多线程的情况下建模会把问题变的清晰。当有多个线程的情况下，事情变的不一样。<br><img src="http://or0igopk2.bkt.clouddn.com/17-12-14/31370324.jpg" alt=""><br>我们可以看到中间有一个环节有两个线程进入了统一个方法，但是该对象的封装并没有考虑如何处理这种场景。这两个方法的调用可能是随机交叉的，这就导致要向保证其内部变量的不变一致行必须引入两个线程之间的协调机制,通常的做法是引入锁来解决这个问题，从而确保同一时间只有一个线程在调用该方法.然而：</p><ul><li>锁机制严重限制了并发度，他们在如今cpu的架构下代价很大，需要操作系统暂停线程然后再过段时间再恢复</li><li>调用线程在block之后不能再做其他的工作</li><li>锁还引入了死锁问题。</li></ul><p><strong>导致的结果</strong></p><ul><li>没有锁状态可能会导致冲突</li><li>引入锁，性能会受损，并容易导致死锁,并且锁一般适用于单机的情况，当有多台机器协调工作时，就需要分布式锁，通常比本地锁的性能更差，更影响其可扩展性。</li></ul><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>对象仅仅能够保证在单线程情况下内部状态的一致性</li><li>锁机制在生产实际中显示并不高效</li></ul><h1 id="现代计算机架构内存共享的问题"><a href="#现代计算机架构内存共享的问题" class="headerlink" title="现代计算机架构内存共享的问题"></a>现代计算机架构内存共享的问题</h1><p>在现代计算机架构中，如果我们定义一个变量，cpu是将其写入cpu缓存，而不是将其直接写到内存中，绝大多数是写到临近该cpu核的cache中去，因此不能被别的cpu core所见，为了让一些本地的修改能让其他的核所见，cpu cache需要将其中的数据刷到其他核中。<br>在jvm中我们通过<code>volatile</code>关键字来明确一块内存地址，这块内存会被所有线程共享。那么为什么我们不将所有的变量能标记为<code>volatile</code>呢？因为将cpu cache的数据传递到各个cpu核是代价非常高的事情</p><h2 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h2><ul><li>没有真正的共享内存，cpu将数据传输到别的cpu核上，就像在网络中做的一样，cpu内部通讯和网络的通讯在实现上也有很多的共通之处。</li><li>与其通过标记为共享或使用原子数据结构的变量来隐藏消息传递，更规范和原则性的方法是将状态信息保存到一个并发实体中，并通过消息的机制显式地传播并发实体之间的数据或事件</li></ul><h1 id="调用栈的说明"><a href="#调用栈的说明" class="headerlink" title="调用栈的说明"></a>调用栈的说明</h1><p>我们今天常常把所谓的堆栈视为理所当然，但是他们是在一个并发编程不那么重要的时代发明的，因为多cpu系统并不常见。调用堆栈不交叉线程，因此，不建模异步调用链。<br>当线程打算将任务委托给“后台任务”时，问题就出现了。实际上，这实际上意味着委托给另一个线程。这不能是一个简单的方法/函数调用，因为调用在线程中是严格的本地调用。通常发生的情况是，“调用者”将一个对象放入一个和被调用的工作线程共享的内存位置，这反过来调用者又可以在某个事件循环中获取它。这允许“caller”线程继续执行其他任务。<br>这种模式下第一个问题是，如何通知“调用者”任务已经完成了。当一个任务失败时，会出现一个更严重的问题。异常传播到什么地方?它将传播到工作线程的异常处理程序，而完全忽略实际的“调用者”是谁。就是无法将异常抛到主线程中去。<br><img src="http://or0igopk2.bkt.clouddn.com/17-12-17/52377993.jpg" alt=""><br>这是个严重的问题，工作的线程是如何处理这种情况的?它可能无法解决这个问题，因为它通常不知道失败的任务的目的“调用者”线程需要以某种方式被通知到，但是没有调用堆栈来解决异常。失败通知只能通过一个侧通道来完成，例如，在“调用者”线程希望得到结果的情况下，放置一个错误代码。如果此通知不到位，则“调用者”永远不会收到失败的通知，任务就会丢失!<strong>这与网络系统的工作方式惊人地相似，在没有任何通知的情况下，消息/请求可能会丢失/失败。</strong></p><p>这种情况再某些情况下会变得更糟，当一个由主线程发起的工作线程遇到了一个bug退出了最终会出现不可恢复的情况。一个由bug引起的内部异常会抛到线程的根，并使线程关闭。这立即引发了问题，谁应该重新启动由线程托管的服务的正常运行，以及如何恢复到已知的良好状态?乍一看，这似乎是可以管理的，但是我们突然面临一个新的、意想不到的现象。实际的任务，即线程当前正在进行的工作，不再位于任务从(通常是队列)中提取任务的共享内存位置。事实上，由于异常到达顶部，解除所有的调用堆栈，任务状态完全丢失!我们已经丢失了一条消息（即此时的工作线程已经退出），尽管这是本地通信，没有涉及到网络(消息丢失将被预期)。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>为了在当前系统上实现任何有意义的并发性和性能，线程必须在不阻塞的情况下以有效的方式将任务分配给彼此。有了这种任务委托并发机制(甚至更多的是通过网络/分布式计算)基于调用堆栈的错误处理，需要引入新的显式的错误信号机制。并且失败成为领域模型的一部分。</li><li>代理模式下的并发系统需要处理服务故障，并有方法从它们中恢复，此类服务的客户端需要意识到任务/消息可能在重新启动时丢失。即使没有发生损失，也可能由于先前的排队任务(长队列)、垃圾收集导致的延迟等原因导致响应延迟。在这些情况下，并发系统应该以超时的形式处理响应期限，就像网络/分布式系统一样。</li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>Akka 官网： <a href="https://doc.akka.io/docs/akka/2.5/guide/actors-motivation.html" target="_blank" rel="external">https://doc.akka.io/docs/akka/2.5/guide/actors-motivation.html</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;actor编程模型很早就由&lt;code&gt;Carl Hewitt&lt;/code&gt;提出了, 最先是设计来解决在高性能的网络环境中的并发处理场景。只是当时没有这样的环境。如今，网络，硬件的基础设施的能力已经远远超越了那个时代，因此，当时提出的actor模型也有了用武之地。这篇文章讨论了关于传统编程的一些假设和现代实际的多线程，多核cpu架构不匹配的地方.&lt;/p&gt;
    
    </summary>
    
      <category term="深度好文" scheme="http://www.aitozi.com/categories/%E6%B7%B1%E5%BA%A6%E5%A5%BD%E6%96%87/"/>
    
    
      <category term="akka" scheme="http://www.aitozi.com/tags/akka/"/>
    
  </entry>
  
  <entry>
    <title>ArrayDeque源码分析</title>
    <link href="http://www.aitozi.com/arraydequeue-source-code.html"/>
    <id>http://www.aitozi.com/arraydequeue-source-code.html</id>
    <published>2017-11-24T07:32:23.000Z</published>
    <updated>2019-03-14T17:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>关于ArrayDeque源码分析</p><a id="more"></a><h1 id="Stack-and-Queue"><a href="#Stack-and-Queue" class="headerlink" title="Stack and Queue"></a>Stack and Queue</h1><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Java里有一个叫做<em>Stack</em>的类，却没有叫做<em>Queue</em>的类（它是个接口名字）。当需要使用栈时，Java已不推荐使用<em>Stack</em>，而是推荐使用更高效的<em>ArrayDeque</em>；既然<em>Queue</em>只是一个接口，当需要使用队列时也就首选<em>ArrayDeque</em>了（次选是<em>LinkedList</em>）。</p><ul><li>stack 先进后出</li><li>queue 先进先出</li></ul><h1 id="总体介绍"><a href="#总体介绍" class="headerlink" title="总体介绍"></a>总体介绍</h1><p>要讲栈和队列，首先要讲<em>Deque</em>接口。<em>Deque</em>的含义是“double ended queue”，即双端队列，它既可以当作栈使用，也可以当作队列使用。下表列出了<em>Deque</em>与<em>Queue</em>相对应的接口：</p><table><thead><tr><th>Queue Method</th><th>Equivalent Deque Method</th><th>说明</th></tr></thead><tbody><tr><td><code>add(e)</code></td><td><code>addLast(e)</code></td><td>向队尾插入元素，失败则抛出异常</td></tr><tr><td><code>offer(e)</code></td><td><code>offerLast(e)</code></td><td>向队尾插入元素，失败则返回<code>false</code></td></tr><tr><td><code>remove()</code></td><td><code>removeFirst()</code></td><td>获取并删除队首元素，失败则抛出异常</td></tr><tr><td><code>poll()</code></td><td><code>pollFirst()</code></td><td>获取并删除队首元素，失败则返回<code>null</code></td></tr><tr><td><code>element()</code></td><td><code>getFirst()</code></td><td>获取但不删除队首元素，失败则抛出异常</td></tr><tr><td><code>peek()</code></td><td><code>peekFirst()</code></td><td>获取但不删除队首元素，失败则返回<code>null</code></td></tr></tbody></table><p>下表列出了<em>Deque</em>与<em>Stack</em>对应的接口：</p><table><thead><tr><th>Stack Method</th><th>Equivalent Deque Method</th><th>说明</th></tr></thead><tbody><tr><td><code>push(e)</code></td><td><code>addFirst(e)</code></td><td>向栈顶插入元素，失败则抛出异常</td></tr><tr><td>无</td><td><code>offerFirst(e)</code></td><td>向栈顶插入元素，失败则返回<code>false</code></td></tr><tr><td><code>pop()</code></td><td><code>removeFirst()</code></td><td>获取并删除栈顶元素，失败则抛出异常</td></tr><tr><td>无</td><td><code>pollFirst()</code></td><td>获取并删除栈顶元素，失败则返回<code>null</code></td></tr><tr><td><code>peek()</code></td><td><code>peekFirst()</code></td><td>获取但不删除栈顶元素，失败则抛出异常</td></tr><tr><td>无</td><td><code>peekFirst()</code></td><td>获取但不删除栈顶元素，失败则返回<code>null</code></td></tr></tbody></table><p>上面两个表共定义了<em>Deque</em>的12个接口。添加，删除，取值都有两套接口，它们功能相同，区别是对失败情况的处理不同。<strong>一套接口遇到失败就会抛出异常，另一套遇到失败会返回特殊值（<code>false</code>或<code>null</code>）</strong>。除非某种实现对容量有限制，大多数情况下，添加操作是不会失败的。<strong>虽然<em>Deque</em>的接口有12个之多，但无非就是对容器的两端进行操作，或添加，或删除，或查看</strong>。明白了这一点讲解起来就会非常简单。</p><p><em>ArrayDeque</em>和<em>LinkedList</em>是<em>Deque</em>的两个通用实现，由于官方更推荐使用<em>AarryDeque</em>用作栈和队列，因为他更高效。<em>ArrayDeque的高效来源于head和tail这两个变量，它们使得物理上简单的从头到尾的数组变为了一个逻辑上循环的数组，避免了在头尾操作时的移</em></p><p>从名字可以看出<em>ArrayDeque</em>底层通过数组实现，为了满足可以同时在数组两端插入或删除元素的需求，该数组还必须是循环的，即<strong>循环数组（circular array）</strong>，也就是说数组的任何一点都可能被看作起点或者终点。<em>ArrayDeque</em>是非线程安全的（not thread-safe），当多个线程同时使用的时候，需要程序员手动同步；另外，该容器不允许放入<code>null</code>元素。</p><p><img src="http://or0igopk2.bkt.clouddn.com/17-11-24/2573607.jpg" alt=""></p><p>上图中我们看到，<strong><code>head</code>指向首端第一个有效元素，<code>tail</code>指向尾端第一个可以插入元素的空位</strong>。因为是循环数组，所以<code>head</code>不一定总等于0，<code>tail</code>也不一定总是比<code>head</code>大。</p><h1 id="方法剖析"><a href="#方法剖析" class="headerlink" title="方法剖析"></a>方法剖析</h1><h2 id="两个重要索引"><a href="#两个重要索引" class="headerlink" title="两个重要索引"></a>两个重要索引</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 第一个元素的索引  </span></div><div class="line"><span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">int</span> head;  </div><div class="line"><span class="comment">// 下个要添加元素的位置，为末尾元素的索引 + 1  </span></div><div class="line"><span class="keyword">private</span> <span class="keyword">transient</span> <span class="keyword">int</span> tail;</div></pre></td></tr></table></figure><ul><li>如果head和tail相同，则数组为空，长度为0。</li><li>如果tail大于head，则第一个元素为elements[head]，最后一个为elements[tail-1]，长度为tail-head，元素索引从head到tail-1。</li><li>如果tail小于head，且为0，则第一个元素为elements[head]，最后一个为elements[elements.length-1]，元素索引从head到elements.length-1。</li><li>如果tail小于head，且大于0，则会形成循环，第一个元素为elements[head]，最后一个是elements[tail-1]，元素索引从head到elements.length-1，然后再从0到tail-1</li></ul><h2 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">ArrayDeque</span><span class="params">()</span> </span>&#123;  </div><div class="line">    elements = (E[]) <span class="keyword">new</span> Object[<span class="number">16</span>]; <span class="comment">// 默认的数组长度大小  </span></div><div class="line">&#125;  </div><div class="line">  </div><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">ArrayDeque</span><span class="params">(<span class="keyword">int</span> numElements)</span> </span>&#123;  </div><div class="line">    allocateElements(numElements); <span class="comment">// 需要的数组长度大小  </span></div><div class="line">&#125;  </div><div class="line">  </div><div class="line"><span class="function"><span class="keyword">public</span> <span class="title">ArrayDeque</span><span class="params">(Collection&lt;? extends E&gt; c)</span> </span>&#123;  </div><div class="line">    allocateElements(c.size()); <span class="comment">// 根据集合来分配数组大小  </span></div><div class="line">    addAll(c); <span class="comment">// 把集合中元素放到数组中  </span></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>这里看出要么是创建默认的16大小的数组，要么是<code>allocateElements(size)</code>去进行动态调整。</p><h2 id="分配的逻辑"><a href="#分配的逻辑" class="headerlink" title="分配的逻辑"></a>分配的逻辑</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">allocateElements</span><span class="params">(<span class="keyword">int</span> numElements)</span> </span>&#123;  </div><div class="line">    <span class="keyword">int</span> initialCapacity = MIN_INITIAL_CAPACITY;  </div><div class="line">    <span class="comment">// 找到大于需要长度的最小的2的幂整数。  </span></div><div class="line">    <span class="comment">// Tests "&lt;=" because arrays aren't kept full.  </span></div><div class="line">    <span class="keyword">if</span> (numElements &gt;= initialCapacity) &#123;  </div><div class="line">        initialCapacity = numElements;  </div><div class="line">        initialCapacity |= (initialCapacity &gt;&gt;&gt;  <span class="number">1</span>);  </div><div class="line">        initialCapacity |= (initialCapacity &gt;&gt;&gt;  <span class="number">2</span>);  </div><div class="line">        initialCapacity |= (initialCapacity &gt;&gt;&gt;  <span class="number">4</span>);  </div><div class="line">        initialCapacity |= (initialCapacity &gt;&gt;&gt;  <span class="number">8</span>);  </div><div class="line">        initialCapacity |= (initialCapacity &gt;&gt;&gt; <span class="number">16</span>);  </div><div class="line">        initialCapacity++;  </div><div class="line">  </div><div class="line">        <span class="keyword">if</span> (initialCapacity &lt; <span class="number">0</span>)   <span class="comment">// Too many elements, must back off  </span></div><div class="line">            initialCapacity &gt;&gt;&gt;= <span class="number">1</span>;<span class="comment">// Good luck allocating 2 ^ 30 elements  </span></div><div class="line">    &#125;  </div><div class="line">    elements = (E[]) <span class="keyword">new</span> Object[initialCapacity];  </div><div class="line">&#125;</div></pre></td></tr></table></figure><p>这段代码的晦涩之处在于：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">initialCapacity |= (initialCapacity &gt;&gt;&gt;  <span class="number">1</span>);</div><div class="line">initialCapacity |= (initialCapacity &gt;&gt;&gt;  <span class="number">2</span>);</div><div class="line">initialCapacity |= (initialCapacity &gt;&gt;&gt;  <span class="number">4</span>);</div><div class="line">initialCapacity |= (initialCapacity &gt;&gt;&gt;  <span class="number">8</span>);</div><div class="line">initialCapacity |= (initialCapacity &gt;&gt;&gt; <span class="number">16</span>);</div><div class="line"></div><div class="line">initialCapacity++;</div></pre></td></tr></table></figure><ul><li>如果numElements小于MIN_INITIAL_CAPACITY，则分配的数组长度就是MIN_INITIAL_CAPACITY，它是一个静态常量，值为8。</li><li>在numElements大于等于8的情况下，分配的实际长度是严格大于numElements并且为2的整数次幂的最小数。比如，如果numElements为10，则实际分配16，如果numElements为32，则为64。</li></ul><p>为什么要为2的幂次数呢？我们待会会看到，这样会使得很多操作的效率很高。</p><p>为什么要严格大于numElements呢？因为循环数组必须时刻至少留一个空位，tail变量指向下一个空位，为了容纳numElements个元素，至少需要numElements+1个位置</p><p>这究竟在干什么？其实，它是在将initialCapacity左边最高位的1复制到右边的每一位，这种复制类似于病毒复制，是1传2、2传4、4传8式的指数级复制，最后再执行initialCapacity++就可以得到比initialCapacity大且为2的幂次方的最小的数。 这个操作可以在<code>Hackers Delight</code>这本书中找到原型：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">highestOneBit</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</div><div class="line">    <span class="comment">// HD, Figure 3-1</span></div><div class="line">    i |= (i &gt;&gt;  <span class="number">1</span>);</div><div class="line">    i |= (i &gt;&gt;  <span class="number">2</span>);</div><div class="line">    i |= (i &gt;&gt;  <span class="number">4</span>);</div><div class="line">    i |= (i &gt;&gt;  <span class="number">8</span>);</div><div class="line">    i |= (i &gt;&gt; <span class="number">16</span>);</div><div class="line">    <span class="keyword">return</span> i - (i &gt;&gt;&gt; <span class="number">1</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="addFirst"><a href="#addFirst" class="headerlink" title="addFirst()"></a>addFirst()</h2><p><code>addFirst(E e)</code>的作用是在<em>Deque</em>的首端插入元素，也就是在<code>head</code>的前面插入元素，在空间足够且下标没有越界的情况下，只需要将<code>elements[--head] = e</code>即可。</p><p><img src="http://or0igopk2.bkt.clouddn.com/17-11-24/76601566.jpg" alt=""></p><p>实际需要考虑：</p><ol><li>空间是否够用</li><li>下标是否越界的问题。上图中，如果<code>head</code>为<code>0</code>之后接着调用<code>addFirst()</code>，虽然空余空间还够用，但<code>head</code>为<code>-1</code>，下标越界了。下列代码很好的解决了这两个问题。</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//addFirst(E e)</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addFirst</span><span class="params">(E e)</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (e == <span class="keyword">null</span>)<span class="comment">//不允许放入null</span></div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> NullPointerException();</div><div class="line">    elements[head = (head - <span class="number">1</span>) &amp; (elements.length - <span class="number">1</span>)] = e;<span class="comment">//2.下标是否越界</span></div><div class="line">    <span class="keyword">if</span> (head == tail)<span class="comment">//1.空间是否够用</span></div><div class="line">        doubleCapacity();<span class="comment">//扩容</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><p>上述代码我们看到，<strong>空间问题是在插入之后解决的</strong>，因为<code>tail</code>总是指向下一个可插入的空位，也就意味着<code>elements</code>数组至少有一个空位，所以插入元素的时候不用考虑空间问题。</p><p>下标越界的处理解决起来非常简单，<code>head = (head - 1) &amp; (elements.length - 1)</code>就可以了，<strong>这段代码相当于取余，同时解决了<code>head</code>为负值的情况</strong>。由上述分配空间的代码可以看出<code>elements.length</code>必是<code>2</code>的指数倍，<code>elements.length - 1</code>就是二进制低位全<code>1</code>，无论是正数还是负数，跟<code>head - 1</code>相与之后就起到了取模的作用，如果<code>head - 1</code>为负数（其实只可能是-1），则相当于对其取相对于<code>elements.length</code>的补码。</p><p>比如说，如果elements.length为8，则(elements.length - 1)为7，二进制为0111，对于负数-1，与7相与，结果为7，对于正数8，与7相与，结果为0，都能达到循环数组中找下一个正确位置的目的。</p><p>下面再说说扩容函数<code>doubleCapacity()</code>，其逻辑是申请一个更大的数组（原数组的两倍），然后将原数组复制过去。过程如下图所示：</p><p><img src="http://or0igopk2.bkt.clouddn.com/17-11-24/65385427.jpg" alt=""></p><p>图中我们看到，复制分两次进行，第一次复制<code>head</code>右边的元素，第二次复制<code>head</code>左边的元素。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//doubleCapacity()</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doubleCapacity</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">assert</span> head == tail;</div><div class="line">    <span class="keyword">int</span> p = head;</div><div class="line">    <span class="keyword">int</span> n = elements.length;</div><div class="line">    <span class="keyword">int</span> r = n - p; <span class="comment">// head右边元素的个数</span></div><div class="line">    <span class="keyword">int</span> newCapacity = n &lt;&lt; <span class="number">1</span>;<span class="comment">//原空间的2倍</span></div><div class="line">    <span class="keyword">if</span> (newCapacity &lt; <span class="number">0</span>)</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(<span class="string">"Sorry, deque too big"</span>);</div><div class="line">    Object[] a = <span class="keyword">new</span> Object[newCapacity];</div><div class="line">    System.arraycopy(elements, p, a, <span class="number">0</span>, r);<span class="comment">//复制右半部分，对应上图中绿色部分</span></div><div class="line">    System.arraycopy(elements, <span class="number">0</span>, a, r, p);<span class="comment">//复制左半部分，对应上图中灰色部分</span></div><div class="line">    elements = (E[])a;</div><div class="line">    head = <span class="number">0</span>;</div><div class="line">    tail = n;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="pollFirst"><a href="#pollFirst" class="headerlink" title="pollFirst()"></a>pollFirst()</h2><p><code>pollFirst()</code>的作用是删除并返回<em>Deque</em>首端元素，也即是<code>head</code>位置处的元素。如果容器不空，只需要直接返回<code>elements[head]</code>即可，当然还需要处理下标的问题。由于<code>ArrayDeque</code>中不允许放入<code>null</code>，当<code>elements[head] == null</code>时，意味着容器为空。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> E <span class="title">pollFirst</span><span class="params">()</span> </span>&#123;</div><div class="line">    E result = elements[head];</div><div class="line">    <span class="keyword">if</span> (result == <span class="keyword">null</span>)<span class="comment">//null值意味着deque为空</span></div><div class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">    elements[h] = <span class="keyword">null</span>;<span class="comment">//let GC work</span></div><div class="line">    head = (head + <span class="number">1</span>) &amp; (elements.length - <span class="number">1</span>);<span class="comment">//下标越界处理</span></div><div class="line">    <span class="keyword">return</span> result;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="pollLast"><a href="#pollLast" class="headerlink" title="pollLast()"></a>pollLast()</h2><p><code>pollLast()</code>的作用是删除并返回<em>Deque</em>尾端元素，也即是<code>tail</code>位置前面的那个元素。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> E <span class="title">pollLast</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> t = (tail - <span class="number">1</span>) &amp; (elements.length - <span class="number">1</span>);<span class="comment">//tail的上一个位置是最后一个元素</span></div><div class="line">    E result = elements[t];</div><div class="line">    <span class="keyword">if</span> (result == <span class="keyword">null</span>)<span class="comment">//null值意味着deque为空</span></div><div class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">    elements[t] = <span class="keyword">null</span>;<span class="comment">//let GC work</span></div><div class="line">    tail = t;</div><div class="line">    <span class="keyword">return</span> result;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="peekFirst"><a href="#peekFirst" class="headerlink" title="peekFirst()"></a>peekFirst()</h2><p><code>peekFirst()</code>的作用是返回但不删除<em>Deque</em>首端元素，也即是<code>head</code>位置处的元素，直接返回<code>elements[head]</code>即可。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> E <span class="title">peekFirst</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> elements[head]; <span class="comment">// elements[head] is null if deque empty</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="peekLast"><a href="#peekLast" class="headerlink" title="peekLast()"></a>peekLast()</h2><p><code>peekLast()</code>的作用是返回但不删除<em>Deque</em>尾端元素，也即是<code>tail</code>位置前面的那个元素。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> E <span class="title">peekLast</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> elements[(tail - <span class="number">1</span>) &amp; (elements.length - <span class="number">1</span>)];</div><div class="line">&#125;</div></pre></td></tr></table></figure><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://github.com/CarpenterLee/JCFInternals/tree/master/markdown" target="_blank" rel="external">https://github.com/CarpenterLee/JCFInternals/tree/master/markdown</a></p><p><a href="http://czj4451.iteye.com/blog/1688693" target="_blank" rel="external">http://czj4451.iteye.com/blog/1688693</a></p><p><a href="https://www.cnblogs.com/swiftma/p/6029547.html" target="_blank" rel="external">https://www.cnblogs.com/swiftma/p/6029547.html</a></p><p><a href="https://stackoverflow.com/questions/28314798/addfirst-method-of-arraydeque-class" target="_blank" rel="external">https://stackoverflow.com/questions/28314798/addfirst-method-of-arraydeque-class</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;关于ArrayDeque源码分析&lt;/p&gt;
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="jdk" scheme="http://www.aitozi.com/tags/jdk/"/>
    
  </entry>
  
  <entry>
    <title>flink-watermark</title>
    <link href="http://www.aitozi.com/flink-watermark.html"/>
    <id>http://www.aitozi.com/flink-watermark.html</id>
    <published>2017-09-10T07:59:23.000Z</published>
    <updated>2019-03-14T17:05:18.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>关于Flink中watermark工作原理代码分析</p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>waterMark，latency，checkpoint这三者实现方式都是上游节点逐步广播消息给下游节点来处理的行为（都是在流中插入一种特殊的数据结构来做处理）</p><h3 id="时间语义"><a href="#时间语义" class="headerlink" title="时间语义"></a>时间语义</h3><p>谈及watermark就要先从Flink支持的时间语义说起，Flink支持三种时间语义：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">process time:指的系统处理对应数据时的系统时间。他是最简单的一种实现，由于不需要额外的协调，因性能最好</div><div class="line">event time:是指数据中携带的时间，而不是数据到达的时间。因此时间的进度完全取决于数据，而不是系统时间。使用event time必须指定生成eventTime和watermark的方式。因为他一般会等待迟到的数据，因此一定会有一定的延时</div><div class="line">ingestion time:是指数据进入flink的时间，在source处插入的时间，和process time一样无法处乱序事件</div></pre></td></tr></table></figure><p>对于eventtime和ingestion time两种语义到达的数据有可能乱序的。从事件产生（例如日志采集数据中的乱序日志），到流经source，再到operator，中间是有一个过程时间的。虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生（out-of-order或者说late element）。<br>但是对于late element，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发window去进行计算了。这个特别的机制，就是watermark，它告诉了算子时间不大于 WaterMark 的消息不应该再被接收【如果出现意味着延迟到达】。也就是说水位线以下的数据均已经到达。WaterMark 从源算子开始 emit，并逐级向下游算子传递。当源算子关闭时，会发射一个携带 Long.MAX_VALUE 值时间戳的 WaterMark，下游算子接收到之后便知道不会再有消息到达。</p><h3 id="waterMark产生方式"><a href="#waterMark产生方式" class="headerlink" title="waterMark产生方式"></a>waterMark产生方式</h3><p>waterMark的产生:有两种方式来产生watermark和timestamp</p><ol><li>在数据源处直接进行assign timestamp 和generate watermark</li><li>通过timestamp和watermark generate operator来产生，如果使用了timestamp assigner和watermark generator在source处产生的timestamp和watermark会被覆盖。</li><li>其实这两种最终的实现方式还是一样的！即第一种也是在source处分配了timestampAssigner</li></ol><h4 id="方法一-在数据源处产生发送"><a href="#方法一-在数据源处产生发送" class="headerlink" title="方法一 在数据源处产生发送"></a>方法一 在数据源处产生发送</h4><p>目前实现了在source提取时间和产生operator的只有0.10版本的kafka fetcher中实现<br>首先根据应用<code>StreamExecutionEnvironment#setStreamTimeCharacteristic</code>设置的时间语义（存储于<code>StreamConfig</code>类）来获取<br>对应的sourceContext，sourceContext接口是sourceFunction发送数据的抽象，有三个实现类，根据时间语义划分</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">switch</span> (timeCharacteristic) &#123;</div><div class="line">            <span class="keyword">case</span> EventTime:</div><div class="line">                ctx = <span class="keyword">new</span> ManualWatermarkContext&lt;&gt;(checkpointLock, output);</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            <span class="keyword">case</span> IngestionTime:</div><div class="line">                ctx = <span class="keyword">new</span> AutomaticWatermarkContext&lt;&gt;(processingTimeService, checkpointLock, output, watermarkInterval);</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            <span class="keyword">case</span> ProcessingTime:</div><div class="line">                ctx = <span class="keyword">new</span> NonTimestampContext&lt;&gt;(checkpointLock, output);</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            <span class="keyword">default</span>:</div><div class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(String.valueOf(timeCharacteristic));</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> ctx;</div></pre></td></tr></table></figure><p>这里我们就考虑eventTime的情况。<br>在kafka AbstractFetcher中还提供了三种模式来控制自己生产时间戳和watermark</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">NO_TIMESTAMPS_WATERMARKS: fetcher 不生产 timestamp 和 watermarks</div><div class="line">PERIODIC_WATERMARKS: fetcher 阶段性定时生产 watermarks</div><div class="line">PUNCTUATED_WATERMARKS: fetcher 生产标记 watermark 【按照特定的消息字段值触发】</div></pre></td></tr></table></figure><p>这里的区分方式来自于<code>FlinkKafkaConsumerBase#assignTimestampsAndWatermarks</code>分配的<code>AssignerWithPunctuatedWatermarks</code>或<code>AssignerWithPeriodicWatermarks</code> 也就是assigner决定了产生方式，这其实就是把方法二给封装到了KafkaConsumerBase里面！当然这个也需要用户在创建consumer之后自定义一个assigner。</p><h4 id="方法二-使用TimestampAssigner来实现"><a href="#方法二-使用TimestampAssigner来实现" class="headerlink" title="方法二 使用TimestampAssigner来实现"></a>方法二 使用TimestampAssigner来实现</h4><p>TimestampAssinger是接收一个流，并产生一个新的流带上了时间戳和watermark，如果原来的流已经带有了timestamp和watermark那么这个将会被覆盖。timestamp assinger只需要在关于时间的操作之前加上即可。</p><p>flink通过接口<code>TimestampAssigner</code>来让用户依据消息的格式自己抽取可能被用于 WaterMark的timestamp，它只定义了一个接口：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(T element, <span class="keyword">long</span> previousElementTimestamp)</span></span>;</div></pre></td></tr></table></figure><p>previousElementTimestamp这个参数传入的是什么呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">final</span> <span class="keyword">long</span> newTimestamp = userFunction.extractTimestamp(value, </div><div class="line">				element.hasTimestamp() ? element.getTimestamp() : Long.MIN_VALUE);</div><div class="line">因此如果一个元素如果已经带有timestamp，比如在source处已经分配，那么在这里处理的时候会被覆盖掉。</div></pre></td></tr></table></figure><p>而<code>TimestampAssigner</code>的两个继承接口<code>AssignerWithPunctuatedWatermarks</code>以及 <code>AssignerWithPeriodicWatermarks</code> 定义了waterMark生成的两种典型方式。</p><h5 id="AssignerWithPeriodicWatermarks"><a href="#AssignerWithPeriodicWatermarks" class="headerlink" title="AssignerWithPeriodicWatermarks"></a>AssignerWithPeriodicWatermarks</h5><p><code>AssignerWithPeriodicWatermarks</code>是周期性的产生watermark，每过一定间隔 <code>ExecutionConfig#getAutoWatermarkInterval()</code>，系统会调用<code>getCurrentWatermark</code>来获取最新的waterMark值，如果新的waterMark值有增长那么就会发送一个新的waterMark，如果没有新的元素进来，那么getCurrentWatermark则不会被周期性的调用，这个接口的好处是，可以定义最大乱序时间，减少因为数据延迟到达而被时间窗口丢弃的行为，实现类<code>BoundedOutOfOrdernessTimestampExtractor</code></p><h5 id="AssignerWithPunctuatedWatermarks"><a href="#AssignerWithPunctuatedWatermarks" class="headerlink" title="AssignerWithPunctuatedWatermarks"></a>AssignerWithPunctuatedWatermarks</h5><p><code>AssignerWithPunctuatedWatermarks</code>的使用场景是针对在接收到特定的elements之后才触发更新waterMark的操作。比如有一个流中有一些元素带有flag表示没有晚于这个元素时间的元素了,那么他的实现代码是这样的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WatermarkOnFlagAssigner</span> <span class="keyword">implements</span> <span class="title">AssignerWithPunctuatedWatermarks</span>&lt;<span class="title">MyElement</span>&gt; </span>&#123;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">extractTimestamp</span><span class="params">(MyElement element, <span class="keyword">long</span> previousElementTimestamp)</span> </span>&#123;</div><div class="line">         <span class="keyword">return</span> element.getSequenceTimestamp();</div><div class="line">     &#125;</div><div class="line"></div><div class="line">     <span class="function"><span class="keyword">public</span> Watermark <span class="title">checkAndGetNextWatermark</span><span class="params">(MyElement lastElement, <span class="keyword">long</span> extractedTimestamp)</span> </span>&#123;</div><div class="line">         <span class="keyword">return</span> lastElement.isEndOfSequence() ? <span class="keyword">new</span> Watermark(extractedTimestamp) : <span class="keyword">null</span>;</div><div class="line">     &#125;</div><div class="line"> &#125;</div></pre></td></tr></table></figure><h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><p>我们平常用的最多的还是<code>AssignerWithPeriodicWatermarks</code> 并设置数据到达最大超时时长。下面的分析我们就以<code>AssignerWithPeriodicWatermarks</code>为例。<br>想到一个点<code>AssignerWithPunctuatedWatermarks</code>的一个使用场景:可能比较适合于数据不是连续发送或者说是批任务的场景，比如说是每天某时候数据有更新之后才有计算，那么只要在进入的最后一个数据打入endFlag，然后进行waterMark更新触发数据处理（一个想法，尚未实践）。</p><h3 id="waterMark在传输及对window-Operator的作用方式"><a href="#waterMark在传输及对window-Operator的作用方式" class="headerlink" title="waterMark在传输及对window Operator的作用方式"></a>waterMark在传输及对window Operator的作用方式</h3><h4 id="TimestampsAndPeriodicWatermarksOperator"><a href="#TimestampsAndPeriodicWatermarksOperator" class="headerlink" title="TimestampsAndPeriodicWatermarksOperator"></a>TimestampsAndPeriodicWatermarksOperator</h4><p>在DataStream调用<code>assignTimestampsAndWatermarks</code>产生了一个<code>TimestampsAndPeriodicWatermarksOperator</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">	</div><div class="line"><span class="function"><span class="keyword">public</span> SingleOutputStreamOperator&lt;T&gt; <span class="title">assignTimestampsAndWatermarks</span><span class="params">(</span></span></div><div class="line">	AssignerWithPeriodicWatermarks&lt;T&gt; timestampAndWatermarkAssigner) &#123;</div><div class="line">		</div><div class="line">	<span class="comment">// match parallelism to input, otherwise dop=1 sources could lead to some strange</span></div><div class="line">	<span class="comment">// behaviour: the watermark will creep along very slowly because the elements</span></div><div class="line">	<span class="comment">// from the source go to each extraction operator round robin.</span></div><div class="line">	<span class="comment">//这里就是说一般会默认将并发度设成和inputOperator的并发度一致，避免因为elements进入extraction operator的时候要随机进入分区。</span></div><div class="line">	<span class="keyword">final</span> <span class="keyword">int</span> inputParallelism = getTransformation().getParallelism();</div><div class="line">	<span class="keyword">final</span> AssignerWithPeriodicWatermarks&lt;T&gt; cleanedAssigner = clean(timestampAndWatermarkAssigner);</div><div class="line">		</div><div class="line">	TimestampsAndPeriodicWatermarksOperator&lt;T&gt; operator = </div><div class="line">			<span class="keyword">new</span> TimestampsAndPeriodicWatermarksOperator&lt;&gt;(cleanedAssigner);</div><div class="line">		</div><div class="line">	<span class="keyword">return</span> transform(<span class="string">"Timestamps/Watermarks"</span>, getTransformation().getOutputType(), operator)</div><div class="line">			.setParallelism(inputParallelism);</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>我们看这个<code>TimestampsAndPeriodicWatermarksOperator</code></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">	<span class="keyword">super</span>.open();</div><div class="line"></div><div class="line">	currentWatermark = Long.MIN_VALUE;</div><div class="line">	watermarkInterval = getExecutionConfig().getAutoWatermarkInterval();</div><div class="line">	</div><div class="line">	<span class="keyword">if</span> (watermarkInterval &gt; <span class="number">0</span>) &#123;</div><div class="line">		<span class="keyword">long</span> now = getProcessingTimeService().getCurrentProcessingTime();</div><div class="line">		getProcessingTimeService().registerTimer(now + watermarkInterval, <span class="keyword">this</span>);</div><div class="line">	&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">	<span class="meta">@Override</span></div><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">	<span class="comment">// register next timer</span></div><div class="line">	Watermark newWatermark = userFunction.getCurrentWatermark();</div><div class="line">	<span class="keyword">if</span> (newWatermark != <span class="keyword">null</span> &amp;&amp; newWatermark.getTimestamp() &gt; currentWatermark) &#123;</div><div class="line">		currentWatermark = newWatermark.getTimestamp();</div><div class="line">		<span class="comment">// emit watermark</span></div><div class="line">		output.emitWatermark(newWatermark);</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	<span class="keyword">long</span> now = getProcessingTimeService().getCurrentProcessingTime();</div><div class="line">	getProcessingTimeService().registerTimer(now + watermarkInterval, <span class="keyword">this</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>通过这两个方法，会从TimestampsAndPeriodicWatermarksOperator定期<code>watermarkInterval</code>发送<code>userFunction.getCurrentWatermark()</code>用户定义的waterMark,当然也是要waterMark有上涨的情况下才会发送</p><p>而且这里有个有意思的地方,这里也定义了<code>processWatermark</code>方法，该方法主要调用时机主要，是在<code>StreamInputProcessor#processInput</code>中,这个类在后面再具体分析</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processWatermark</span><span class="params">(Watermark mark)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">	<span class="comment">// if we receive a Long.MAX_VALUE watermark we forward it since it is used</span></div><div class="line">	<span class="comment">// to signal the end of input and to not block watermark progress downstream</span></div><div class="line">	<span class="keyword">if</span> (mark.getTimestamp() == Long.MAX_VALUE &amp;&amp; currentWatermark != Long.MAX_VALUE) &#123;</div><div class="line">		currentWatermark = Long.MAX_VALUE;</div><div class="line">		output.emitWatermark(mark);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>但是这里覆盖了<code>AbstractStreamOperator</code>中的写法是为什么呢?</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processWatermark</span><span class="params">(Watermark mark)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">	<span class="keyword">for</span> (HeapInternalTimerService&lt;?, ?&gt; service : timerServices.values()) &#123;</div><div class="line">		service.advanceWatermark(mark.getTimestamp());</div><div class="line">	&#125;</div><div class="line">	output.emitWatermark(mark);</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>这里其实体现了在最后一个<code>TimestampsAndPeriodicWatermarksOperator</code>之前定义的waterMark传递此operator的时候除非是一个流结束标志<code>Long.MAX_VALUE</code>,否则不会发送，只会有该operator定时发送waterMark给下游处理，这也就说明了在source处定义的waterMark会被后面定义的给覆盖。</p><p>所以大部分Operator处理waterMark的方式是<code>AbstractStreamOperator</code>中定义好的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">processWatermark</span><span class="params">(Watermark mark)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">	<span class="keyword">for</span> (HeapInternalTimerService&lt;?, ?&gt; service : timerServices.values()) &#123;</div><div class="line">		service.advanceWatermark(mark.getTimestamp());<span class="comment">//这个处理逻辑还要在看下</span></div><div class="line">	&#125;</div><div class="line">	output.emitWatermark(mark);</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>这里看到他是会给所有的下游channel发送一个watermark</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">emitWatermark</span><span class="params">(Watermark mark)</span> </span>&#123;</div><div class="line">	<span class="keyword">for</span> (Output&lt;StreamRecord&lt;OUT&gt;&gt; out : allOutputs) &#123;</div><div class="line">		out.emitWatermark(mark);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="StreamInputProcessor"><a href="#StreamInputProcessor" class="headerlink" title="StreamInputProcessor"></a>StreamInputProcessor</h4><p>processWatermark是在<code>StreamInputprocessor</code>中调用的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (recordOrMark.isWatermark()) &#123;</div><div class="line">		<span class="keyword">long</span> watermarkMillis = recordOrMark.asWatermark().getTimestamp();</div><div class="line">		<span class="keyword">if</span> (watermarkMillis &gt; watermarks[currentChannel]) &#123;</div><div class="line">			watermarks[currentChannel] = watermarkMillis;</div><div class="line">			<span class="keyword">long</span> newMinWatermark = Long.MAX_VALUE;</div><div class="line">			<span class="keyword">for</span> (<span class="keyword">long</span> watermark: watermarks) &#123;</div><div class="line">				newMinWatermark = Math.min(watermark, newMinWatermark);</div><div class="line">			&#125;</div><div class="line">			<span class="keyword">if</span> (newMinWatermark &gt; lastEmittedWatermark) &#123;</div><div class="line">				lastEmittedWatermark = newMinWatermark;</div><div class="line">				<span class="keyword">synchronized</span> (lock) &#123;</div><div class="line">					streamOperator.processWatermark(<span class="keyword">new</span> Watermark(lastEmittedWatermark));</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">continue</span>;</div><div class="line">	&#125;</div></pre></td></tr></table></figure><p>逻辑总结:（channel的理解还不够）</p><ol><li>如果消费到的消息是一个 WaterMark，获得其对应的 source channel id 并将时间更新进去，同时记录下当前所有 channel 的最小 WaterMark 时间</li><li>如果当前最小 WaterMark 时间【所有的 channel 都至少消费到该时间】大于上次发射给下游的 WaterMark 时间，则更新 WaterMark 时间并将其交给算子处理</li><li>通常算子在处理【尤其是涉及了窗口计算或者需要时间缓存策略的算子】后会将 WaterMark 继续往下游广播发送</li></ol><h3 id="waterMark对window作用形式"><a href="#waterMark对window作用形式" class="headerlink" title="waterMark对window作用形式"></a>waterMark对window作用形式</h3><p>waterMark如何触发窗口计算</p><h4 id="情况一-late-element"><a href="#情况一-late-element" class="headerlink" title="情况一: late element"></a>情况一: late element</h4><ul><li>Event Time &lt; watermark时间（对于late element太多的数据而言）,这种情况下只要来一条数据就会触发窗口计算，其他属于该窗口的数据到达后都会被丢弃。</li></ul><h4 id="情况二：乱序"><a href="#情况二：乱序" class="headerlink" title="情况二：乱序"></a>情况二：乱序</h4><ul><li>watermark时间 &gt;= window_end_time（对于out-of-order以及正常的数据而言）</li><li>在[window_start_time,window_end_time)中有数据存在</li></ul><p>window的触发机制，是先按照自然时间将window划分，如果window大小是3秒，那么1分钟内会把window划分为如下的形式:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[00:00:00,00:00:03)</div><div class="line">[00:00:03,00:00:06)</div><div class="line">...</div><div class="line">[00:00:57,00:01:00)</div></pre></td></tr></table></figure><p>如果window大小是10秒，则window会被分为如下的形式：当然还有一个offset值可以控制window的起始值不是整点。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[00:00:00,00:00:10)</div><div class="line">[00:00:10,00:00:20)</div><div class="line">...</div><div class="line">[00:00:50,00:01:00)</div></pre></td></tr></table></figure><p></p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> TriggerResult <span class="title">onElement</span><span class="params">(Object element, <span class="keyword">long</span> timestamp, TimeWindow window, TriggerContext ctx)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">	<span class="keyword">if</span> (window.maxTimestamp() &lt;= ctx.getCurrentWatermark()) &#123;</div><div class="line">		<span class="comment">// if the watermark is already past the window fire immediately</span></div><div class="line">		<span class="keyword">return</span> TriggerResult.FIRE;</div><div class="line">	&#125; <span class="keyword">else</span> &#123;</div><div class="line">		ctx.registerEventTimeTimer(window.maxTimestamp());</div><div class="line">		<span class="keyword">return</span> TriggerResult.CONTINUE;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>我们可以看到<code>EventTimeTrigger</code>中当<code>ctx.getCurrentWatermark &gt; window.maxTimestamp</code>时立刻触发窗口计算.</p><p>当然没有内容是不会触发计算的:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (triggerResult.isFire()) &#123;</div><div class="line">					ACC contents = windowState.get();</div><div class="line">					<span class="keyword">if</span> (contents == <span class="keyword">null</span>) &#123;</div><div class="line">						<span class="keyword">continue</span>;</div><div class="line">					&#125;</div><div class="line">					emitWindowContents(actualWindow, contents);</div><div class="line">				&#125;</div></pre></td></tr></table></figure><p>输入的数据中，根据自身的Event Time，将数据划分到不同的window中，如果window中有数据，则当watermark时间&gt;=window_edn_time时，就符合了window触发的条件了，最终决定window触发，还是由数据本身的Event Time所属的window中的window_end_time决定。</p><p>以上代码和情况二相符，其实情况一也是情况二的特殊情况，watermark &gt; 数据的 eventtime也就是说超过了最大的延迟时间，此时其实也是来了之后watermark &gt; window.endtime，然后必然会被触发只是其他该窗格的数据会被丢弃罢了。</p><p>数据清理的逻辑</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">isLate</span><span class="params">(W window)</span> </span>&#123;</div><div class="line">	<span class="keyword">return</span> (windowAssigner.isEventTime() &amp;&amp; (cleanupTime(window) &lt;= internalTimerService.currentWatermark()));</div><div class="line">&#125;</div></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">cleanupTime</span><span class="params">(W window)</span> </span>&#123;</div><div class="line">		<span class="keyword">if</span> (windowAssigner.isEventTime()) &#123;</div><div class="line">			<span class="keyword">long</span> cleanupTime = window.maxTimestamp() + allowedLateness;</div><div class="line">			<span class="keyword">return</span> cleanupTime &gt;= window.maxTimestamp() ? cleanupTime : Long.MAX_VALUE;</div><div class="line">		&#125; <span class="keyword">else</span> &#123;</div><div class="line">			<span class="keyword">return</span> window.maxTimestamp();</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure><h4 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h4><p>最大乱序时间要结合自己的业务以及数据情况去设置。如果maxOutOfOrderness设置的太小，而自身数据发送时由于网络等原因导致乱序或者late太多，那么最终的结果就是会有很多单条的数据在window中被触发，数据的正确性影响太大，对此可以通过在windowOperator处添加metrics监控来指导业务方设置成什么样的一个值才是最合理的。</p><h4 id="allowLatency"><a href="#allowLatency" class="headerlink" title="allowLatency"></a>allowLatency</h4><p>最后一点上文代码里提到的这个allowLatency又有什么作用呢?</p><p>默认情况下当watermark涨过了window的endtime之后，再有属于该窗口的数据到来的时候该数据会被丢弃，设置了allowLatency这个值之后，也就是定义了数据在watermark涨过window.endtime但是又在allowlatency之前到达的话仍旧会被加到对应的窗口去。会使得窗口<strong>再次</strong>被触发。Flink会保存窗口的状态直到allow latenness 超期。</p><p>待补充：</p><ol><li>对流传输过程中channel的理解</li><li>keyby操作过程对watermark的影响</li><li>advanceWatermark操作理解</li></ol><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="http://blog.csdn.net/lmalds/article/details/52704170" target="_blank" rel="external">http://blog.csdn.net/lmalds/article/details/52704170</a><br><a href="http://chenyuzhao.me/2017/02/09/flink-watermark-checkpoint/" target="_blank" rel="external">http://chenyuzhao.me/2017/02/09/flink-watermark-checkpoint/</a><br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/event_timestamps_watermarks.html" target="_blank" rel="external">https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/event_timestamps_watermarks.html</a><br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/windows.html#allowed-lateness" target="_blank" rel="external">https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/windows.html#allowed-lateness</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;关于Flink中watermark工作原理代码分析&lt;/p&gt;
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>flink-latency-tracker</title>
    <link href="http://www.aitozi.com/flink-latency-tracker.html"/>
    <id>http://www.aitozi.com/flink-latency-tracker.html</id>
    <published>2017-09-08T16:47:35.000Z</published>
    <updated>2017-09-10T08:03:51.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>关于Flink中的Latency track</p><a id="more"></a><h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>为了监控Flink数据端到端的数据延迟</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><ol><li>定时在source处定时的发送一个特殊的event，类似于watermark的处理方式，这里叫做LatencyMark。</li><li>latencyMarker仅在source处产生,latencyMarker对象包含的是source operator的区分标志 subIndex以及vertexID（根据subIndex以及vertexID区分是否是同一个Marker）以及携带了发送的时间</li><li>LatencyMark在各个operator间传递，在每个operator处将会比较LatencyMark和它当前的系统时间来决定延迟的大小，并存入LatencyGauge，每一个operator都会维护这样一个Metric（因此LatencyMark的实现就是基于TM和JM集群的机器系统时间是进行过同步的）</li><li>当Operator有多个Output的时候，他会随机选择一个来发送，这确保了每一个Marker在整个流中只会出现一次，repartition也不会导致LatencyMark的数量暴增。<br>在sink operator处会维护source的最近128个latencyMarker，通过一个LatencyGauge来展示</li></ol><h3 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h3><p>默认latencyTrackingInterval是2000，也就是2s发送一个LatencyMarker。在StreamSource中判断，如果开启了latency track.那么就会定期发送LatencyMarker。</p><p>在StreamSource（Operator）中的定时发送</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">latencyMarkTimer = processingTimeService.scheduleAtFixedRate(</div><div class="line">   <span class="keyword">new</span> ProcessingTimeCallback() &#123;</div><div class="line">      <span class="meta">@Override</span></div><div class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onProcessingTime</span><span class="params">(<span class="keyword">long</span> timestamp)</span> <span class="keyword">throws</span> Exception </span>&#123;</div><div class="line">         <span class="keyword">try</span> &#123;</div><div class="line">            <span class="comment">// ProcessingTimeService callbacks are executed under the checkpointing lock</span></div><div class="line">            output.emitLatencyMarker(<span class="keyword">new</span> LatencyMarker(timestamp, vertexID, subtaskIndex));</div><div class="line">         &#125; <span class="keyword">catch</span> (Throwable t) &#123;</div><div class="line">            <span class="comment">// we catch the Throwables here so that we don't trigger the processing</span></div><div class="line">            <span class="comment">// timer services async exception handler</span></div><div class="line">            LOG.warn(<span class="string">"Error while emitting latency marker."</span>, t);</div><div class="line">         &#125;</div><div class="line">      &#125;</div><div class="line">   &#125;,</div><div class="line">   <span class="number">0L</span>,</div><div class="line">   latencyTrackingInterval);</div></pre></td></tr></table></figure><p>在Operator中处理：首先进行LatencyMarker处理再发送，<br>在AbstractStreamOperator类中定义的latencyMarker处理</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reportLatency</span><span class="params">(LatencyMarker marker, <span class="keyword">boolean</span> isSink)</span> </span>&#123;</div><div class="line">   LatencySourceDescriptor sourceDescriptor = LatencySourceDescriptor.of(marker, !isSink);</div><div class="line">   DescriptiveStatistics sourceStats = latencyStats.get(sourceDescriptor);</div><div class="line">   <span class="keyword">if</span> (sourceStats == <span class="keyword">null</span>) &#123;</div><div class="line">      <span class="comment">// 512 element window (4 kb)</span></div><div class="line">      sourceStats = <span class="keyword">new</span> DescriptiveStatistics(<span class="keyword">this</span>.historySize);</div><div class="line">      latencyStats.put(sourceDescriptor, sourceStats);</div><div class="line">   &#125;</div><div class="line">   <span class="keyword">long</span> now = System.currentTimeMillis();</div><div class="line">   sourceStats.addValue(now - marker.getMarkedTime());  <span class="comment">//所以latency的计算时间是当前时间减去marker摄入的时间</span></div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="number">1</span>.首先生成一个Latency的描述符，sink operator的区分不同subIndex为不同LatencySource，其他operator不区分subIndex，只按照vertexID来区分</div><div class="line"><span class="number">2</span>.然后生成对应Latency的描述符的最近<span class="string">"historySize:128"</span>个Latency的值（WindowSize controls the number of values that contribute to the reported statistics. ）</div><div class="line"><span class="number">3</span>.在这里值没有在web展示的原因是因为guage展示的不是一个数字，因而无法被展示</div></pre></td></tr></table></figure><p>然后只要不是sink类型的operator，就会往后继续传递LatencyMarker。随机选择一个Channel来发送,这里就是为了保证一个latencyMarker在整个流中只会出现一次。这里和watermark的机制有点不一样，waterMark是遍历全部的channel来发送。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">emitLatencyMarker</span><span class="params">(LatencyMarker latencyMarker)</span> </span>&#123;</div><div class="line">   <span class="keyword">if</span>(outputs.length &lt;= <span class="number">0</span>) &#123;</div><div class="line">      <span class="comment">// ignore</span></div><div class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span>(outputs.length == <span class="number">1</span>) &#123;</div><div class="line">      outputs[<span class="number">0</span>].emitLatencyMarker(latencyMarker);</div><div class="line">   &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// randomly select an output</span></div><div class="line">      outputs[RNG.nextInt(outputs.length)].emitLatencyMarker(latencyMarker);</div><div class="line">   &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p><strong>LatencyMarker不会参与窗口时间的计算，应该说是不参与任何operator的计算，因此他只能用来衡量数据在整个DAG中流通的速度不能衡量operator计算的时间，这个只能通过单测来进行计算</strong><br><code>StreamInputProcessor#processInput</code> 这里进行对进入的element进行处理，对于watermark和LatencyMarker类型会先处理发送掉，不会经由后面的windowOperator或其他operator来处理</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (result.isFullRecord()) &#123;</div><div class="line">   StreamElement recordOrMark = deserializationDelegate.getInstance();</div><div class="line"> </div><div class="line">   <span class="keyword">if</span> (recordOrMark.isWatermark()) &#123;</div><div class="line">      <span class="keyword">long</span> watermarkMillis = recordOrMark.asWatermark().getTimestamp();</div><div class="line">      <span class="keyword">if</span> (watermarkMillis &gt; watermarks[currentChannel]) &#123;</div><div class="line">         watermarks[currentChannel] = watermarkMillis;</div><div class="line">         <span class="keyword">long</span> newMinWatermark = Long.MAX_VALUE;</div><div class="line">         <span class="keyword">for</span> (<span class="keyword">long</span> watermark: watermarks) &#123;</div><div class="line">            newMinWatermark = Math.min(watermark, newMinWatermark);</div><div class="line">         &#125;</div><div class="line">         <span class="keyword">if</span> (newMinWatermark &gt; lastEmittedWatermark) &#123;</div><div class="line">            lastEmittedWatermark = newMinWatermark;</div><div class="line">            <span class="keyword">synchronized</span> (lock) &#123;</div><div class="line">               streamOperator.processWatermark(<span class="keyword">new</span> Watermark(lastEmittedWatermark));</div><div class="line">            &#125;</div><div class="line">         &#125;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">continue</span>;</div><div class="line">   &#125; <span class="keyword">else</span> <span class="keyword">if</span>(recordOrMark.isLatencyMarker()) &#123;</div><div class="line">      <span class="comment">// handle latency marker</span></div><div class="line">      <span class="keyword">synchronized</span> (lock) &#123;</div><div class="line">         streamOperator.processLatencyMarker(recordOrMark.asLatencyMarker());</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">continue</span>;</div><div class="line">   &#125; <span class="keyword">else</span> &#123;</div><div class="line">      <span class="comment">// now we can do the actual processing</span></div><div class="line">      StreamRecord&lt;IN&gt; record = recordOrMark.asRecord();</div><div class="line">      <span class="keyword">synchronized</span> (lock) &#123;</div><div class="line">         numRecordsIn.inc();</div><div class="line">         streamOperator.setKeyContextElement1(record);</div><div class="line">         streamOperator.processElement(record);</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">   &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure><p>sink中只进行report不再进行forward了（StreamSink.java）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reportOrForwardLatencyMarker</span><span class="params">(LatencyMarker maker)</span> </span>&#123;</div><div class="line">   <span class="comment">// all operators are tracking latencies</span></div><div class="line">   <span class="keyword">this</span>.latencyGauge.reportLatency(maker, <span class="keyword">true</span>);</div><div class="line"> </div><div class="line">   <span class="comment">// sinks don't forward latency markers</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>LatencyMarker能够较好的监控因网络抖动或数据反压引起的延迟，可以提前预警反压情况</li><li>在正常情况（没有反压）下数据在DAG图中的流动延迟大概0.5s左右，所以说Flink的确是一个很快的引擎：）</li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;关于Flink中的Latency track&lt;/p&gt;
    
    </summary>
    
      <category term="源码分析" scheme="http://www.aitozi.com/categories/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Flink" scheme="http://www.aitozi.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Hexo tutorial</title>
    <link href="http://www.aitozi.com/hello-world.html"/>
    <id>http://www.aitozi.com/hello-world.html</id>
    <published>2016-10-11T16:47:35.000Z</published>
    <updated>2017-09-10T07:02:25.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --><p>Hexo 写文章常用语法</p><a id="more"></a><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p><p>###Create a page</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new page tags</div></pre></td></tr></table></figure><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Mar 15 2019 23:08:17 GMT+0800 (CST) --&gt;&lt;p&gt;Hexo 写文章常用语法&lt;/p&gt;
    
    </summary>
    
      <category term="常用命令" scheme="http://www.aitozi.com/categories/%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    
    
      <category term="hexo" scheme="http://www.aitozi.com/tags/hexo/"/>
    
  </entry>
  
</feed>
